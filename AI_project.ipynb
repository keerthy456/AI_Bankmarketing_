{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-project",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQiCfCAkqSU-",
        "outputId": "7e2e872a-b3e6-4aa5-9b3b-1bc3393ce486"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/keerthy456/AI_Bankmarketing_/main/AIphase1.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-25 03:04:28--  https://raw.githubusercontent.com/keerthy456/AI_Bankmarketing_/main/AIphase1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2733333 (2.6M) [text/plain]\n",
            "Saving to: ‘AIphase1.csv’\n",
            "\n",
            "AIphase1.csv        100%[===================>]   2.61M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-04-25 03:04:28 (31.6 MB/s) - ‘AIphase1.csv’ saved [2733333/2733333]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIVgQ1K_rlaU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import random\n",
        "\n",
        "data = pd.read_csv('AIphase1.csv',delimiter=\",\")\n",
        "\n",
        "dataset = np.genfromtxt('AIphase1.csv',delimiter=',',skip_header=True)\n",
        "np.set_printoptions(formatter={'float':'{:0.1f}'.format})\n",
        "np.random.shuffle(dataset)\n",
        "#loading data set\n",
        "X = dataset[:,:-1]\n",
        "Y = dataset[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2jl4GqRr0il"
      },
      "source": [
        "max_array = []\n",
        "min_array =[]\n",
        "size = dataset.shape\n",
        "column= size[1]\n",
        "for i in range(column):\n",
        "  min = dataset[:,i].min(axis=0)\n",
        "  max = dataset[:,i].max(axis=0)\n",
        "  max_array.append(max)\n",
        "  min_array.append(min)\n",
        "  dataset[:,i] = (dataset[:,i]-min)/(max-min)\n",
        "X = dataset[:,:-1]\n",
        "Y = dataset[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnd4uY7YsBUq",
        "outputId": "0aec41f6-83b1-47a0-bf3b-861ea3e6fa78"
      },
      "source": [
        "\n",
        "np.random.shuffle(dataset)\n",
        "\n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "XVALID = X[:index]\n",
        "YVALID = Y[:index]\n",
        "XTRAIN = X[index:]\n",
        "YTRAIN = Y[index:]\n",
        "\n",
        "high = len(YVALID) - sum(YVALID)\n",
        "print(\"Baseline accuracy : \",high/len(YVALID))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline accuracy :  0.8937014248704663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VN23DMEjq1qf",
        "outputId": "6f4e0491-44ed-4433-d6c2-06302dbaa02e"
      },
      "source": [
        "#0.3\n",
        "model1 = Sequential()\n",
        "model1.add(Dense(1, input_dim = 20,activation='sigmoid'))\n",
        "model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model1.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4630 - accuracy: 0.8137 - val_loss: 0.3154 - val_accuracy: 0.8910\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.3241 - accuracy: 0.8823 - val_loss: 0.2941 - val_accuracy: 0.8911\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2951 - accuracy: 0.8874 - val_loss: 0.2824 - val_accuracy: 0.8924\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2848 - accuracy: 0.8882 - val_loss: 0.2754 - val_accuracy: 0.8960\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2768 - accuracy: 0.8942 - val_loss: 0.2702 - val_accuracy: 0.8993\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2765 - accuracy: 0.8941 - val_loss: 0.2656 - val_accuracy: 0.9001\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2671 - accuracy: 0.8975 - val_loss: 0.2616 - val_accuracy: 0.9011\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2668 - accuracy: 0.8964 - val_loss: 0.2579 - val_accuracy: 0.9015\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2595 - accuracy: 0.9000 - val_loss: 0.2548 - val_accuracy: 0.9022\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2646 - accuracy: 0.8977 - val_loss: 0.2516 - val_accuracy: 0.9030\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2602 - accuracy: 0.8973 - val_loss: 0.2488 - val_accuracy: 0.9038\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2553 - accuracy: 0.9003 - val_loss: 0.2462 - val_accuracy: 0.9042\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2497 - accuracy: 0.9015 - val_loss: 0.2439 - val_accuracy: 0.9047\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2522 - accuracy: 0.9004 - val_loss: 0.2417 - val_accuracy: 0.9048\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2435 - accuracy: 0.9038 - val_loss: 0.2398 - val_accuracy: 0.9057\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2451 - accuracy: 0.9025 - val_loss: 0.2381 - val_accuracy: 0.9065\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2437 - accuracy: 0.9022 - val_loss: 0.2366 - val_accuracy: 0.9071\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2385 - accuracy: 0.9037 - val_loss: 0.2349 - val_accuracy: 0.9079\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2383 - accuracy: 0.9042 - val_loss: 0.2335 - val_accuracy: 0.9081\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2346 - accuracy: 0.9048 - val_loss: 0.2315 - val_accuracy: 0.9083\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2358 - accuracy: 0.9051 - val_loss: 0.2303 - val_accuracy: 0.9086\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2365 - accuracy: 0.9034 - val_loss: 0.2289 - val_accuracy: 0.9082\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2298 - accuracy: 0.9051 - val_loss: 0.2287 - val_accuracy: 0.9089\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2319 - accuracy: 0.9067 - val_loss: 0.2269 - val_accuracy: 0.9089\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2309 - accuracy: 0.9057 - val_loss: 0.2262 - val_accuracy: 0.9092\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2279 - accuracy: 0.9071 - val_loss: 0.2250 - val_accuracy: 0.9091\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2344 - accuracy: 0.9027 - val_loss: 0.2242 - val_accuracy: 0.9091\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2270 - accuracy: 0.9066 - val_loss: 0.2236 - val_accuracy: 0.9097\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2248 - accuracy: 0.9067 - val_loss: 0.2228 - val_accuracy: 0.9099\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2298 - accuracy: 0.9065 - val_loss: 0.2226 - val_accuracy: 0.9101\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2229 - accuracy: 0.9074 - val_loss: 0.2217 - val_accuracy: 0.9105\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2290 - accuracy: 0.9071 - val_loss: 0.2209 - val_accuracy: 0.9102\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2228 - accuracy: 0.9098 - val_loss: 0.2209 - val_accuracy: 0.9110\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2188 - accuracy: 0.9111 - val_loss: 0.2198 - val_accuracy: 0.9101\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2255 - accuracy: 0.9073 - val_loss: 0.2195 - val_accuracy: 0.9113\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2189 - accuracy: 0.9099 - val_loss: 0.2193 - val_accuracy: 0.9114\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2218 - accuracy: 0.9093 - val_loss: 0.2187 - val_accuracy: 0.9114\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2235 - accuracy: 0.9079 - val_loss: 0.2184 - val_accuracy: 0.9117\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2226 - accuracy: 0.9074 - val_loss: 0.2186 - val_accuracy: 0.9123\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2266 - accuracy: 0.9071 - val_loss: 0.2176 - val_accuracy: 0.9114\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2225 - accuracy: 0.9091 - val_loss: 0.2180 - val_accuracy: 0.9124\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2270 - accuracy: 0.9056 - val_loss: 0.2170 - val_accuracy: 0.9114\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2165 - accuracy: 0.9096 - val_loss: 0.2175 - val_accuracy: 0.9125\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2206 - accuracy: 0.9099 - val_loss: 0.2168 - val_accuracy: 0.9120\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2221 - accuracy: 0.9080 - val_loss: 0.2164 - val_accuracy: 0.9118\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2222 - accuracy: 0.9076 - val_loss: 0.2170 - val_accuracy: 0.9124\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2206 - accuracy: 0.9080 - val_loss: 0.2160 - val_accuracy: 0.9116\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2207 - accuracy: 0.9079 - val_loss: 0.2160 - val_accuracy: 0.9118\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2126 - accuracy: 0.9126 - val_loss: 0.2162 - val_accuracy: 0.9123\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2150 - accuracy: 0.9130 - val_loss: 0.2162 - val_accuracy: 0.9127\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2206 - accuracy: 0.9074 - val_loss: 0.2158 - val_accuracy: 0.9124\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2162 - accuracy: 0.9107 - val_loss: 0.2153 - val_accuracy: 0.9121\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2175 - accuracy: 0.9099 - val_loss: 0.2157 - val_accuracy: 0.9129\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2206 - accuracy: 0.9080 - val_loss: 0.2152 - val_accuracy: 0.9123\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2177 - accuracy: 0.9089 - val_loss: 0.2149 - val_accuracy: 0.9119\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2218 - accuracy: 0.9078 - val_loss: 0.2148 - val_accuracy: 0.9120\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2145 - accuracy: 0.9111 - val_loss: 0.2149 - val_accuracy: 0.9127\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2156 - accuracy: 0.9110 - val_loss: 0.2150 - val_accuracy: 0.9128\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2169 - accuracy: 0.9095 - val_loss: 0.2146 - val_accuracy: 0.9122\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2153 - accuracy: 0.9109 - val_loss: 0.2146 - val_accuracy: 0.9130\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2178 - accuracy: 0.9100 - val_loss: 0.2148 - val_accuracy: 0.9131\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2171 - accuracy: 0.9083 - val_loss: 0.2144 - val_accuracy: 0.9129\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2150 - accuracy: 0.9114 - val_loss: 0.2145 - val_accuracy: 0.9131\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2162 - accuracy: 0.9096 - val_loss: 0.2146 - val_accuracy: 0.9131\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2199 - accuracy: 0.9073 - val_loss: 0.2143 - val_accuracy: 0.9130\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2137 - accuracy: 0.9121 - val_loss: 0.2148 - val_accuracy: 0.9134\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2218 - accuracy: 0.9059 - val_loss: 0.2141 - val_accuracy: 0.9129\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2201 - accuracy: 0.9073 - val_loss: 0.2140 - val_accuracy: 0.9128\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2170 - accuracy: 0.9089 - val_loss: 0.2144 - val_accuracy: 0.9133\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2173 - accuracy: 0.9097 - val_loss: 0.2145 - val_accuracy: 0.9135\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2219 - accuracy: 0.9072 - val_loss: 0.2142 - val_accuracy: 0.9133\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2185 - accuracy: 0.9097 - val_loss: 0.2140 - val_accuracy: 0.9131\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2165 - accuracy: 0.9097 - val_loss: 0.2158 - val_accuracy: 0.9130\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2244 - accuracy: 0.9069 - val_loss: 0.2145 - val_accuracy: 0.9132\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2191 - accuracy: 0.9094 - val_loss: 0.2144 - val_accuracy: 0.9136\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2207 - accuracy: 0.9095 - val_loss: 0.2142 - val_accuracy: 0.9134\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2209 - accuracy: 0.9080 - val_loss: 0.2138 - val_accuracy: 0.9131\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2198 - accuracy: 0.9098 - val_loss: 0.2139 - val_accuracy: 0.9131\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2168 - accuracy: 0.9099 - val_loss: 0.2138 - val_accuracy: 0.9130\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2150 - accuracy: 0.9109 - val_loss: 0.2137 - val_accuracy: 0.9130\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2185 - accuracy: 0.9085 - val_loss: 0.2137 - val_accuracy: 0.9131\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2223 - accuracy: 0.9079 - val_loss: 0.2141 - val_accuracy: 0.9127\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2202 - accuracy: 0.9089 - val_loss: 0.2139 - val_accuracy: 0.9130\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2249 - accuracy: 0.9073 - val_loss: 0.2136 - val_accuracy: 0.9130\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2218 - accuracy: 0.9070 - val_loss: 0.2136 - val_accuracy: 0.9130\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2140 - accuracy: 0.9113 - val_loss: 0.2137 - val_accuracy: 0.9132\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2191 - accuracy: 0.9101 - val_loss: 0.2136 - val_accuracy: 0.9131\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9096 - val_loss: 0.2137 - val_accuracy: 0.9132\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2169 - accuracy: 0.9091 - val_loss: 0.2137 - val_accuracy: 0.9128\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2166 - accuracy: 0.9102 - val_loss: 0.2137 - val_accuracy: 0.9131\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2147 - accuracy: 0.9113 - val_loss: 0.2139 - val_accuracy: 0.9125\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2161 - accuracy: 0.9096 - val_loss: 0.2137 - val_accuracy: 0.9127\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2206 - accuracy: 0.9098 - val_loss: 0.2138 - val_accuracy: 0.9125\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2125 - accuracy: 0.9127 - val_loss: 0.2140 - val_accuracy: 0.9124\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2162 - accuracy: 0.9118 - val_loss: 0.2143 - val_accuracy: 0.9126\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2139 - accuracy: 0.9113 - val_loss: 0.2144 - val_accuracy: 0.9126\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2133 - accuracy: 0.9113 - val_loss: 0.2139 - val_accuracy: 0.9126\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2152 - accuracy: 0.9110 - val_loss: 0.2137 - val_accuracy: 0.9123\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2166 - accuracy: 0.9118 - val_loss: 0.2140 - val_accuracy: 0.9124\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2147 - accuracy: 0.9113 - val_loss: 0.2140 - val_accuracy: 0.9126\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2146 - accuracy: 0.9114 - val_loss: 0.2136 - val_accuracy: 0.9125\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2125 - accuracy: 0.9130 - val_loss: 0.2136 - val_accuracy: 0.9125\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2171 - accuracy: 0.9103 - val_loss: 0.2134 - val_accuracy: 0.9128\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2136 - accuracy: 0.9112 - val_loss: 0.2135 - val_accuracy: 0.9122\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2154 - accuracy: 0.9111 - val_loss: 0.2147 - val_accuracy: 0.9127\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2115 - accuracy: 0.9139 - val_loss: 0.2140 - val_accuracy: 0.9129\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2169 - accuracy: 0.9108 - val_loss: 0.2137 - val_accuracy: 0.9125\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2187 - accuracy: 0.9113 - val_loss: 0.2137 - val_accuracy: 0.9127\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2148 - accuracy: 0.9117 - val_loss: 0.2140 - val_accuracy: 0.9129\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2195 - accuracy: 0.9109 - val_loss: 0.2136 - val_accuracy: 0.9125\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2200 - accuracy: 0.9093 - val_loss: 0.2142 - val_accuracy: 0.9126\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2104 - accuracy: 0.9142 - val_loss: 0.2138 - val_accuracy: 0.9130\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2119 - accuracy: 0.9121 - val_loss: 0.2137 - val_accuracy: 0.9126\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2176 - accuracy: 0.9091 - val_loss: 0.2137 - val_accuracy: 0.9130\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2165 - accuracy: 0.9094 - val_loss: 0.2140 - val_accuracy: 0.9128\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2193 - accuracy: 0.9091 - val_loss: 0.2134 - val_accuracy: 0.9128\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2161 - accuracy: 0.9125 - val_loss: 0.2135 - val_accuracy: 0.9126\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2117 - accuracy: 0.9137 - val_loss: 0.2136 - val_accuracy: 0.9126\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2162 - accuracy: 0.9111 - val_loss: 0.2141 - val_accuracy: 0.9129\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2162 - accuracy: 0.9120 - val_loss: 0.2135 - val_accuracy: 0.9126\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2174 - accuracy: 0.9114 - val_loss: 0.2137 - val_accuracy: 0.9129\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2189 - accuracy: 0.9093 - val_loss: 0.2136 - val_accuracy: 0.9127\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2109 - accuracy: 0.9113 - val_loss: 0.2136 - val_accuracy: 0.9128\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2122 - accuracy: 0.9116 - val_loss: 0.2139 - val_accuracy: 0.9128\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2156 - accuracy: 0.9097 - val_loss: 0.2140 - val_accuracy: 0.9131\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2147 - accuracy: 0.9108 - val_loss: 0.2137 - val_accuracy: 0.9129\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2147 - accuracy: 0.9123 - val_loss: 0.2138 - val_accuracy: 0.9130\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2175 - accuracy: 0.9096 - val_loss: 0.2144 - val_accuracy: 0.9131\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-9HZyYj8vWq"
      },
      "source": [
        "model_bst1 = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5T8pzJfzUgAb",
        "outputId": "a96feaab-817b-4d38-d83c-69252082cd18"
      },
      "source": [
        "\n",
        "model_bst1.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_bst1.add(Dense(4,activation='relu'))\n",
        "model_bst1.add(Dense(1,activation='sigmoid'))\n",
        "model_bst1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_bst1.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3780 - accuracy: 0.8598 - val_loss: 0.2668 - val_accuracy: 0.8959\n",
            "Epoch 2/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2687 - accuracy: 0.8926 - val_loss: 0.2243 - val_accuracy: 0.9118\n",
            "Epoch 3/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2246 - accuracy: 0.9078 - val_loss: 0.2050 - val_accuracy: 0.9128\n",
            "Epoch 4/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2098 - accuracy: 0.9092 - val_loss: 0.1942 - val_accuracy: 0.9156\n",
            "Epoch 5/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2069 - accuracy: 0.9105 - val_loss: 0.1920 - val_accuracy: 0.9162\n",
            "Epoch 6/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2035 - accuracy: 0.9110 - val_loss: 0.1903 - val_accuracy: 0.9154\n",
            "Epoch 7/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2098 - accuracy: 0.9075 - val_loss: 0.1888 - val_accuracy: 0.9156\n",
            "Epoch 8/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2061 - accuracy: 0.9058 - val_loss: 0.1899 - val_accuracy: 0.9158\n",
            "Epoch 9/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2043 - accuracy: 0.9085 - val_loss: 0.1873 - val_accuracy: 0.9161\n",
            "Epoch 10/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2087 - accuracy: 0.9084 - val_loss: 0.1870 - val_accuracy: 0.9165\n",
            "Epoch 11/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2007 - accuracy: 0.9099 - val_loss: 0.1919 - val_accuracy: 0.9154\n",
            "Epoch 12/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1971 - accuracy: 0.9125 - val_loss: 0.1861 - val_accuracy: 0.9150\n",
            "Epoch 13/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1980 - accuracy: 0.9127 - val_loss: 0.1883 - val_accuracy: 0.9161\n",
            "Epoch 14/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2004 - accuracy: 0.9101 - val_loss: 0.1842 - val_accuracy: 0.9161\n",
            "Epoch 15/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9111 - val_loss: 0.1850 - val_accuracy: 0.9153\n",
            "Epoch 16/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2011 - accuracy: 0.9096 - val_loss: 0.1864 - val_accuracy: 0.9161\n",
            "Epoch 17/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1999 - accuracy: 0.9105 - val_loss: 0.1876 - val_accuracy: 0.9160\n",
            "Epoch 18/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1927 - accuracy: 0.9138 - val_loss: 0.1827 - val_accuracy: 0.9156\n",
            "Epoch 19/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1970 - accuracy: 0.9119 - val_loss: 0.1838 - val_accuracy: 0.9154\n",
            "Epoch 20/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1957 - accuracy: 0.9123 - val_loss: 0.1829 - val_accuracy: 0.9159\n",
            "Epoch 21/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1927 - accuracy: 0.9125 - val_loss: 0.1835 - val_accuracy: 0.9173\n",
            "Epoch 22/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9108 - val_loss: 0.1825 - val_accuracy: 0.9153\n",
            "Epoch 23/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1995 - accuracy: 0.9083 - val_loss: 0.1821 - val_accuracy: 0.9156\n",
            "Epoch 24/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1970 - accuracy: 0.9107 - val_loss: 0.1846 - val_accuracy: 0.9157\n",
            "Epoch 25/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9113 - val_loss: 0.1842 - val_accuracy: 0.9152\n",
            "Epoch 26/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2001 - accuracy: 0.9096 - val_loss: 0.1819 - val_accuracy: 0.9144\n",
            "Epoch 27/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1964 - accuracy: 0.9086 - val_loss: 0.1820 - val_accuracy: 0.9148\n",
            "Epoch 28/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1938 - accuracy: 0.9128 - val_loss: 0.1817 - val_accuracy: 0.9141\n",
            "Epoch 29/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1931 - accuracy: 0.9118 - val_loss: 0.1812 - val_accuracy: 0.9152\n",
            "Epoch 30/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1958 - accuracy: 0.9110 - val_loss: 0.1821 - val_accuracy: 0.9159\n",
            "Epoch 31/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1940 - accuracy: 0.9130 - val_loss: 0.1818 - val_accuracy: 0.9150\n",
            "Epoch 32/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1980 - accuracy: 0.9085 - val_loss: 0.1904 - val_accuracy: 0.9137\n",
            "Epoch 33/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9144 - val_loss: 0.1836 - val_accuracy: 0.9143\n",
            "Epoch 34/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1950 - accuracy: 0.9115 - val_loss: 0.1842 - val_accuracy: 0.9148\n",
            "Epoch 35/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1942 - accuracy: 0.9109 - val_loss: 0.1827 - val_accuracy: 0.9145\n",
            "Epoch 36/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9097 - val_loss: 0.1837 - val_accuracy: 0.9140\n",
            "Epoch 37/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9102 - val_loss: 0.1807 - val_accuracy: 0.9140\n",
            "Epoch 38/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1937 - accuracy: 0.9092 - val_loss: 0.1830 - val_accuracy: 0.9138\n",
            "Epoch 39/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1905 - accuracy: 0.9101 - val_loss: 0.1803 - val_accuracy: 0.9146\n",
            "Epoch 40/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9112 - val_loss: 0.1814 - val_accuracy: 0.9141\n",
            "Epoch 41/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9115 - val_loss: 0.1800 - val_accuracy: 0.9141\n",
            "Epoch 42/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1914 - accuracy: 0.9096 - val_loss: 0.1798 - val_accuracy: 0.9147\n",
            "Epoch 43/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9131 - val_loss: 0.1820 - val_accuracy: 0.9146\n",
            "Epoch 44/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9148 - val_loss: 0.1813 - val_accuracy: 0.9142\n",
            "Epoch 45/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9129 - val_loss: 0.1835 - val_accuracy: 0.9143\n",
            "Epoch 46/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9104 - val_loss: 0.1797 - val_accuracy: 0.9148\n",
            "Epoch 47/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1843 - accuracy: 0.9148 - val_loss: 0.1800 - val_accuracy: 0.9140\n",
            "Epoch 48/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9122 - val_loss: 0.1797 - val_accuracy: 0.9153\n",
            "Epoch 49/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.9149 - val_loss: 0.1803 - val_accuracy: 0.9143\n",
            "Epoch 50/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9121 - val_loss: 0.1798 - val_accuracy: 0.9144\n",
            "Epoch 51/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9141 - val_loss: 0.1804 - val_accuracy: 0.9144\n",
            "Epoch 52/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1877 - accuracy: 0.9126 - val_loss: 0.1829 - val_accuracy: 0.9143\n",
            "Epoch 53/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1890 - accuracy: 0.9094 - val_loss: 0.1797 - val_accuracy: 0.9152\n",
            "Epoch 54/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9134 - val_loss: 0.1804 - val_accuracy: 0.9139\n",
            "Epoch 55/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9097 - val_loss: 0.1797 - val_accuracy: 0.9142\n",
            "Epoch 56/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1894 - accuracy: 0.9111 - val_loss: 0.1803 - val_accuracy: 0.9144\n",
            "Epoch 57/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1923 - accuracy: 0.9102 - val_loss: 0.1794 - val_accuracy: 0.9145\n",
            "Epoch 58/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1937 - accuracy: 0.9103 - val_loss: 0.1800 - val_accuracy: 0.9143\n",
            "Epoch 59/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1887 - accuracy: 0.9108 - val_loss: 0.1807 - val_accuracy: 0.9145\n",
            "Epoch 60/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9109 - val_loss: 0.1801 - val_accuracy: 0.9134\n",
            "Epoch 61/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.9145 - val_loss: 0.1798 - val_accuracy: 0.9143\n",
            "Epoch 62/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1873 - accuracy: 0.9115 - val_loss: 0.1793 - val_accuracy: 0.9131\n",
            "Epoch 63/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9125 - val_loss: 0.1811 - val_accuracy: 0.9143\n",
            "Epoch 64/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9114 - val_loss: 0.1795 - val_accuracy: 0.9141\n",
            "Epoch 65/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9123 - val_loss: 0.1794 - val_accuracy: 0.9141\n",
            "Epoch 66/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9159 - val_loss: 0.1794 - val_accuracy: 0.9160\n",
            "Epoch 67/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1895 - accuracy: 0.9114 - val_loss: 0.1787 - val_accuracy: 0.9135\n",
            "Epoch 68/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9122 - val_loss: 0.1812 - val_accuracy: 0.9153\n",
            "Epoch 69/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9131 - val_loss: 0.1818 - val_accuracy: 0.9154\n",
            "Epoch 70/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9131 - val_loss: 0.1821 - val_accuracy: 0.9143\n",
            "Epoch 71/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1891 - accuracy: 0.9099 - val_loss: 0.1844 - val_accuracy: 0.9152\n",
            "Epoch 72/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9136 - val_loss: 0.1782 - val_accuracy: 0.9151\n",
            "Epoch 73/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9103 - val_loss: 0.1779 - val_accuracy: 0.9155\n",
            "Epoch 74/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9096 - val_loss: 0.1906 - val_accuracy: 0.9131\n",
            "Epoch 75/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9144 - val_loss: 0.1786 - val_accuracy: 0.9140\n",
            "Epoch 76/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1792 - accuracy: 0.9171 - val_loss: 0.1805 - val_accuracy: 0.9146\n",
            "Epoch 77/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9117 - val_loss: 0.1851 - val_accuracy: 0.9140\n",
            "Epoch 78/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9100 - val_loss: 0.1795 - val_accuracy: 0.9143\n",
            "Epoch 79/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1843 - accuracy: 0.9131 - val_loss: 0.1799 - val_accuracy: 0.9141\n",
            "Epoch 80/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9106 - val_loss: 0.1827 - val_accuracy: 0.9139\n",
            "Epoch 81/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1901 - accuracy: 0.9099 - val_loss: 0.1802 - val_accuracy: 0.9145\n",
            "Epoch 82/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9133 - val_loss: 0.1783 - val_accuracy: 0.9145\n",
            "Epoch 83/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9126 - val_loss: 0.1777 - val_accuracy: 0.9161\n",
            "Epoch 84/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9116 - val_loss: 0.1780 - val_accuracy: 0.9157\n",
            "Epoch 85/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1812 - accuracy: 0.9156 - val_loss: 0.1883 - val_accuracy: 0.9145\n",
            "Epoch 86/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9117 - val_loss: 0.1822 - val_accuracy: 0.9143\n",
            "Epoch 87/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9138 - val_loss: 0.1790 - val_accuracy: 0.9143\n",
            "Epoch 88/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1846 - accuracy: 0.9146 - val_loss: 0.1833 - val_accuracy: 0.9156\n",
            "Epoch 89/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1806 - accuracy: 0.9133 - val_loss: 0.1791 - val_accuracy: 0.9144\n",
            "Epoch 90/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9129 - val_loss: 0.1771 - val_accuracy: 0.9153\n",
            "Epoch 91/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9150 - val_loss: 0.1786 - val_accuracy: 0.9158\n",
            "Epoch 92/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1843 - accuracy: 0.9116 - val_loss: 0.1785 - val_accuracy: 0.9150\n",
            "Epoch 93/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1806 - accuracy: 0.9139 - val_loss: 0.1771 - val_accuracy: 0.9158\n",
            "Epoch 94/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1857 - accuracy: 0.9126 - val_loss: 0.1794 - val_accuracy: 0.9145\n",
            "Epoch 95/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9118 - val_loss: 0.1777 - val_accuracy: 0.9165\n",
            "Epoch 96/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9152 - val_loss: 0.1776 - val_accuracy: 0.9148\n",
            "Epoch 97/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9125 - val_loss: 0.1769 - val_accuracy: 0.9160\n",
            "Epoch 98/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9098 - val_loss: 0.1834 - val_accuracy: 0.9144\n",
            "Epoch 99/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1805 - accuracy: 0.9134 - val_loss: 0.1765 - val_accuracy: 0.9154\n",
            "Epoch 100/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9120 - val_loss: 0.1764 - val_accuracy: 0.9155\n",
            "Epoch 101/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1827 - accuracy: 0.9145 - val_loss: 0.1787 - val_accuracy: 0.9158\n",
            "Epoch 102/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1864 - accuracy: 0.9127 - val_loss: 0.1770 - val_accuracy: 0.9152\n",
            "Epoch 103/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9093 - val_loss: 0.1774 - val_accuracy: 0.9158\n",
            "Epoch 104/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9126 - val_loss: 0.1772 - val_accuracy: 0.9156\n",
            "Epoch 105/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1837 - accuracy: 0.9125 - val_loss: 0.1772 - val_accuracy: 0.9162\n",
            "Epoch 106/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9103 - val_loss: 0.1785 - val_accuracy: 0.9150\n",
            "Epoch 107/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9137 - val_loss: 0.1763 - val_accuracy: 0.9157\n",
            "Epoch 108/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9122 - val_loss: 0.1762 - val_accuracy: 0.9156\n",
            "Epoch 109/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1797 - accuracy: 0.9154 - val_loss: 0.1762 - val_accuracy: 0.9164\n",
            "Epoch 110/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9116 - val_loss: 0.1831 - val_accuracy: 0.9145\n",
            "Epoch 111/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9119 - val_loss: 0.1764 - val_accuracy: 0.9163\n",
            "Epoch 112/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9142 - val_loss: 0.1770 - val_accuracy: 0.9160\n",
            "Epoch 113/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1864 - accuracy: 0.9112 - val_loss: 0.1762 - val_accuracy: 0.9161\n",
            "Epoch 114/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1824 - accuracy: 0.9137 - val_loss: 0.1759 - val_accuracy: 0.9162\n",
            "Epoch 115/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9121 - val_loss: 0.1771 - val_accuracy: 0.9161\n",
            "Epoch 116/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1775 - accuracy: 0.9164 - val_loss: 0.1757 - val_accuracy: 0.9160\n",
            "Epoch 117/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9134 - val_loss: 0.1761 - val_accuracy: 0.9163\n",
            "Epoch 118/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1834 - accuracy: 0.9126 - val_loss: 0.1764 - val_accuracy: 0.9161\n",
            "Epoch 119/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1835 - accuracy: 0.9128 - val_loss: 0.1765 - val_accuracy: 0.9164\n",
            "Epoch 120/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9135 - val_loss: 0.1763 - val_accuracy: 0.9155\n",
            "Epoch 121/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9118 - val_loss: 0.1790 - val_accuracy: 0.9150\n",
            "Epoch 122/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9130 - val_loss: 0.1765 - val_accuracy: 0.9160\n",
            "Epoch 123/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9131 - val_loss: 0.1775 - val_accuracy: 0.9157\n",
            "Epoch 124/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1831 - accuracy: 0.9118 - val_loss: 0.1803 - val_accuracy: 0.9165\n",
            "Epoch 125/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9134 - val_loss: 0.1818 - val_accuracy: 0.9166\n",
            "Epoch 126/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9124 - val_loss: 0.1765 - val_accuracy: 0.9160\n",
            "Epoch 127/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9131 - val_loss: 0.1766 - val_accuracy: 0.9165\n",
            "Epoch 128/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1834 - accuracy: 0.9111 - val_loss: 0.1752 - val_accuracy: 0.9167\n",
            "Epoch 129/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9128 - val_loss: 0.1766 - val_accuracy: 0.9165\n",
            "Epoch 130/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9120 - val_loss: 0.1758 - val_accuracy: 0.9164\n",
            "Epoch 131/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9145 - val_loss: 0.1762 - val_accuracy: 0.9166\n",
            "Epoch 132/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1850 - accuracy: 0.9107 - val_loss: 0.1777 - val_accuracy: 0.9161\n",
            "Epoch 133/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1808 - accuracy: 0.9149 - val_loss: 0.1761 - val_accuracy: 0.9167\n",
            "Epoch 134/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9118 - val_loss: 0.1761 - val_accuracy: 0.9161\n",
            "Epoch 135/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9135 - val_loss: 0.1776 - val_accuracy: 0.9165\n",
            "Epoch 136/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.9169 - val_loss: 0.1775 - val_accuracy: 0.9170\n",
            "Epoch 137/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9149 - val_loss: 0.1765 - val_accuracy: 0.9161\n",
            "Epoch 138/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1812 - accuracy: 0.9136 - val_loss: 0.1772 - val_accuracy: 0.9168\n",
            "Epoch 139/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9124 - val_loss: 0.1757 - val_accuracy: 0.9166\n",
            "Epoch 140/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9134 - val_loss: 0.1810 - val_accuracy: 0.9153\n",
            "Epoch 141/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9110 - val_loss: 0.1761 - val_accuracy: 0.9163\n",
            "Epoch 142/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1810 - accuracy: 0.9136 - val_loss: 0.1793 - val_accuracy: 0.9160\n",
            "Epoch 143/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9095 - val_loss: 0.1824 - val_accuracy: 0.9174\n",
            "Epoch 144/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1793 - accuracy: 0.9160 - val_loss: 0.1763 - val_accuracy: 0.9169\n",
            "Epoch 145/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9137 - val_loss: 0.1794 - val_accuracy: 0.9156\n",
            "Epoch 146/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.9151 - val_loss: 0.1757 - val_accuracy: 0.9169\n",
            "Epoch 147/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9132 - val_loss: 0.1794 - val_accuracy: 0.9169\n",
            "Epoch 148/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1837 - accuracy: 0.9141 - val_loss: 0.1764 - val_accuracy: 0.9172\n",
            "Epoch 149/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9129 - val_loss: 0.1824 - val_accuracy: 0.9160\n",
            "Epoch 150/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9136 - val_loss: 0.1816 - val_accuracy: 0.9165\n",
            "Epoch 151/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9151 - val_loss: 0.1757 - val_accuracy: 0.9162\n",
            "Epoch 152/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1834 - accuracy: 0.9135 - val_loss: 0.1761 - val_accuracy: 0.9165\n",
            "Epoch 153/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9141 - val_loss: 0.1777 - val_accuracy: 0.9161\n",
            "Epoch 154/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1787 - accuracy: 0.9136 - val_loss: 0.1791 - val_accuracy: 0.9177\n",
            "Epoch 155/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9120 - val_loss: 0.1755 - val_accuracy: 0.9169\n",
            "Epoch 156/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9138 - val_loss: 0.1790 - val_accuracy: 0.9158\n",
            "Epoch 157/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9113 - val_loss: 0.1802 - val_accuracy: 0.9174\n",
            "Epoch 158/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9142 - val_loss: 0.1761 - val_accuracy: 0.9170\n",
            "Epoch 159/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1795 - accuracy: 0.9151 - val_loss: 0.1760 - val_accuracy: 0.9164\n",
            "Epoch 160/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9152 - val_loss: 0.1761 - val_accuracy: 0.9165\n",
            "Epoch 161/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.9159 - val_loss: 0.1785 - val_accuracy: 0.9161\n",
            "Epoch 162/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9127 - val_loss: 0.1768 - val_accuracy: 0.9170\n",
            "Epoch 163/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.9149 - val_loss: 0.1763 - val_accuracy: 0.9168\n",
            "Epoch 164/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9118 - val_loss: 0.1770 - val_accuracy: 0.9173\n",
            "Epoch 165/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9179 - val_loss: 0.1759 - val_accuracy: 0.9166\n",
            "Epoch 166/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1814 - accuracy: 0.9146 - val_loss: 0.1766 - val_accuracy: 0.9170\n",
            "Epoch 167/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1820 - accuracy: 0.9140 - val_loss: 0.1761 - val_accuracy: 0.9165\n",
            "Epoch 168/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9129 - val_loss: 0.1768 - val_accuracy: 0.9177\n",
            "Epoch 169/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1820 - accuracy: 0.9148 - val_loss: 0.1762 - val_accuracy: 0.9161\n",
            "Epoch 170/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.9148 - val_loss: 0.1766 - val_accuracy: 0.9167\n",
            "Epoch 171/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9165 - val_loss: 0.1788 - val_accuracy: 0.9166\n",
            "Epoch 172/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.9141 - val_loss: 0.1753 - val_accuracy: 0.9170\n",
            "Epoch 173/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1824 - accuracy: 0.9123 - val_loss: 0.1756 - val_accuracy: 0.9169\n",
            "Epoch 174/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9148 - val_loss: 0.1751 - val_accuracy: 0.9170\n",
            "Epoch 175/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1786 - accuracy: 0.9155 - val_loss: 0.1760 - val_accuracy: 0.9177\n",
            "Epoch 176/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1804 - accuracy: 0.9131 - val_loss: 0.1752 - val_accuracy: 0.9167\n",
            "Epoch 177/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9112 - val_loss: 0.1760 - val_accuracy: 0.9173\n",
            "Epoch 178/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9138 - val_loss: 0.1766 - val_accuracy: 0.9165\n",
            "Epoch 179/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.9185 - val_loss: 0.1753 - val_accuracy: 0.9171\n",
            "Epoch 180/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1731 - accuracy: 0.9187 - val_loss: 0.1776 - val_accuracy: 0.9157\n",
            "Epoch 181/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9148 - val_loss: 0.1757 - val_accuracy: 0.9173\n",
            "Epoch 182/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9165 - val_loss: 0.1767 - val_accuracy: 0.9167\n",
            "Epoch 183/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9127 - val_loss: 0.1766 - val_accuracy: 0.9173\n",
            "Epoch 184/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9126 - val_loss: 0.1753 - val_accuracy: 0.9172\n",
            "Epoch 185/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9127 - val_loss: 0.1763 - val_accuracy: 0.9173\n",
            "Epoch 186/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1792 - accuracy: 0.9148 - val_loss: 0.1855 - val_accuracy: 0.9147\n",
            "Epoch 187/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9163 - val_loss: 0.1750 - val_accuracy: 0.9176\n",
            "Epoch 188/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9140 - val_loss: 0.1766 - val_accuracy: 0.9156\n",
            "Epoch 189/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9150 - val_loss: 0.1779 - val_accuracy: 0.9178\n",
            "Epoch 190/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1760 - accuracy: 0.9173 - val_loss: 0.1760 - val_accuracy: 0.9171\n",
            "Epoch 191/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9126 - val_loss: 0.1756 - val_accuracy: 0.9173\n",
            "Epoch 192/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9178 - val_loss: 0.1759 - val_accuracy: 0.9160\n",
            "Epoch 193/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1803 - accuracy: 0.9160 - val_loss: 0.1792 - val_accuracy: 0.9157\n",
            "Epoch 194/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9116 - val_loss: 0.1762 - val_accuracy: 0.9167\n",
            "Epoch 195/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9116 - val_loss: 0.1764 - val_accuracy: 0.9177\n",
            "Epoch 196/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9155 - val_loss: 0.1758 - val_accuracy: 0.9166\n",
            "Epoch 197/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9168 - val_loss: 0.1752 - val_accuracy: 0.9168\n",
            "Epoch 198/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.9159 - val_loss: 0.1767 - val_accuracy: 0.9181\n",
            "Epoch 199/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1764 - accuracy: 0.9172 - val_loss: 0.1757 - val_accuracy: 0.9175\n",
            "Epoch 200/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9163 - val_loss: 0.1764 - val_accuracy: 0.9166\n",
            "Epoch 201/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9137 - val_loss: 0.1763 - val_accuracy: 0.9178\n",
            "Epoch 202/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9137 - val_loss: 0.1752 - val_accuracy: 0.9165\n",
            "Epoch 203/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9155 - val_loss: 0.1767 - val_accuracy: 0.9167\n",
            "Epoch 204/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1742 - accuracy: 0.9177 - val_loss: 0.1751 - val_accuracy: 0.9162\n",
            "Epoch 205/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1791 - accuracy: 0.9160 - val_loss: 0.1753 - val_accuracy: 0.9169\n",
            "Epoch 206/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1814 - accuracy: 0.9127 - val_loss: 0.1757 - val_accuracy: 0.9169\n",
            "Epoch 207/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1803 - accuracy: 0.9144 - val_loss: 0.1752 - val_accuracy: 0.9171\n",
            "Epoch 208/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9140 - val_loss: 0.1788 - val_accuracy: 0.9171\n",
            "Epoch 209/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9150 - val_loss: 0.1755 - val_accuracy: 0.9169\n",
            "Epoch 210/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1835 - accuracy: 0.9113 - val_loss: 0.1763 - val_accuracy: 0.9166\n",
            "Epoch 211/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9163 - val_loss: 0.1836 - val_accuracy: 0.9148\n",
            "Epoch 212/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9119 - val_loss: 0.1751 - val_accuracy: 0.9169\n",
            "Epoch 213/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9145 - val_loss: 0.1756 - val_accuracy: 0.9165\n",
            "Epoch 214/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9119 - val_loss: 0.1814 - val_accuracy: 0.9157\n",
            "Epoch 215/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.9167 - val_loss: 0.1801 - val_accuracy: 0.9160\n",
            "Epoch 216/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9161 - val_loss: 0.1787 - val_accuracy: 0.9165\n",
            "Epoch 217/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.9165 - val_loss: 0.1756 - val_accuracy: 0.9174\n",
            "Epoch 218/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9136 - val_loss: 0.1750 - val_accuracy: 0.9169\n",
            "Epoch 219/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1837 - accuracy: 0.9134 - val_loss: 0.1773 - val_accuracy: 0.9177\n",
            "Epoch 220/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9129 - val_loss: 0.1787 - val_accuracy: 0.9173\n",
            "Epoch 221/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9140 - val_loss: 0.1755 - val_accuracy: 0.9168\n",
            "Epoch 222/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9135 - val_loss: 0.1750 - val_accuracy: 0.9168\n",
            "Epoch 223/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1826 - accuracy: 0.9121 - val_loss: 0.1756 - val_accuracy: 0.9164\n",
            "Epoch 224/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9130 - val_loss: 0.1751 - val_accuracy: 0.9170\n",
            "Epoch 225/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9139 - val_loss: 0.1755 - val_accuracy: 0.9170\n",
            "Epoch 226/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9149 - val_loss: 0.1753 - val_accuracy: 0.9166\n",
            "Epoch 227/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1810 - accuracy: 0.9154 - val_loss: 0.1752 - val_accuracy: 0.9171\n",
            "Epoch 228/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9147 - val_loss: 0.1781 - val_accuracy: 0.9173\n",
            "Epoch 229/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1808 - accuracy: 0.9150 - val_loss: 0.1797 - val_accuracy: 0.9156\n",
            "Epoch 230/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9106 - val_loss: 0.1778 - val_accuracy: 0.9156\n",
            "Epoch 231/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9180 - val_loss: 0.1763 - val_accuracy: 0.9165\n",
            "Epoch 232/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1797 - accuracy: 0.9169 - val_loss: 0.1756 - val_accuracy: 0.9165\n",
            "Epoch 233/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1806 - accuracy: 0.9140 - val_loss: 0.1788 - val_accuracy: 0.9166\n",
            "Epoch 234/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9158 - val_loss: 0.1764 - val_accuracy: 0.9162\n",
            "Epoch 235/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9156 - val_loss: 0.1756 - val_accuracy: 0.9180\n",
            "Epoch 236/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.9141 - val_loss: 0.1752 - val_accuracy: 0.9176\n",
            "Epoch 237/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9159 - val_loss: 0.1748 - val_accuracy: 0.9169\n",
            "Epoch 238/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1806 - accuracy: 0.9149 - val_loss: 0.1794 - val_accuracy: 0.9177\n",
            "Epoch 239/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9143 - val_loss: 0.1780 - val_accuracy: 0.9174\n",
            "Epoch 240/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1758 - accuracy: 0.9163 - val_loss: 0.1759 - val_accuracy: 0.9167\n",
            "Epoch 241/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.9163 - val_loss: 0.1767 - val_accuracy: 0.9161\n",
            "Epoch 242/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1804 - accuracy: 0.9138 - val_loss: 0.1762 - val_accuracy: 0.9158\n",
            "Epoch 243/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9122 - val_loss: 0.1754 - val_accuracy: 0.9169\n",
            "Epoch 244/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9096 - val_loss: 0.1758 - val_accuracy: 0.9161\n",
            "Epoch 245/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9143 - val_loss: 0.1757 - val_accuracy: 0.9169\n",
            "Epoch 246/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9127 - val_loss: 0.1750 - val_accuracy: 0.9169\n",
            "Epoch 247/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1834 - accuracy: 0.9093 - val_loss: 0.1801 - val_accuracy: 0.9156\n",
            "Epoch 248/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9136 - val_loss: 0.1776 - val_accuracy: 0.9176\n",
            "Epoch 249/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1824 - accuracy: 0.9138 - val_loss: 0.1750 - val_accuracy: 0.9168\n",
            "Epoch 250/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9151 - val_loss: 0.1770 - val_accuracy: 0.9152\n",
            "Epoch 251/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9162 - val_loss: 0.1759 - val_accuracy: 0.9180\n",
            "Epoch 252/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1782 - accuracy: 0.9149 - val_loss: 0.1754 - val_accuracy: 0.9178\n",
            "Epoch 253/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9157 - val_loss: 0.1754 - val_accuracy: 0.9167\n",
            "Epoch 254/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9144 - val_loss: 0.1747 - val_accuracy: 0.9173\n",
            "Epoch 255/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9115 - val_loss: 0.1768 - val_accuracy: 0.9177\n",
            "Epoch 256/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1786 - accuracy: 0.9169 - val_loss: 0.1754 - val_accuracy: 0.9163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "fcdsMAjfQKnw",
        "outputId": "bf135850-8e76-4a91-89ec-5caafdce2dcc"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iet1_5BT4af",
        "outputId": "b73e68d5-2537-4ced-df7e-794a9bd6b70d"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "p = model_bst1.predict(XVALID)\n",
        "accuracy = model_bst1.evaluate(XVALID, YVALID)\n",
        "model_accuracy = accuracy_score(YVALID, p.round())\n",
        "\n",
        "model_precision =  precision_score (YVALID, p.round())\n",
        "\n",
        "recall_value = recall_score (YVALID, p.round())\n",
        "\n",
        "model_flscore = f1_score(YVALID, p.round())\n",
        "\n",
        "print(\"Accuracy Value: %.2f%%\" % (model_accuracy))\n",
        "\n",
        "print(\"Precision Value: %.2f%%\" % (model_precision))\n",
        "\n",
        "print(\"Recall Value: %.2f%%\" % (recall_value))\n",
        "\n",
        "print(\"Fl-score Value: %.2f\\n\" % (model_flscore))\n",
        "model_bst1.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "386/386 [==============================] - 0s 948us/step - loss: 0.1754 - accuracy: 0.9163\n",
            "Accuracy Value: 0.92%\n",
            "Precision Value: 0.62%\n",
            "Recall Value: 0.54%\n",
            "Fl-score Value: 0.58\n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 8)                 168       \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 209\n",
            "Trainable params: 209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Function**"
      ],
      "metadata": {
        "id": "urJKJwFxZM2H"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onYMgt5JQma2"
      },
      "source": [
        "def linear(x):\n",
        "\treturn np.maximum(0, x)\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoidFunction(x):\n",
        "    return 1 / (1 + np.exp(np.negative(x)))\n",
        "\n",
        "def custom_model(model, validXData, validYData, display_weight=None):\n",
        "    prediction = validXData\n",
        "    layTotal = len(model.layers)\n",
        "\n",
        "    for Num_lay, lay in enumerate(model.layers):\n",
        "        weights = lay.get_weights()[0].T\n",
        "        biases = lay.get_weights()[1].T\n",
        "        if display_weight is not None:\n",
        "            print(\"Layer #%s\" %Num_lay)\n",
        "            print(\"Weights:\\n %s\" % weights)\n",
        "            print(\"Bias:\\n %s\\n\" % biases)\n",
        "\n",
        "        finalValue = []\n",
        "        for row in prediction:\n",
        "            value = []\n",
        "            for i, w in enumerate(weights):                \n",
        "                val = np.dot(w, row) + biases[i]\n",
        "                value.append(val)\n",
        "\n",
        "            if Num_lay < layTotal - 1:\n",
        "                finalValue.append(linear(value))\n",
        "            else:\n",
        "                finalValue.append(sigmoidFunction(value))\n",
        "            \n",
        "        # Update new input value\n",
        "        prediction = np.array(finalValue)\n",
        "\n",
        "    accuracy = accuracy_score(validYData, prediction.round()) * 100.0\n",
        "    precision = precision_score(validYData, prediction.round()) * 100.0\n",
        "    recall = recall_score(validYData, prediction.round()) * 100.0\n",
        "    f1score = f1_score(validYData, prediction.round()) * 100.0\n",
        "\n",
        "    print(\"Accuracy: %.2f%%\" % (accuracy))\n",
        "    print(\"Precision: %.2f%%\" % (precision))\n",
        "    print(\"Recall: %.2f%%\" % (recall))\n",
        "    print(\"F1-score: %.2f\\n\" % (f1score))\n",
        "\n",
        "    print(\"Custom Model Predicted Values\")\n",
        "    print(\"===============================================================\")\n",
        "    for i in range(5):\n",
        "\t    print(\"X=%s, Predicted=%s\" % (validXData[i], prediction[i]))\n",
        "\n",
        "    print(\"Keras Model Predicted Values\") \n",
        "    print(\"========================================================================\")\n",
        "    for i in range(5):\n",
        "\t    print(\"X=%s, Y=%s\" % (validXData[i], model_bst1.predict(validXData)[i]))\n",
        "    \n",
        "    print(\"Actual Xvalid Yvalid Values\") \n",
        "    print(\"======================================================================\")\n",
        "    for i in range(5):\n",
        "\t    print(\"X=%s, Y=%s\" % (validXData[i], validYData[i]))\n",
        "    return prediction\n",
        "custom_model(model_bst1,XVALID,YVALID,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q7Im96fyCkic",
        "outputId": "2473ca4d-2926-4db2-a0e0-15a05da99bcd"
      },
      "source": [
        "\n",
        "max_array = []\n",
        "min_array =[]\n",
        "size = dataset.shape\n",
        "column= size[1]\n",
        "for i in range(column):\n",
        "  min = dataset[:,i].min(axis=0)\n",
        "  max = dataset[:,i].max(axis=0)\n",
        "  max_array.append(max)\n",
        "  min_array.append(min)\n",
        "  dataset[:,i] = (dataset[:,i]-min)/(max-min)\n",
        "X1 = dataset[:,:]\n",
        "Y1 = dataset[:,-1]\n",
        "#Data Spliting#Data Spliting\n",
        "np.random.shuffle(dataset)\n",
        "\n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "XVALID = X1[:index]\n",
        "YVALID = Y1[:index]\n",
        "XTRAIN = X1[index:]\n",
        "YTRAIN = Y1[index:]\n",
        "\n",
        "model_b = Sequential()\n",
        "model_b.add(Dense(8, input_dim = 21,activation='relu'))\n",
        "model_b.add(Dense(4,activation='relu'))\n",
        "model_b.add(Dense(1,activation='sigmoid'))\n",
        "model_b.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_b.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3310 - accuracy: 0.8478 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.9768e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.6628e-04 - accuracy: 1.0000 - val_loss: 2.0579e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.5237e-04 - accuracy: 1.0000 - val_loss: 9.9344e-05 - val_accuracy: 1.0000\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 7.1829e-05 - accuracy: 1.0000 - val_loss: 5.2384e-05 - val_accuracy: 1.0000\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.7422e-05 - accuracy: 1.0000 - val_loss: 2.8859e-05 - val_accuracy: 1.0000\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.0379e-05 - accuracy: 1.0000 - val_loss: 1.6535e-05 - val_accuracy: 1.0000\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.0979e-05 - accuracy: 1.0000 - val_loss: 9.7892e-06 - val_accuracy: 1.0000\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 6.3042e-06 - accuracy: 1.0000 - val_loss: 5.9005e-06 - val_accuracy: 1.0000\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.5896e-06 - accuracy: 1.0000 - val_loss: 3.4555e-06 - val_accuracy: 1.0000\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.1356e-06 - accuracy: 1.0000 - val_loss: 2.1053e-06 - val_accuracy: 1.0000\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.2233e-06 - accuracy: 1.0000 - val_loss: 1.3045e-06 - val_accuracy: 1.0000\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 7.7692e-07 - accuracy: 1.0000 - val_loss: 8.0475e-07 - val_accuracy: 1.0000\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 4.3996e-07 - accuracy: 1.0000 - val_loss: 4.8256e-07 - val_accuracy: 1.0000\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.6935e-07 - accuracy: 1.0000 - val_loss: 3.1077e-07 - val_accuracy: 1.0000\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.5555e-07 - accuracy: 1.0000 - val_loss: 1.8238e-07 - val_accuracy: 1.0000\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 9.2213e-08 - accuracy: 1.0000 - val_loss: 1.1532e-07 - val_accuracy: 1.0000\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 6.0472e-08 - accuracy: 1.0000 - val_loss: 7.3007e-08 - val_accuracy: 1.0000\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.5798e-08 - accuracy: 1.0000 - val_loss: 4.4636e-08 - val_accuracy: 1.0000\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2644e-08 - accuracy: 1.0000 - val_loss: 2.9563e-08 - val_accuracy: 1.0000\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 1.4109e-08 - accuracy: 1.0000 - val_loss: 1.8300e-08 - val_accuracy: 1.0000\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 8.5729e-09 - accuracy: 1.0000 - val_loss: 1.1738e-08 - val_accuracy: 1.0000\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 5.8639e-09 - accuracy: 1.0000 - val_loss: 7.9568e-09 - val_accuracy: 1.0000\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.8877e-09 - accuracy: 1.0000 - val_loss: 5.6926e-09 - val_accuracy: 1.0000\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.8316e-09 - accuracy: 1.0000 - val_loss: 4.1290e-09 - val_accuracy: 1.0000\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 2.1761e-09 - accuracy: 1.0000 - val_loss: 3.1634e-09 - val_accuracy: 1.0000\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.8048e-09 - accuracy: 1.0000 - val_loss: 2.4901e-09 - val_accuracy: 1.0000\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.4257e-09 - accuracy: 1.0000 - val_loss: 2.0150e-09 - val_accuracy: 1.0000\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.1768e-09 - accuracy: 1.0000 - val_loss: 1.6962e-09 - val_accuracy: 1.0000\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.0030e-09 - accuracy: 1.0000 - val_loss: 1.4467e-09 - val_accuracy: 1.0000\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 9.3990e-10 - accuracy: 1.0000 - val_loss: 1.2861e-09 - val_accuracy: 1.0000\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 8.2514e-10 - accuracy: 1.0000 - val_loss: 1.1637e-09 - val_accuracy: 1.0000\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 7.5361e-10 - accuracy: 1.0000 - val_loss: 1.0503e-09 - val_accuracy: 1.0000\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 6.7595e-10 - accuracy: 1.0000 - val_loss: 9.8302e-10 - val_accuracy: 1.0000\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 6.9177e-10 - accuracy: 1.0000 - val_loss: 8.8706e-10 - val_accuracy: 1.0000\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 6.1987e-10 - accuracy: 1.0000 - val_loss: 8.3662e-10 - val_accuracy: 1.0000\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 6.1520e-10 - accuracy: 1.0000 - val_loss: 7.6352e-10 - val_accuracy: 1.0000\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 5.2718e-10 - accuracy: 1.0000 - val_loss: 7.1807e-10 - val_accuracy: 1.0000\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 5.2347e-10 - accuracy: 1.0000 - val_loss: 6.7443e-10 - val_accuracy: 1.0000\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 4.9492e-10 - accuracy: 1.0000 - val_loss: 6.3865e-10 - val_accuracy: 1.0000\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 4.5024e-10 - accuracy: 1.0000 - val_loss: 6.1373e-10 - val_accuracy: 1.0000\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 4.3111e-10 - accuracy: 1.0000 - val_loss: 5.8382e-10 - val_accuracy: 1.0000\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 4.1101e-10 - accuracy: 1.0000 - val_loss: 5.6644e-10 - val_accuracy: 1.0000\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 4.1217e-10 - accuracy: 1.0000 - val_loss: 5.4365e-10 - val_accuracy: 1.0000\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.9593e-10 - accuracy: 1.0000 - val_loss: 5.2717e-10 - val_accuracy: 1.0000\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.9586e-10 - accuracy: 1.0000 - val_loss: 5.1274e-10 - val_accuracy: 1.0000\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.6510e-10 - accuracy: 1.0000 - val_loss: 4.9934e-10 - val_accuracy: 1.0000\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.6842e-10 - accuracy: 1.0000 - val_loss: 4.8819e-10 - val_accuracy: 1.0000\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.6474e-10 - accuracy: 1.0000 - val_loss: 4.7804e-10 - val_accuracy: 1.0000\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.5609e-10 - accuracy: 1.0000 - val_loss: 4.6381e-10 - val_accuracy: 1.0000\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.4908e-10 - accuracy: 1.0000 - val_loss: 4.5080e-10 - val_accuracy: 1.0000\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.5178e-10 - accuracy: 1.0000 - val_loss: 4.4429e-10 - val_accuracy: 1.0000\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.3233e-10 - accuracy: 1.0000 - val_loss: 4.3694e-10 - val_accuracy: 1.0000\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.3808e-10 - accuracy: 1.0000 - val_loss: 4.3063e-10 - val_accuracy: 1.0000\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.2318e-10 - accuracy: 1.0000 - val_loss: 4.3076e-10 - val_accuracy: 1.0000\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.2703e-10 - accuracy: 1.0000 - val_loss: 4.2045e-10 - val_accuracy: 1.0000\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.1177e-10 - accuracy: 1.0000 - val_loss: 4.0907e-10 - val_accuracy: 1.0000\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.2601e-10 - accuracy: 1.0000 - val_loss: 4.0266e-10 - val_accuracy: 1.0000\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.1235e-10 - accuracy: 1.0000 - val_loss: 3.9514e-10 - val_accuracy: 1.0000\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.9711e-10 - accuracy: 1.0000 - val_loss: 3.9396e-10 - val_accuracy: 1.0000\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.0991e-10 - accuracy: 1.0000 - val_loss: 3.9233e-10 - val_accuracy: 1.0000\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 3.0599e-10 - accuracy: 1.0000 - val_loss: 3.7643e-10 - val_accuracy: 1.0000\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.8874e-10 - accuracy: 1.0000 - val_loss: 3.7381e-10 - val_accuracy: 1.0000\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.9172e-10 - accuracy: 1.0000 - val_loss: 3.7079e-10 - val_accuracy: 1.0000\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.9045e-10 - accuracy: 1.0000 - val_loss: 3.6689e-10 - val_accuracy: 1.0000\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.9478e-10 - accuracy: 1.0000 - val_loss: 3.6277e-10 - val_accuracy: 1.0000\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.8981e-10 - accuracy: 1.0000 - val_loss: 3.5138e-10 - val_accuracy: 1.0000\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.8330e-10 - accuracy: 1.0000 - val_loss: 3.4702e-10 - val_accuracy: 1.0000\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.7951e-10 - accuracy: 1.0000 - val_loss: 3.4207e-10 - val_accuracy: 1.0000\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.6119e-10 - accuracy: 1.0000 - val_loss: 3.3695e-10 - val_accuracy: 1.0000\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.6914e-10 - accuracy: 1.0000 - val_loss: 3.3107e-10 - val_accuracy: 1.0000\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.6253e-10 - accuracy: 1.0000 - val_loss: 3.2595e-10 - val_accuracy: 1.0000\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.5938e-10 - accuracy: 1.0000 - val_loss: 3.2012e-10 - val_accuracy: 1.0000\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.5423e-10 - accuracy: 1.0000 - val_loss: 3.1876e-10 - val_accuracy: 1.0000\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.5399e-10 - accuracy: 1.0000 - val_loss: 3.1675e-10 - val_accuracy: 1.0000\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.5610e-10 - accuracy: 1.0000 - val_loss: 3.1719e-10 - val_accuracy: 1.0000\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.4763e-10 - accuracy: 1.0000 - val_loss: 3.1225e-10 - val_accuracy: 1.0000\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.5121e-10 - accuracy: 1.0000 - val_loss: 3.0933e-10 - val_accuracy: 1.0000\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.4440e-10 - accuracy: 1.0000 - val_loss: 3.0652e-10 - val_accuracy: 1.0000\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.4853e-10 - accuracy: 1.0000 - val_loss: 3.0338e-10 - val_accuracy: 1.0000\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.3127e-10 - accuracy: 1.0000 - val_loss: 3.0013e-10 - val_accuracy: 1.0000\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.4661e-10 - accuracy: 1.0000 - val_loss: 2.9707e-10 - val_accuracy: 1.0000\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3771e-10 - accuracy: 1.0000 - val_loss: 2.9717e-10 - val_accuracy: 1.0000\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.4422e-10 - accuracy: 1.0000 - val_loss: 2.9448e-10 - val_accuracy: 1.0000\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3335e-10 - accuracy: 1.0000 - val_loss: 2.9320e-10 - val_accuracy: 1.0000\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3636e-10 - accuracy: 1.0000 - val_loss: 2.9473e-10 - val_accuracy: 1.0000\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3015e-10 - accuracy: 1.0000 - val_loss: 2.9062e-10 - val_accuracy: 1.0000\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.3745e-10 - accuracy: 1.0000 - val_loss: 2.8954e-10 - val_accuracy: 1.0000\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.4040e-10 - accuracy: 1.0000 - val_loss: 2.9173e-10 - val_accuracy: 1.0000\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3703e-10 - accuracy: 1.0000 - val_loss: 2.8770e-10 - val_accuracy: 1.0000\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3236e-10 - accuracy: 1.0000 - val_loss: 2.8654e-10 - val_accuracy: 1.0000\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.4511e-10 - accuracy: 1.0000 - val_loss: 2.9130e-10 - val_accuracy: 1.0000\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3211e-10 - accuracy: 1.0000 - val_loss: 2.8536e-10 - val_accuracy: 1.0000\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3571e-10 - accuracy: 1.0000 - val_loss: 2.8379e-10 - val_accuracy: 1.0000\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3465e-10 - accuracy: 1.0000 - val_loss: 2.8185e-10 - val_accuracy: 1.0000\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.3008e-10 - accuracy: 1.0000 - val_loss: 2.8002e-10 - val_accuracy: 1.0000\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.2191e-10 - accuracy: 1.0000 - val_loss: 2.7774e-10 - val_accuracy: 1.0000\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3298e-10 - accuracy: 1.0000 - val_loss: 2.8186e-10 - val_accuracy: 1.0000\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.4080e-10 - accuracy: 1.0000 - val_loss: 2.8560e-10 - val_accuracy: 1.0000\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3923e-10 - accuracy: 1.0000 - val_loss: 2.7853e-10 - val_accuracy: 1.0000\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3379e-10 - accuracy: 1.0000 - val_loss: 2.7588e-10 - val_accuracy: 1.0000\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3125e-10 - accuracy: 1.0000 - val_loss: 2.7926e-10 - val_accuracy: 1.0000\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3753e-10 - accuracy: 1.0000 - val_loss: 2.7625e-10 - val_accuracy: 1.0000\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.3468e-10 - accuracy: 1.0000 - val_loss: 2.7326e-10 - val_accuracy: 1.0000\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2679e-10 - accuracy: 1.0000 - val_loss: 2.7610e-10 - val_accuracy: 1.0000\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.2892e-10 - accuracy: 1.0000 - val_loss: 2.7290e-10 - val_accuracy: 1.0000\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2427e-10 - accuracy: 1.0000 - val_loss: 2.7643e-10 - val_accuracy: 1.0000\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2864e-10 - accuracy: 1.0000 - val_loss: 2.7215e-10 - val_accuracy: 1.0000\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2994e-10 - accuracy: 1.0000 - val_loss: 2.6855e-10 - val_accuracy: 1.0000\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2188e-10 - accuracy: 1.0000 - val_loss: 2.7109e-10 - val_accuracy: 1.0000\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.1600e-10 - accuracy: 1.0000 - val_loss: 2.6880e-10 - val_accuracy: 1.0000\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.1750e-10 - accuracy: 1.0000 - val_loss: 2.6960e-10 - val_accuracy: 1.0000\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2407e-10 - accuracy: 1.0000 - val_loss: 2.7188e-10 - val_accuracy: 1.0000\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3508e-10 - accuracy: 1.0000 - val_loss: 2.6805e-10 - val_accuracy: 1.0000\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.3260e-10 - accuracy: 1.0000 - val_loss: 2.6983e-10 - val_accuracy: 1.0000\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.1716e-10 - accuracy: 1.0000 - val_loss: 2.6559e-10 - val_accuracy: 1.0000\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.1627e-10 - accuracy: 1.0000 - val_loss: 2.6745e-10 - val_accuracy: 1.0000\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.1936e-10 - accuracy: 1.0000 - val_loss: 2.6313e-10 - val_accuracy: 1.0000\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3594e-10 - accuracy: 1.0000 - val_loss: 2.6487e-10 - val_accuracy: 1.0000\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2193e-10 - accuracy: 1.0000 - val_loss: 2.6659e-10 - val_accuracy: 1.0000\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.2090e-10 - accuracy: 1.0000 - val_loss: 2.6190e-10 - val_accuracy: 1.0000\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.2069e-10 - accuracy: 1.0000 - val_loss: 2.6339e-10 - val_accuracy: 1.0000\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.1449e-10 - accuracy: 1.0000 - val_loss: 2.6483e-10 - val_accuracy: 1.0000\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3313e-10 - accuracy: 1.0000 - val_loss: 2.6632e-10 - val_accuracy: 1.0000\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2492e-10 - accuracy: 1.0000 - val_loss: 2.6118e-10 - val_accuracy: 1.0000\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.3031e-10 - accuracy: 1.0000 - val_loss: 2.6240e-10 - val_accuracy: 1.0000\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 2.2880e-10 - accuracy: 1.0000 - val_loss: 2.6359e-10 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "_CDqd_0HD4Ku",
        "outputId": "ad347855-0d01-4513-8f6b-0f2a287da5ed"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZtD3MUYzN4N_",
        "outputId": "e5af3247-b5ae-4948-e7ee-abb241ec7a09"
      },
      "source": [
        "\n",
        "model_b = Sequential()\n",
        "model_b.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_b.add(Dense(4,activation='relu'))\n",
        "model_b.add(Dense(1,activation='sigmoid'))\n",
        "model_b.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_b.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3372 - accuracy: 0.8847 - val_loss: 0.2668 - val_accuracy: 0.8927\n",
            "Epoch 2/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2721 - accuracy: 0.8915 - val_loss: 0.2410 - val_accuracy: 0.9074\n",
            "Epoch 3/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2379 - accuracy: 0.9047 - val_loss: 0.2121 - val_accuracy: 0.9123\n",
            "Epoch 4/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2176 - accuracy: 0.9097 - val_loss: 0.2051 - val_accuracy: 0.9136\n",
            "Epoch 5/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2113 - accuracy: 0.9103 - val_loss: 0.2014 - val_accuracy: 0.9153\n",
            "Epoch 6/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2115 - accuracy: 0.9110 - val_loss: 0.1969 - val_accuracy: 0.9152\n",
            "Epoch 7/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2041 - accuracy: 0.9107 - val_loss: 0.1982 - val_accuracy: 0.9155\n",
            "Epoch 8/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2020 - accuracy: 0.9115 - val_loss: 0.1947 - val_accuracy: 0.9145\n",
            "Epoch 9/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9081 - val_loss: 0.1916 - val_accuracy: 0.9152\n",
            "Epoch 10/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9118 - val_loss: 0.1910 - val_accuracy: 0.9154\n",
            "Epoch 11/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1998 - accuracy: 0.9127 - val_loss: 0.1904 - val_accuracy: 0.9147\n",
            "Epoch 12/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9119 - val_loss: 0.1900 - val_accuracy: 0.9152\n",
            "Epoch 13/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9131 - val_loss: 0.1917 - val_accuracy: 0.9146\n",
            "Epoch 14/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1995 - accuracy: 0.9112 - val_loss: 0.1914 - val_accuracy: 0.9141\n",
            "Epoch 15/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1975 - accuracy: 0.9105 - val_loss: 0.1867 - val_accuracy: 0.9152\n",
            "Epoch 16/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1985 - accuracy: 0.9094 - val_loss: 0.1892 - val_accuracy: 0.9141\n",
            "Epoch 17/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1943 - accuracy: 0.9116 - val_loss: 0.1923 - val_accuracy: 0.9145\n",
            "Epoch 18/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9091 - val_loss: 0.1855 - val_accuracy: 0.9168\n",
            "Epoch 19/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9136 - val_loss: 0.1868 - val_accuracy: 0.9148\n",
            "Epoch 20/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1944 - accuracy: 0.9120 - val_loss: 0.1850 - val_accuracy: 0.9162\n",
            "Epoch 21/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1932 - accuracy: 0.9133 - val_loss: 0.2080 - val_accuracy: 0.9088\n",
            "Epoch 22/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9129 - val_loss: 0.1879 - val_accuracy: 0.9148\n",
            "Epoch 23/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9111 - val_loss: 0.1851 - val_accuracy: 0.9150\n",
            "Epoch 24/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9145 - val_loss: 0.1943 - val_accuracy: 0.9145\n",
            "Epoch 25/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1928 - accuracy: 0.9103 - val_loss: 0.1838 - val_accuracy: 0.9154\n",
            "Epoch 26/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9136 - val_loss: 0.1895 - val_accuracy: 0.9135\n",
            "Epoch 27/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1960 - accuracy: 0.9110 - val_loss: 0.1842 - val_accuracy: 0.9148\n",
            "Epoch 28/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1949 - accuracy: 0.9105 - val_loss: 0.1856 - val_accuracy: 0.9148\n",
            "Epoch 29/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9107 - val_loss: 0.1837 - val_accuracy: 0.9156\n",
            "Epoch 30/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9130 - val_loss: 0.1866 - val_accuracy: 0.9126\n",
            "Epoch 31/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9115 - val_loss: 0.1825 - val_accuracy: 0.9159\n",
            "Epoch 32/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9116 - val_loss: 0.1823 - val_accuracy: 0.9148\n",
            "Epoch 33/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9126 - val_loss: 0.1901 - val_accuracy: 0.9115\n",
            "Epoch 34/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9092 - val_loss: 0.1827 - val_accuracy: 0.9153\n",
            "Epoch 35/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9108 - val_loss: 0.1828 - val_accuracy: 0.9157\n",
            "Epoch 36/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9108 - val_loss: 0.1818 - val_accuracy: 0.9138\n",
            "Epoch 37/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9128 - val_loss: 0.1818 - val_accuracy: 0.9138\n",
            "Epoch 38/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9096 - val_loss: 0.1844 - val_accuracy: 0.9132\n",
            "Epoch 39/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9108 - val_loss: 0.1870 - val_accuracy: 0.9124\n",
            "Epoch 40/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9101 - val_loss: 0.1817 - val_accuracy: 0.9132\n",
            "Epoch 41/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9126 - val_loss: 0.1813 - val_accuracy: 0.9125\n",
            "Epoch 42/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9123 - val_loss: 0.1841 - val_accuracy: 0.9134\n",
            "Epoch 43/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9109 - val_loss: 0.1823 - val_accuracy: 0.9124\n",
            "Epoch 44/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9114 - val_loss: 0.1890 - val_accuracy: 0.9127\n",
            "Epoch 45/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9067 - val_loss: 0.1814 - val_accuracy: 0.9132\n",
            "Epoch 46/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9120 - val_loss: 0.1812 - val_accuracy: 0.9134\n",
            "Epoch 47/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9107 - val_loss: 0.1807 - val_accuracy: 0.9120\n",
            "Epoch 48/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1804 - accuracy: 0.9154 - val_loss: 0.1804 - val_accuracy: 0.9138\n",
            "Epoch 49/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9099 - val_loss: 0.1852 - val_accuracy: 0.9126\n",
            "Epoch 50/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9131 - val_loss: 0.1805 - val_accuracy: 0.9129\n",
            "Epoch 51/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1831 - accuracy: 0.9130 - val_loss: 0.1818 - val_accuracy: 0.9134\n",
            "Epoch 52/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9119 - val_loss: 0.1814 - val_accuracy: 0.9134\n",
            "Epoch 53/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9113 - val_loss: 0.1804 - val_accuracy: 0.9133\n",
            "Epoch 54/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9111 - val_loss: 0.1813 - val_accuracy: 0.9116\n",
            "Epoch 55/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9063 - val_loss: 0.1813 - val_accuracy: 0.9136\n",
            "Epoch 56/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9110 - val_loss: 0.1805 - val_accuracy: 0.9126\n",
            "Epoch 57/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9104 - val_loss: 0.1803 - val_accuracy: 0.9143\n",
            "Epoch 58/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9132 - val_loss: 0.1807 - val_accuracy: 0.9136\n",
            "Epoch 59/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9106 - val_loss: 0.1801 - val_accuracy: 0.9135\n",
            "Epoch 60/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9133 - val_loss: 0.1804 - val_accuracy: 0.9131\n",
            "Epoch 61/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9147 - val_loss: 0.1791 - val_accuracy: 0.9148\n",
            "Epoch 62/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9088 - val_loss: 0.1810 - val_accuracy: 0.9130\n",
            "Epoch 63/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9099 - val_loss: 0.1799 - val_accuracy: 0.9139\n",
            "Epoch 64/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9109 - val_loss: 0.1899 - val_accuracy: 0.9122\n",
            "Epoch 65/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9097 - val_loss: 0.1786 - val_accuracy: 0.9139\n",
            "Epoch 66/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9132 - val_loss: 0.1793 - val_accuracy: 0.9129\n",
            "Epoch 67/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9143 - val_loss: 0.1809 - val_accuracy: 0.9135\n",
            "Epoch 68/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1802 - accuracy: 0.9141 - val_loss: 0.1800 - val_accuracy: 0.9139\n",
            "Epoch 69/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1812 - accuracy: 0.9144 - val_loss: 0.1789 - val_accuracy: 0.9155\n",
            "Epoch 70/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9130 - val_loss: 0.1834 - val_accuracy: 0.9139\n",
            "Epoch 71/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9158 - val_loss: 0.1838 - val_accuracy: 0.9131\n",
            "Epoch 72/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1820 - accuracy: 0.9116 - val_loss: 0.1783 - val_accuracy: 0.9140\n",
            "Epoch 73/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9118 - val_loss: 0.1785 - val_accuracy: 0.9141\n",
            "Epoch 74/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9144 - val_loss: 0.1783 - val_accuracy: 0.9146\n",
            "Epoch 75/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1805 - accuracy: 0.9133 - val_loss: 0.1825 - val_accuracy: 0.9119\n",
            "Epoch 76/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1826 - accuracy: 0.9150 - val_loss: 0.1792 - val_accuracy: 0.9143\n",
            "Epoch 77/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9151 - val_loss: 0.1807 - val_accuracy: 0.9157\n",
            "Epoch 78/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9135 - val_loss: 0.1834 - val_accuracy: 0.9142\n",
            "Epoch 79/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9120 - val_loss: 0.1786 - val_accuracy: 0.9142\n",
            "Epoch 80/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9162 - val_loss: 0.1777 - val_accuracy: 0.9154\n",
            "Epoch 81/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9154 - val_loss: 0.1792 - val_accuracy: 0.9131\n",
            "Epoch 82/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9119 - val_loss: 0.1787 - val_accuracy: 0.9148\n",
            "Epoch 83/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9153 - val_loss: 0.1774 - val_accuracy: 0.9148\n",
            "Epoch 84/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9116 - val_loss: 0.1782 - val_accuracy: 0.9152\n",
            "Epoch 85/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9117 - val_loss: 0.1773 - val_accuracy: 0.9146\n",
            "Epoch 86/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9133 - val_loss: 0.1785 - val_accuracy: 0.9142\n",
            "Epoch 87/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1808 - accuracy: 0.9123 - val_loss: 0.1775 - val_accuracy: 0.9152\n",
            "Epoch 88/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9106 - val_loss: 0.1769 - val_accuracy: 0.9157\n",
            "Epoch 89/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9133 - val_loss: 0.1772 - val_accuracy: 0.9142\n",
            "Epoch 90/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1812 - accuracy: 0.9146 - val_loss: 0.1769 - val_accuracy: 0.9148\n",
            "Epoch 91/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1727 - accuracy: 0.9175 - val_loss: 0.1771 - val_accuracy: 0.9151\n",
            "Epoch 92/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9159 - val_loss: 0.1837 - val_accuracy: 0.9127\n",
            "Epoch 93/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1820 - accuracy: 0.9137 - val_loss: 0.1787 - val_accuracy: 0.9144\n",
            "Epoch 94/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.9161 - val_loss: 0.1798 - val_accuracy: 0.9138\n",
            "Epoch 95/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9134 - val_loss: 0.1785 - val_accuracy: 0.9143\n",
            "Epoch 96/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1805 - accuracy: 0.9147 - val_loss: 0.1777 - val_accuracy: 0.9139\n",
            "Epoch 97/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9153 - val_loss: 0.1809 - val_accuracy: 0.9139\n",
            "Epoch 98/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9151 - val_loss: 0.1774 - val_accuracy: 0.9157\n",
            "Epoch 99/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9130 - val_loss: 0.1804 - val_accuracy: 0.9133\n",
            "Epoch 100/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9134 - val_loss: 0.1767 - val_accuracy: 0.9156\n",
            "Epoch 101/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9166 - val_loss: 0.1773 - val_accuracy: 0.9148\n",
            "Epoch 102/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9108 - val_loss: 0.1769 - val_accuracy: 0.9147\n",
            "Epoch 103/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9113 - val_loss: 0.1791 - val_accuracy: 0.9146\n",
            "Epoch 104/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9136 - val_loss: 0.1767 - val_accuracy: 0.9152\n",
            "Epoch 105/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9140 - val_loss: 0.1775 - val_accuracy: 0.9158\n",
            "Epoch 106/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9112 - val_loss: 0.1796 - val_accuracy: 0.9145\n",
            "Epoch 107/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9147 - val_loss: 0.1761 - val_accuracy: 0.9168\n",
            "Epoch 108/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9172 - val_loss: 0.1775 - val_accuracy: 0.9152\n",
            "Epoch 109/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9115 - val_loss: 0.1783 - val_accuracy: 0.9162\n",
            "Epoch 110/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9156 - val_loss: 0.1775 - val_accuracy: 0.9171\n",
            "Epoch 111/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9146 - val_loss: 0.1761 - val_accuracy: 0.9165\n",
            "Epoch 112/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9142 - val_loss: 0.1765 - val_accuracy: 0.9162\n",
            "Epoch 113/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9172 - val_loss: 0.1789 - val_accuracy: 0.9134\n",
            "Epoch 114/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9148 - val_loss: 0.1800 - val_accuracy: 0.9156\n",
            "Epoch 115/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9166 - val_loss: 0.1773 - val_accuracy: 0.9146\n",
            "Epoch 116/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1763 - accuracy: 0.9180 - val_loss: 0.1763 - val_accuracy: 0.9154\n",
            "Epoch 117/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9143 - val_loss: 0.1780 - val_accuracy: 0.9152\n",
            "Epoch 118/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9146 - val_loss: 0.1796 - val_accuracy: 0.9126\n",
            "Epoch 119/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9153 - val_loss: 0.1767 - val_accuracy: 0.9160\n",
            "Epoch 120/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9158 - val_loss: 0.1791 - val_accuracy: 0.9158\n",
            "Epoch 121/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9136 - val_loss: 0.1788 - val_accuracy: 0.9171\n",
            "Epoch 122/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9155 - val_loss: 0.1807 - val_accuracy: 0.9130\n",
            "Epoch 123/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9151 - val_loss: 0.1789 - val_accuracy: 0.9156\n",
            "Epoch 124/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9143 - val_loss: 0.1771 - val_accuracy: 0.9161\n",
            "Epoch 125/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9142 - val_loss: 0.1764 - val_accuracy: 0.9156\n",
            "Epoch 126/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9163 - val_loss: 0.1758 - val_accuracy: 0.9172\n",
            "Epoch 127/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9146 - val_loss: 0.1758 - val_accuracy: 0.9169\n",
            "Epoch 128/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9155 - val_loss: 0.1770 - val_accuracy: 0.9171\n",
            "Epoch 129/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9158 - val_loss: 0.1795 - val_accuracy: 0.9134\n",
            "Epoch 130/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9144 - val_loss: 0.1765 - val_accuracy: 0.9143\n",
            "Epoch 131/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9144 - val_loss: 0.1761 - val_accuracy: 0.9176\n",
            "Epoch 132/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9156 - val_loss: 0.1773 - val_accuracy: 0.9160\n",
            "Epoch 133/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9168 - val_loss: 0.1763 - val_accuracy: 0.9164\n",
            "Epoch 134/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9138 - val_loss: 0.1761 - val_accuracy: 0.9164\n",
            "Epoch 135/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9156 - val_loss: 0.1762 - val_accuracy: 0.9156\n",
            "Epoch 136/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9179 - val_loss: 0.1757 - val_accuracy: 0.9182\n",
            "Epoch 137/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9151 - val_loss: 0.1760 - val_accuracy: 0.9165\n",
            "Epoch 138/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9153 - val_loss: 0.1759 - val_accuracy: 0.9156\n",
            "Epoch 139/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9126 - val_loss: 0.1768 - val_accuracy: 0.9153\n",
            "Epoch 140/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9143 - val_loss: 0.1767 - val_accuracy: 0.9170\n",
            "Epoch 141/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9149 - val_loss: 0.1835 - val_accuracy: 0.9114\n",
            "Epoch 142/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9158 - val_loss: 0.1763 - val_accuracy: 0.9167\n",
            "Epoch 143/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1763 - accuracy: 0.9169 - val_loss: 0.1768 - val_accuracy: 0.9169\n",
            "Epoch 144/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9153 - val_loss: 0.1760 - val_accuracy: 0.9183\n",
            "Epoch 145/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1810 - accuracy: 0.9142 - val_loss: 0.1767 - val_accuracy: 0.9158\n",
            "Epoch 146/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9175 - val_loss: 0.1771 - val_accuracy: 0.9169\n",
            "Epoch 147/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9154 - val_loss: 0.1758 - val_accuracy: 0.9174\n",
            "Epoch 148/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9176 - val_loss: 0.1777 - val_accuracy: 0.9146\n",
            "Epoch 149/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9150 - val_loss: 0.1754 - val_accuracy: 0.9162\n",
            "Epoch 150/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9157 - val_loss: 0.1760 - val_accuracy: 0.9169\n",
            "Epoch 151/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9154 - val_loss: 0.1767 - val_accuracy: 0.9163\n",
            "Epoch 152/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9127 - val_loss: 0.1763 - val_accuracy: 0.9166\n",
            "Epoch 153/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9149 - val_loss: 0.1763 - val_accuracy: 0.9164\n",
            "Epoch 154/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9187 - val_loss: 0.1804 - val_accuracy: 0.9147\n",
            "Epoch 155/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9141 - val_loss: 0.1773 - val_accuracy: 0.9167\n",
            "Epoch 156/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9141 - val_loss: 0.1769 - val_accuracy: 0.9152\n",
            "Epoch 157/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1766 - accuracy: 0.9191 - val_loss: 0.1773 - val_accuracy: 0.9165\n",
            "Epoch 158/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9149 - val_loss: 0.1758 - val_accuracy: 0.9169\n",
            "Epoch 159/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9176 - val_loss: 0.1767 - val_accuracy: 0.9160\n",
            "Epoch 160/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9169 - val_loss: 0.1761 - val_accuracy: 0.9166\n",
            "Epoch 161/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9152 - val_loss: 0.1769 - val_accuracy: 0.9174\n",
            "Epoch 162/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9160 - val_loss: 0.1762 - val_accuracy: 0.9168\n",
            "Epoch 163/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9154 - val_loss: 0.1765 - val_accuracy: 0.9149\n",
            "Epoch 164/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9128 - val_loss: 0.1766 - val_accuracy: 0.9159\n",
            "Epoch 165/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1746 - accuracy: 0.9176 - val_loss: 0.1831 - val_accuracy: 0.9150\n",
            "Epoch 166/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9118 - val_loss: 0.1763 - val_accuracy: 0.9170\n",
            "Epoch 167/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9131 - val_loss: 0.1759 - val_accuracy: 0.9169\n",
            "Epoch 168/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9166 - val_loss: 0.1772 - val_accuracy: 0.9159\n",
            "Epoch 169/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1764 - accuracy: 0.9169 - val_loss: 0.1759 - val_accuracy: 0.9181\n",
            "Epoch 170/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9123 - val_loss: 0.1758 - val_accuracy: 0.9179\n",
            "Epoch 171/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9153 - val_loss: 0.1764 - val_accuracy: 0.9172\n",
            "Epoch 172/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9175 - val_loss: 0.1760 - val_accuracy: 0.9156\n",
            "Epoch 173/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9152 - val_loss: 0.1780 - val_accuracy: 0.9152\n",
            "Epoch 174/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9168 - val_loss: 0.1770 - val_accuracy: 0.9160\n",
            "Epoch 175/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9165 - val_loss: 0.1764 - val_accuracy: 0.9165\n",
            "Epoch 176/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1761 - accuracy: 0.9169 - val_loss: 0.1763 - val_accuracy: 0.9177\n",
            "Epoch 177/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9165 - val_loss: 0.1783 - val_accuracy: 0.9163\n",
            "Epoch 178/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9157 - val_loss: 0.1760 - val_accuracy: 0.9172\n",
            "Epoch 179/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9140 - val_loss: 0.1761 - val_accuracy: 0.9168\n",
            "Epoch 180/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9171 - val_loss: 0.1782 - val_accuracy: 0.9164\n",
            "Epoch 181/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9116 - val_loss: 0.1780 - val_accuracy: 0.9160\n",
            "Epoch 182/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9140 - val_loss: 0.1763 - val_accuracy: 0.9157\n",
            "Epoch 183/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9150 - val_loss: 0.1770 - val_accuracy: 0.9156\n",
            "Epoch 184/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9139 - val_loss: 0.1770 - val_accuracy: 0.9161\n",
            "Epoch 185/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9130 - val_loss: 0.1760 - val_accuracy: 0.9168\n",
            "Epoch 186/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9141 - val_loss: 0.1758 - val_accuracy: 0.9163\n",
            "Epoch 187/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9171 - val_loss: 0.1762 - val_accuracy: 0.9154\n",
            "Epoch 188/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1748 - accuracy: 0.9191 - val_loss: 0.1755 - val_accuracy: 0.9169\n",
            "Epoch 189/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9158 - val_loss: 0.1766 - val_accuracy: 0.9171\n",
            "Epoch 190/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9178 - val_loss: 0.1752 - val_accuracy: 0.9174\n",
            "Epoch 191/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9187 - val_loss: 0.1754 - val_accuracy: 0.9172\n",
            "Epoch 192/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9118 - val_loss: 0.1777 - val_accuracy: 0.9153\n",
            "Epoch 193/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9145 - val_loss: 0.1758 - val_accuracy: 0.9168\n",
            "Epoch 194/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1752 - accuracy: 0.9162 - val_loss: 0.1756 - val_accuracy: 0.9165\n",
            "Epoch 195/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9151 - val_loss: 0.1801 - val_accuracy: 0.9143\n",
            "Epoch 196/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9135 - val_loss: 0.1759 - val_accuracy: 0.9173\n",
            "Epoch 197/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9168 - val_loss: 0.1771 - val_accuracy: 0.9159\n",
            "Epoch 198/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9156 - val_loss: 0.1771 - val_accuracy: 0.9166\n",
            "Epoch 199/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9143 - val_loss: 0.1759 - val_accuracy: 0.9177\n",
            "Epoch 200/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1759 - accuracy: 0.9162 - val_loss: 0.1796 - val_accuracy: 0.9151\n",
            "Epoch 201/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9166 - val_loss: 0.1762 - val_accuracy: 0.9166\n",
            "Epoch 202/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9136 - val_loss: 0.1758 - val_accuracy: 0.9165\n",
            "Epoch 203/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1782 - accuracy: 0.9179 - val_loss: 0.1762 - val_accuracy: 0.9171\n",
            "Epoch 204/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9166 - val_loss: 0.1751 - val_accuracy: 0.9177\n",
            "Epoch 205/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9154 - val_loss: 0.1778 - val_accuracy: 0.9163\n",
            "Epoch 206/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1745 - accuracy: 0.9161 - val_loss: 0.1781 - val_accuracy: 0.9162\n",
            "Epoch 207/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9166 - val_loss: 0.1781 - val_accuracy: 0.9152\n",
            "Epoch 208/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9171 - val_loss: 0.1758 - val_accuracy: 0.9174\n",
            "Epoch 209/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9160 - val_loss: 0.1755 - val_accuracy: 0.9173\n",
            "Epoch 210/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1747 - accuracy: 0.9168 - val_loss: 0.1754 - val_accuracy: 0.9169\n",
            "Epoch 211/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9166 - val_loss: 0.1757 - val_accuracy: 0.9172\n",
            "Epoch 212/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9167 - val_loss: 0.1770 - val_accuracy: 0.9165\n",
            "Epoch 213/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9145 - val_loss: 0.1764 - val_accuracy: 0.9165\n",
            "Epoch 214/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9144 - val_loss: 0.1769 - val_accuracy: 0.9169\n",
            "Epoch 215/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9136 - val_loss: 0.1788 - val_accuracy: 0.9157\n",
            "Epoch 216/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9159 - val_loss: 0.1764 - val_accuracy: 0.9169\n",
            "Epoch 217/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9154 - val_loss: 0.1765 - val_accuracy: 0.9173\n",
            "Epoch 218/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9145 - val_loss: 0.1774 - val_accuracy: 0.9165\n",
            "Epoch 219/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9184 - val_loss: 0.1787 - val_accuracy: 0.9146\n",
            "Epoch 220/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.9182 - val_loss: 0.1755 - val_accuracy: 0.9176\n",
            "Epoch 221/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9133 - val_loss: 0.1764 - val_accuracy: 0.9172\n",
            "Epoch 222/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9166 - val_loss: 0.1757 - val_accuracy: 0.9177\n",
            "Epoch 223/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9144 - val_loss: 0.1761 - val_accuracy: 0.9174\n",
            "Epoch 224/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1743 - accuracy: 0.9174 - val_loss: 0.1784 - val_accuracy: 0.9143\n",
            "Epoch 225/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1763 - accuracy: 0.9171 - val_loss: 0.1755 - val_accuracy: 0.9173\n",
            "Epoch 226/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1782 - accuracy: 0.9158 - val_loss: 0.1762 - val_accuracy: 0.9184\n",
            "Epoch 227/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9147 - val_loss: 0.1759 - val_accuracy: 0.9169\n",
            "Epoch 228/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1738 - accuracy: 0.9181 - val_loss: 0.1769 - val_accuracy: 0.9179\n",
            "Epoch 229/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9159 - val_loss: 0.1779 - val_accuracy: 0.9153\n",
            "Epoch 230/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9153 - val_loss: 0.1754 - val_accuracy: 0.9181\n",
            "Epoch 231/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9170 - val_loss: 0.1780 - val_accuracy: 0.9169\n",
            "Epoch 232/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9150 - val_loss: 0.1760 - val_accuracy: 0.9179\n",
            "Epoch 233/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1755 - accuracy: 0.9163 - val_loss: 0.1814 - val_accuracy: 0.9131\n",
            "Epoch 234/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9134 - val_loss: 0.1766 - val_accuracy: 0.9158\n",
            "Epoch 235/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9177 - val_loss: 0.1755 - val_accuracy: 0.9174\n",
            "Epoch 236/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9161 - val_loss: 0.1762 - val_accuracy: 0.9155\n",
            "Epoch 237/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9167 - val_loss: 0.1754 - val_accuracy: 0.9169\n",
            "Epoch 238/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9145 - val_loss: 0.1751 - val_accuracy: 0.9182\n",
            "Epoch 239/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1752 - accuracy: 0.9169 - val_loss: 0.1845 - val_accuracy: 0.9139\n",
            "Epoch 240/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1745 - accuracy: 0.9193 - val_loss: 0.1767 - val_accuracy: 0.9169\n",
            "Epoch 241/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1753 - accuracy: 0.9158 - val_loss: 0.1780 - val_accuracy: 0.9157\n",
            "Epoch 242/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1733 - accuracy: 0.9177 - val_loss: 0.1775 - val_accuracy: 0.9160\n",
            "Epoch 243/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9161 - val_loss: 0.1761 - val_accuracy: 0.9176\n",
            "Epoch 244/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9176 - val_loss: 0.1775 - val_accuracy: 0.9160\n",
            "Epoch 245/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9156 - val_loss: 0.1755 - val_accuracy: 0.9180\n",
            "Epoch 246/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9170 - val_loss: 0.1746 - val_accuracy: 0.9177\n",
            "Epoch 247/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1742 - accuracy: 0.9188 - val_loss: 0.1750 - val_accuracy: 0.9173\n",
            "Epoch 248/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1727 - accuracy: 0.9181 - val_loss: 0.1820 - val_accuracy: 0.9145\n",
            "Epoch 249/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9175 - val_loss: 0.1753 - val_accuracy: 0.9176\n",
            "Epoch 250/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9165 - val_loss: 0.1788 - val_accuracy: 0.9165\n",
            "Epoch 251/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1763 - accuracy: 0.9166 - val_loss: 0.1752 - val_accuracy: 0.9176\n",
            "Epoch 252/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1751 - accuracy: 0.9159 - val_loss: 0.1748 - val_accuracy: 0.9179\n",
            "Epoch 253/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9163 - val_loss: 0.1755 - val_accuracy: 0.9171\n",
            "Epoch 254/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9170 - val_loss: 0.1828 - val_accuracy: 0.9119\n",
            "Epoch 255/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9165 - val_loss: 0.1755 - val_accuracy: 0.9169\n",
            "Epoch 256/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9168 - val_loss: 0.1767 - val_accuracy: 0.9174\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "feszLoOlr7KZ",
        "outputId": "07bcd475-5ede-4e7f-c502-3b611e7246d5"
      },
      "source": [
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model3.add(Dense(4,activation='relu'))\n",
        "model3.add(Dense(1,activation='sigmoid'))\n",
        "model3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model3.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3754 - accuracy: 0.8815 - val_loss: 0.2724 - val_accuracy: 0.8957\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2708 - accuracy: 0.8950 - val_loss: 0.2319 - val_accuracy: 0.9042\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2368 - accuracy: 0.9034 - val_loss: 0.2151 - val_accuracy: 0.9131\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2189 - accuracy: 0.9081 - val_loss: 0.2054 - val_accuracy: 0.9134\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2121 - accuracy: 0.9091 - val_loss: 0.2021 - val_accuracy: 0.9134\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2033 - accuracy: 0.9131 - val_loss: 0.2015 - val_accuracy: 0.9124\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2042 - accuracy: 0.9104 - val_loss: 0.1989 - val_accuracy: 0.9143\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2028 - accuracy: 0.9097 - val_loss: 0.1993 - val_accuracy: 0.9139\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1993 - accuracy: 0.9129 - val_loss: 0.1983 - val_accuracy: 0.9137\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2024 - accuracy: 0.9105 - val_loss: 0.1969 - val_accuracy: 0.9139\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1997 - accuracy: 0.9097 - val_loss: 0.1956 - val_accuracy: 0.9135\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2029 - accuracy: 0.9062 - val_loss: 0.1951 - val_accuracy: 0.9130\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1937 - accuracy: 0.9128 - val_loss: 0.1937 - val_accuracy: 0.9129\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9100 - val_loss: 0.1931 - val_accuracy: 0.9129\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2013 - accuracy: 0.9085 - val_loss: 0.1921 - val_accuracy: 0.9143\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1949 - accuracy: 0.9144 - val_loss: 0.1916 - val_accuracy: 0.9152\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1929 - accuracy: 0.9107 - val_loss: 0.1909 - val_accuracy: 0.9146\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1918 - accuracy: 0.9115 - val_loss: 0.1901 - val_accuracy: 0.9147\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1935 - accuracy: 0.9113 - val_loss: 0.1895 - val_accuracy: 0.9148\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1928 - accuracy: 0.9132 - val_loss: 0.1889 - val_accuracy: 0.9148\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9135 - val_loss: 0.1887 - val_accuracy: 0.9152\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1944 - accuracy: 0.9109 - val_loss: 0.1892 - val_accuracy: 0.9150\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9138 - val_loss: 0.1880 - val_accuracy: 0.9145\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1961 - accuracy: 0.9105 - val_loss: 0.1873 - val_accuracy: 0.9156\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1932 - accuracy: 0.9107 - val_loss: 0.1879 - val_accuracy: 0.9152\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9134 - val_loss: 0.1878 - val_accuracy: 0.9151\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9114 - val_loss: 0.1872 - val_accuracy: 0.9166\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1927 - accuracy: 0.9093 - val_loss: 0.1876 - val_accuracy: 0.9158\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1943 - accuracy: 0.9082 - val_loss: 0.1935 - val_accuracy: 0.9140\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1873 - accuracy: 0.9113 - val_loss: 0.1893 - val_accuracy: 0.9136\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1949 - accuracy: 0.9084 - val_loss: 0.1868 - val_accuracy: 0.9152\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1952 - accuracy: 0.9105 - val_loss: 0.1865 - val_accuracy: 0.9156\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1877 - accuracy: 0.9129 - val_loss: 0.1859 - val_accuracy: 0.9154\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1916 - accuracy: 0.9127 - val_loss: 0.1871 - val_accuracy: 0.9154\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9125 - val_loss: 0.1901 - val_accuracy: 0.9152\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1935 - accuracy: 0.9097 - val_loss: 0.1855 - val_accuracy: 0.9160\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1958 - accuracy: 0.9081 - val_loss: 0.1856 - val_accuracy: 0.9160\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9121 - val_loss: 0.1874 - val_accuracy: 0.9152\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9103 - val_loss: 0.1878 - val_accuracy: 0.9156\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1901 - accuracy: 0.9118 - val_loss: 0.1847 - val_accuracy: 0.9151\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9118 - val_loss: 0.1868 - val_accuracy: 0.9156\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1885 - accuracy: 0.9133 - val_loss: 0.1877 - val_accuracy: 0.9159\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1907 - accuracy: 0.9126 - val_loss: 0.1869 - val_accuracy: 0.9152\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9128 - val_loss: 0.1867 - val_accuracy: 0.9153\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1876 - accuracy: 0.9144 - val_loss: 0.1900 - val_accuracy: 0.9155\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1937 - accuracy: 0.9097 - val_loss: 0.1861 - val_accuracy: 0.9151\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9100 - val_loss: 0.1853 - val_accuracy: 0.9156\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1928 - accuracy: 0.9109 - val_loss: 0.1871 - val_accuracy: 0.9152\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1903 - accuracy: 0.9126 - val_loss: 0.1847 - val_accuracy: 0.9147\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1870 - accuracy: 0.9122 - val_loss: 0.1910 - val_accuracy: 0.9145\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9153 - val_loss: 0.1849 - val_accuracy: 0.9165\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1949 - accuracy: 0.9099 - val_loss: 0.1853 - val_accuracy: 0.9156\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9132 - val_loss: 0.1880 - val_accuracy: 0.9146\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9118 - val_loss: 0.1864 - val_accuracy: 0.9158\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9099 - val_loss: 0.1843 - val_accuracy: 0.9169\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9104 - val_loss: 0.1846 - val_accuracy: 0.9153\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9099 - val_loss: 0.1839 - val_accuracy: 0.9153\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9128 - val_loss: 0.1837 - val_accuracy: 0.9164\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1906 - accuracy: 0.9107 - val_loss: 0.1847 - val_accuracy: 0.9159\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9123 - val_loss: 0.1858 - val_accuracy: 0.9141\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1916 - accuracy: 0.9122 - val_loss: 0.1883 - val_accuracy: 0.9146\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9145 - val_loss: 0.1839 - val_accuracy: 0.9143\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1890 - accuracy: 0.9124 - val_loss: 0.1840 - val_accuracy: 0.9167\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9111 - val_loss: 0.1825 - val_accuracy: 0.9157\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9144 - val_loss: 0.1831 - val_accuracy: 0.9159\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1865 - accuracy: 0.9135 - val_loss: 0.1839 - val_accuracy: 0.9162\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1864 - accuracy: 0.9123 - val_loss: 0.1824 - val_accuracy: 0.9169\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9136 - val_loss: 0.1825 - val_accuracy: 0.9174\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9142 - val_loss: 0.1826 - val_accuracy: 0.9181\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9173 - val_loss: 0.1823 - val_accuracy: 0.9164\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9109 - val_loss: 0.1829 - val_accuracy: 0.9180\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9131 - val_loss: 0.1830 - val_accuracy: 0.9174\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1875 - accuracy: 0.9143 - val_loss: 0.1851 - val_accuracy: 0.9159\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9141 - val_loss: 0.1826 - val_accuracy: 0.9177\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9113 - val_loss: 0.1823 - val_accuracy: 0.9161\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1839 - accuracy: 0.9161 - val_loss: 0.1820 - val_accuracy: 0.9174\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9124 - val_loss: 0.1827 - val_accuracy: 0.9178\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9143 - val_loss: 0.1884 - val_accuracy: 0.9138\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9158 - val_loss: 0.1822 - val_accuracy: 0.9183\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9167 - val_loss: 0.1827 - val_accuracy: 0.9170\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9149 - val_loss: 0.1819 - val_accuracy: 0.9186\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1808 - accuracy: 0.9145 - val_loss: 0.1862 - val_accuracy: 0.9171\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9125 - val_loss: 0.1822 - val_accuracy: 0.9171\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1894 - accuracy: 0.9099 - val_loss: 0.1810 - val_accuracy: 0.9183\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9139 - val_loss: 0.1835 - val_accuracy: 0.9171\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9155 - val_loss: 0.1820 - val_accuracy: 0.9164\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1871 - accuracy: 0.9144 - val_loss: 0.1816 - val_accuracy: 0.9179\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9141 - val_loss: 0.1825 - val_accuracy: 0.9183\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1851 - accuracy: 0.9135 - val_loss: 0.1816 - val_accuracy: 0.9179\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9134 - val_loss: 0.1810 - val_accuracy: 0.9183\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1892 - accuracy: 0.9112 - val_loss: 0.1842 - val_accuracy: 0.9183\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1840 - accuracy: 0.9165 - val_loss: 0.1809 - val_accuracy: 0.9184\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9142 - val_loss: 0.1811 - val_accuracy: 0.9190\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9113 - val_loss: 0.1805 - val_accuracy: 0.9180\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9157 - val_loss: 0.1805 - val_accuracy: 0.9175\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9135 - val_loss: 0.1823 - val_accuracy: 0.9169\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9155 - val_loss: 0.1823 - val_accuracy: 0.9168\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9158 - val_loss: 0.1813 - val_accuracy: 0.9182\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9146 - val_loss: 0.1832 - val_accuracy: 0.9168\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9124 - val_loss: 0.1811 - val_accuracy: 0.9178\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9150 - val_loss: 0.1878 - val_accuracy: 0.9155\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.9165 - val_loss: 0.1801 - val_accuracy: 0.9184\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9153 - val_loss: 0.1813 - val_accuracy: 0.9184\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9161 - val_loss: 0.1811 - val_accuracy: 0.9182\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1859 - accuracy: 0.9138 - val_loss: 0.1819 - val_accuracy: 0.9174\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1831 - accuracy: 0.9154 - val_loss: 0.1806 - val_accuracy: 0.9184\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9155 - val_loss: 0.1809 - val_accuracy: 0.9170\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.9128 - val_loss: 0.1808 - val_accuracy: 0.9181\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9140 - val_loss: 0.1814 - val_accuracy: 0.9190\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9147 - val_loss: 0.1804 - val_accuracy: 0.9176\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9137 - val_loss: 0.1824 - val_accuracy: 0.9173\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9156 - val_loss: 0.1800 - val_accuracy: 0.9186\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1891 - accuracy: 0.9157 - val_loss: 0.1816 - val_accuracy: 0.9164\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9162 - val_loss: 0.1812 - val_accuracy: 0.9178\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.9140 - val_loss: 0.1803 - val_accuracy: 0.9186\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9121 - val_loss: 0.1824 - val_accuracy: 0.9179\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9139 - val_loss: 0.1816 - val_accuracy: 0.9174\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1876 - accuracy: 0.9124 - val_loss: 0.1801 - val_accuracy: 0.9186\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9121 - val_loss: 0.1822 - val_accuracy: 0.9177\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9138 - val_loss: 0.1815 - val_accuracy: 0.9178\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9145 - val_loss: 0.1825 - val_accuracy: 0.9183\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9135 - val_loss: 0.1805 - val_accuracy: 0.9186\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9156 - val_loss: 0.1815 - val_accuracy: 0.9170\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9137 - val_loss: 0.1807 - val_accuracy: 0.9168\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1844 - accuracy: 0.9149 - val_loss: 0.1819 - val_accuracy: 0.9165\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9150 - val_loss: 0.1811 - val_accuracy: 0.9185\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1826 - accuracy: 0.9143 - val_loss: 0.1808 - val_accuracy: 0.9175\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9153 - val_loss: 0.1806 - val_accuracy: 0.9176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jblc7f7ESxBB",
        "outputId": "a2cfc276-849f-47c8-fb22-632c1a0bbfff"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 8)                 168       \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 209\n",
            "Trainable params: 209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zf4oOP7BN5I2",
        "outputId": "10820247-f7e7-44dd-cc53-35770f6583e5"
      },
      "source": [
        "\n",
        "model_es1 = Sequential()\n",
        "model_es1.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_es1.add(Dense(4,activation='relu'))\n",
        "model_es1.add(Dense(1,activation='sigmoid'))\n",
        "model_es1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "callback_a = ModelCheckpoint(filepath = 'model_es1.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "# The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=120, verbose=1)\n",
        "\n",
        "history = model_es1.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256, batch_size=30, callbacks = [callback_a, callback_b])\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4480 - accuracy: 0.7828 - val_loss: 0.2661 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89500, saving model to model_es1.hdf5\n",
            "Epoch 2/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2714 - accuracy: 0.8919 - val_loss: 0.2250 - val_accuracy: 0.9118\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89500 to 0.91176, saving model to model_es1.hdf5\n",
            "Epoch 3/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2273 - accuracy: 0.9068 - val_loss: 0.2069 - val_accuracy: 0.9145\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.91176 to 0.91451, saving model to model_es1.hdf5\n",
            "Epoch 4/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2234 - accuracy: 0.9031 - val_loss: 0.1976 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.91451 to 0.91597, saving model to model_es1.hdf5\n",
            "Epoch 5/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2090 - accuracy: 0.9086 - val_loss: 0.1939 - val_accuracy: 0.9158\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.91597\n",
            "Epoch 6/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9127 - val_loss: 0.1933 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.91597 to 0.91605, saving model to model_es1.hdf5\n",
            "Epoch 7/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9099 - val_loss: 0.1911 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.91605\n",
            "Epoch 8/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2015 - accuracy: 0.9099 - val_loss: 0.1935 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.91605\n",
            "Epoch 9/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2021 - accuracy: 0.9093 - val_loss: 0.1892 - val_accuracy: 0.9128\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.91605\n",
            "Epoch 10/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2029 - accuracy: 0.9106 - val_loss: 0.1887 - val_accuracy: 0.9166\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.91605 to 0.91661, saving model to model_es1.hdf5\n",
            "Epoch 11/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9141 - val_loss: 0.1876 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.91661\n",
            "Epoch 12/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9099 - val_loss: 0.1869 - val_accuracy: 0.9162\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.91661\n",
            "Epoch 13/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1978 - accuracy: 0.9123 - val_loss: 0.1892 - val_accuracy: 0.9166\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91661\n",
            "Epoch 14/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9096 - val_loss: 0.1860 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.91661 to 0.91742, saving model to model_es1.hdf5\n",
            "Epoch 15/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9108 - val_loss: 0.1854 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.91742 to 0.91767, saving model to model_es1.hdf5\n",
            "Epoch 16/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9129 - val_loss: 0.1857 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91767\n",
            "Epoch 17/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9095 - val_loss: 0.1844 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.91767 to 0.91775, saving model to model_es1.hdf5\n",
            "Epoch 18/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9106 - val_loss: 0.1871 - val_accuracy: 0.9166\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91775\n",
            "Epoch 19/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9098 - val_loss: 0.1843 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91775\n",
            "Epoch 20/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9108 - val_loss: 0.1853 - val_accuracy: 0.9176\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91775\n",
            "Epoch 21/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9119 - val_loss: 0.1851 - val_accuracy: 0.9186\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.91775 to 0.91864, saving model to model_es1.hdf5\n",
            "Epoch 22/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9131 - val_loss: 0.1832 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91864\n",
            "Epoch 23/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9094 - val_loss: 0.1873 - val_accuracy: 0.9178\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91864\n",
            "Epoch 24/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9142 - val_loss: 0.1839 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91864\n",
            "Epoch 25/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9142 - val_loss: 0.1854 - val_accuracy: 0.9171\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91864\n",
            "Epoch 26/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9108 - val_loss: 0.1827 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91864\n",
            "Epoch 27/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9092 - val_loss: 0.1831 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91864\n",
            "Epoch 28/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9129 - val_loss: 0.1826 - val_accuracy: 0.9179\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91864\n",
            "Epoch 29/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9120 - val_loss: 0.1842 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91864\n",
            "Epoch 30/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9120 - val_loss: 0.1875 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91864\n",
            "Epoch 31/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9088 - val_loss: 0.1829 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91864\n",
            "Epoch 32/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9103 - val_loss: 0.1850 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91864\n",
            "Epoch 33/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9124 - val_loss: 0.1860 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91864\n",
            "Epoch 34/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9128 - val_loss: 0.1833 - val_accuracy: 0.9162\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91864\n",
            "Epoch 35/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9107 - val_loss: 0.1836 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91864\n",
            "Epoch 36/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9116 - val_loss: 0.1829 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91864\n",
            "Epoch 37/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9124 - val_loss: 0.1858 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91864\n",
            "Epoch 38/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9123 - val_loss: 0.1830 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91864\n",
            "Epoch 39/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9109 - val_loss: 0.1841 - val_accuracy: 0.9151\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91864\n",
            "Epoch 40/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9128 - val_loss: 0.1825 - val_accuracy: 0.9162\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91864\n",
            "Epoch 41/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9121 - val_loss: 0.1836 - val_accuracy: 0.9161\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91864\n",
            "Epoch 42/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9141 - val_loss: 0.1826 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91864\n",
            "Epoch 43/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9098 - val_loss: 0.1824 - val_accuracy: 0.9161\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91864\n",
            "Epoch 44/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9104 - val_loss: 0.1850 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91864\n",
            "Epoch 45/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9101 - val_loss: 0.1834 - val_accuracy: 0.9162\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91864\n",
            "Epoch 46/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9109 - val_loss: 0.1815 - val_accuracy: 0.9162\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91864\n",
            "Epoch 47/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9117 - val_loss: 0.1876 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91864\n",
            "Epoch 48/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9152 - val_loss: 0.1812 - val_accuracy: 0.9164\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91864\n",
            "Epoch 49/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9094 - val_loss: 0.1822 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91864\n",
            "Epoch 50/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9130 - val_loss: 0.1836 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91864\n",
            "Epoch 51/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9133 - val_loss: 0.1828 - val_accuracy: 0.9153\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91864\n",
            "Epoch 52/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9124 - val_loss: 0.1816 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91864\n",
            "Epoch 53/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9104 - val_loss: 0.1822 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91864\n",
            "Epoch 54/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9100 - val_loss: 0.1817 - val_accuracy: 0.9171\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91864\n",
            "Epoch 55/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9120 - val_loss: 0.1818 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91864\n",
            "Epoch 56/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9105 - val_loss: 0.1811 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91864\n",
            "Epoch 57/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9117 - val_loss: 0.1838 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91864\n",
            "Epoch 58/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9142 - val_loss: 0.1829 - val_accuracy: 0.9168\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91864\n",
            "Epoch 59/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9102 - val_loss: 0.1808 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91864\n",
            "Epoch 60/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9111 - val_loss: 0.1802 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91864\n",
            "Epoch 61/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9107 - val_loss: 0.1813 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91864\n",
            "Epoch 62/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9129 - val_loss: 0.1808 - val_accuracy: 0.9161\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91864\n",
            "Epoch 63/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9127 - val_loss: 0.1826 - val_accuracy: 0.9151\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91864\n",
            "Epoch 64/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9097 - val_loss: 0.1807 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91864\n",
            "Epoch 65/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9091 - val_loss: 0.1803 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.91864\n",
            "Epoch 66/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9127 - val_loss: 0.1814 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.91864\n",
            "Epoch 67/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9133 - val_loss: 0.1835 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00067: val_accuracy did not improve from 0.91864\n",
            "Epoch 68/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9107 - val_loss: 0.1797 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.91864\n",
            "Epoch 69/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9127 - val_loss: 0.1801 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.91864\n",
            "Epoch 70/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9124 - val_loss: 0.1799 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.91864\n",
            "Epoch 71/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9111 - val_loss: 0.1820 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.91864\n",
            "Epoch 72/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9107 - val_loss: 0.1801 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.91864\n",
            "Epoch 73/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9126 - val_loss: 0.1823 - val_accuracy: 0.9167\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.91864\n",
            "Epoch 74/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9142 - val_loss: 0.1809 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.91864\n",
            "Epoch 75/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9155 - val_loss: 0.1850 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.91864\n",
            "Epoch 76/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9126 - val_loss: 0.1811 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.91864\n",
            "Epoch 77/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9139 - val_loss: 0.1808 - val_accuracy: 0.9171\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.91864\n",
            "Epoch 78/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9127 - val_loss: 0.1797 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.91864\n",
            "Epoch 79/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9115 - val_loss: 0.1814 - val_accuracy: 0.9157\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.91864\n",
            "Epoch 80/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9111 - val_loss: 0.1784 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.91864\n",
            "Epoch 81/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9123 - val_loss: 0.1792 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.91864\n",
            "Epoch 82/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9114 - val_loss: 0.1786 - val_accuracy: 0.9179\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.91864\n",
            "Epoch 83/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9115 - val_loss: 0.1829 - val_accuracy: 0.9171\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.91864\n",
            "Epoch 84/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9107 - val_loss: 0.1797 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.91864\n",
            "Epoch 85/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9129 - val_loss: 0.1832 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.91864\n",
            "Epoch 86/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9122 - val_loss: 0.1788 - val_accuracy: 0.9183\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.91864\n",
            "Epoch 87/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9124 - val_loss: 0.1784 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.91864\n",
            "Epoch 88/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9118 - val_loss: 0.1779 - val_accuracy: 0.9176\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.91864\n",
            "Epoch 89/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9108 - val_loss: 0.1794 - val_accuracy: 0.9168\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.91864\n",
            "Epoch 90/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9108 - val_loss: 0.1800 - val_accuracy: 0.9171\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.91864\n",
            "Epoch 91/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9129 - val_loss: 0.1776 - val_accuracy: 0.9184\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.91864\n",
            "Epoch 92/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9138 - val_loss: 0.1785 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.91864\n",
            "Epoch 93/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9138 - val_loss: 0.1784 - val_accuracy: 0.9182\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.91864\n",
            "Epoch 94/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9137 - val_loss: 0.1786 - val_accuracy: 0.9176\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.91864\n",
            "Epoch 95/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9091 - val_loss: 0.1786 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.91864\n",
            "Epoch 96/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9119 - val_loss: 0.1777 - val_accuracy: 0.9186\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.91864\n",
            "Epoch 97/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9142 - val_loss: 0.1782 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.91864\n",
            "Epoch 98/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9108 - val_loss: 0.1833 - val_accuracy: 0.9151\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.91864\n",
            "Epoch 99/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9133 - val_loss: 0.1778 - val_accuracy: 0.9186\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.91864\n",
            "Epoch 100/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9159 - val_loss: 0.1776 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.91864\n",
            "Epoch 101/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9118 - val_loss: 0.1775 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.91864\n",
            "Epoch 102/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9116 - val_loss: 0.1867 - val_accuracy: 0.9128\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.91864\n",
            "Epoch 103/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9147 - val_loss: 0.1780 - val_accuracy: 0.9182\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.91864\n",
            "Epoch 104/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9150 - val_loss: 0.1786 - val_accuracy: 0.9178\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.91864\n",
            "Epoch 105/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9165 - val_loss: 0.1774 - val_accuracy: 0.9184\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.91864\n",
            "Epoch 106/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9124 - val_loss: 0.1778 - val_accuracy: 0.9192\n",
            "\n",
            "Epoch 00106: val_accuracy improved from 0.91864 to 0.91920, saving model to model_es1.hdf5\n",
            "Epoch 107/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9138 - val_loss: 0.1847 - val_accuracy: 0.9141\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.91920\n",
            "Epoch 108/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9132 - val_loss: 0.1778 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.91920\n",
            "Epoch 109/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9150 - val_loss: 0.1814 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.91920\n",
            "Epoch 110/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9131 - val_loss: 0.1776 - val_accuracy: 0.9178\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.91920\n",
            "Epoch 111/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9137 - val_loss: 0.1784 - val_accuracy: 0.9184\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.91920\n",
            "Epoch 112/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9130 - val_loss: 0.1793 - val_accuracy: 0.9164\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.91920\n",
            "Epoch 113/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9142 - val_loss: 0.1785 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.91920\n",
            "Epoch 114/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9130 - val_loss: 0.1777 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.91920\n",
            "Epoch 115/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9125 - val_loss: 0.1784 - val_accuracy: 0.9179\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.91920\n",
            "Epoch 116/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9112 - val_loss: 0.1781 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.91920\n",
            "Epoch 117/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9130 - val_loss: 0.1768 - val_accuracy: 0.9184\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.91920\n",
            "Epoch 118/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9141 - val_loss: 0.1767 - val_accuracy: 0.9185\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.91920\n",
            "Epoch 119/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9139 - val_loss: 0.1773 - val_accuracy: 0.9185\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.91920\n",
            "Epoch 120/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9140 - val_loss: 0.1811 - val_accuracy: 0.9171\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.91920\n",
            "Epoch 121/256\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9115 - val_loss: 0.1772 - val_accuracy: 0.9182\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.91920\n",
            "Epoch 00121: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O7OsSmuw5bn5",
        "outputId": "02b00c86-e589-450c-e541-a7b1bd64cfa3"
      },
      "source": [
        "#0.3\n",
        "model_es = Sequential()\n",
        "model_es.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_es.add(Dense(4,activation='relu'))\n",
        "model_es.add(Dense(1,activation='sigmoid'))\n",
        "model_es.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "callback_a = ModelCheckpoint(filepath = 'model_es.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "# The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=100, verbose=1)\n",
        "\n",
        "history = model_es.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256, batch_size=40, callbacks = [callback_a, callback_b])\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "721/721 [==============================] - 2s 2ms/step - loss: 0.3324 - accuracy: 0.8857 - val_loss: 0.2816 - val_accuracy: 0.8928\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89281, saving model to model_es.hdf5\n",
            "Epoch 2/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2888 - accuracy: 0.8866 - val_loss: 0.2576 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89281 to 0.89694, saving model to model_es.hdf5\n",
            "Epoch 3/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2726 - accuracy: 0.8890 - val_loss: 0.2266 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89694 to 0.90714, saving model to model_es.hdf5\n",
            "Epoch 4/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2326 - accuracy: 0.9030 - val_loss: 0.2043 - val_accuracy: 0.9134\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90714 to 0.91337, saving model to model_es.hdf5\n",
            "Epoch 5/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2172 - accuracy: 0.9059 - val_loss: 0.1982 - val_accuracy: 0.9152\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.91337 to 0.91524, saving model to model_es.hdf5\n",
            "Epoch 6/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2092 - accuracy: 0.9114 - val_loss: 0.1974 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.91524\n",
            "Epoch 7/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2045 - accuracy: 0.9098 - val_loss: 0.1954 - val_accuracy: 0.9146\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.91524\n",
            "Epoch 8/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2022 - accuracy: 0.9093 - val_loss: 0.1954 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.91524 to 0.91548, saving model to model_es.hdf5\n",
            "Epoch 9/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2065 - accuracy: 0.9089 - val_loss: 0.1943 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.91548 to 0.91605, saving model to model_es.hdf5\n",
            "Epoch 10/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2096 - accuracy: 0.9066 - val_loss: 0.1962 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.91605\n",
            "Epoch 11/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2037 - accuracy: 0.9114 - val_loss: 0.1936 - val_accuracy: 0.9144\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.91605\n",
            "Epoch 12/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2029 - accuracy: 0.9102 - val_loss: 0.1944 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.91605\n",
            "Epoch 13/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9120 - val_loss: 0.1929 - val_accuracy: 0.9146\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91605\n",
            "Epoch 14/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9082 - val_loss: 0.1934 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.91605\n",
            "Epoch 15/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1999 - accuracy: 0.9114 - val_loss: 0.1928 - val_accuracy: 0.9146\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.91605\n",
            "Epoch 16/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2027 - accuracy: 0.9104 - val_loss: 0.1923 - val_accuracy: 0.9146\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91605\n",
            "Epoch 17/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2066 - accuracy: 0.9069 - val_loss: 0.1916 - val_accuracy: 0.9152\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.91605\n",
            "Epoch 18/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2018 - accuracy: 0.9093 - val_loss: 0.1935 - val_accuracy: 0.9152\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91605\n",
            "Epoch 19/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2032 - accuracy: 0.9103 - val_loss: 0.1911 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91605\n",
            "Epoch 20/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2042 - accuracy: 0.9066 - val_loss: 0.1906 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91605\n",
            "Epoch 21/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2046 - accuracy: 0.9076 - val_loss: 0.1906 - val_accuracy: 0.9162\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.91605 to 0.91621, saving model to model_es.hdf5\n",
            "Epoch 22/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2057 - accuracy: 0.9056 - val_loss: 0.1903 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91621\n",
            "Epoch 23/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1972 - accuracy: 0.9098 - val_loss: 0.1908 - val_accuracy: 0.9153\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91621\n",
            "Epoch 24/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9112 - val_loss: 0.1897 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.91621 to 0.91653, saving model to model_es.hdf5\n",
            "Epoch 25/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9115 - val_loss: 0.1902 - val_accuracy: 0.9149\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91653\n",
            "Epoch 26/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2025 - accuracy: 0.9078 - val_loss: 0.1898 - val_accuracy: 0.9164\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91653\n",
            "Epoch 27/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1997 - accuracy: 0.9101 - val_loss: 0.1897 - val_accuracy: 0.9153\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91653\n",
            "Epoch 28/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2027 - accuracy: 0.9101 - val_loss: 0.1895 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91653\n",
            "Epoch 29/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1998 - accuracy: 0.9087 - val_loss: 0.1886 - val_accuracy: 0.9151\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91653\n",
            "Epoch 30/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2009 - accuracy: 0.9107 - val_loss: 0.1883 - val_accuracy: 0.9155\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91653\n",
            "Epoch 31/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1972 - accuracy: 0.9104 - val_loss: 0.1883 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91653\n",
            "Epoch 32/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1975 - accuracy: 0.9109 - val_loss: 0.1887 - val_accuracy: 0.9146\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91653\n",
            "Epoch 33/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1984 - accuracy: 0.9112 - val_loss: 0.1887 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91653\n",
            "Epoch 34/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9103 - val_loss: 0.1880 - val_accuracy: 0.9151\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91653\n",
            "Epoch 35/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2010 - accuracy: 0.9096 - val_loss: 0.1878 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91653\n",
            "Epoch 36/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9115 - val_loss: 0.1870 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91653\n",
            "Epoch 37/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1967 - accuracy: 0.9118 - val_loss: 0.1866 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91653\n",
            "Epoch 38/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9093 - val_loss: 0.1863 - val_accuracy: 0.9164\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91653\n",
            "Epoch 39/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1922 - accuracy: 0.9123 - val_loss: 0.1864 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91653\n",
            "Epoch 40/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9154 - val_loss: 0.1898 - val_accuracy: 0.9143\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91653\n",
            "Epoch 41/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9142 - val_loss: 0.1868 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91653\n",
            "Epoch 42/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1935 - accuracy: 0.9124 - val_loss: 0.1856 - val_accuracy: 0.9153\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91653\n",
            "Epoch 43/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1943 - accuracy: 0.9120 - val_loss: 0.1874 - val_accuracy: 0.9159\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91653\n",
            "Epoch 44/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1932 - accuracy: 0.9131 - val_loss: 0.1872 - val_accuracy: 0.9161\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91653\n",
            "Epoch 45/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1997 - accuracy: 0.9097 - val_loss: 0.1849 - val_accuracy: 0.9152\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91653\n",
            "Epoch 46/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1936 - accuracy: 0.9115 - val_loss: 0.1846 - val_accuracy: 0.9158\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91653\n",
            "Epoch 47/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1952 - accuracy: 0.9104 - val_loss: 0.1885 - val_accuracy: 0.9157\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91653\n",
            "Epoch 48/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1941 - accuracy: 0.9119 - val_loss: 0.1862 - val_accuracy: 0.9145\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91653\n",
            "Epoch 49/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9135 - val_loss: 0.1858 - val_accuracy: 0.9159\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91653\n",
            "Epoch 50/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1922 - accuracy: 0.9121 - val_loss: 0.1852 - val_accuracy: 0.9161\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91653\n",
            "Epoch 51/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9137 - val_loss: 0.1884 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91653\n",
            "Epoch 52/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1952 - accuracy: 0.9084 - val_loss: 0.1841 - val_accuracy: 0.9158\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91653\n",
            "Epoch 53/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9107 - val_loss: 0.1858 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91653\n",
            "Epoch 54/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1901 - accuracy: 0.9124 - val_loss: 0.1840 - val_accuracy: 0.9159\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91653\n",
            "Epoch 55/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1972 - accuracy: 0.9076 - val_loss: 0.1833 - val_accuracy: 0.9159\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91653\n",
            "Epoch 56/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9128 - val_loss: 0.1846 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91653\n",
            "Epoch 57/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9120 - val_loss: 0.1838 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91653\n",
            "Epoch 58/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9093 - val_loss: 0.1830 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.91653 to 0.91726, saving model to model_es.hdf5\n",
            "Epoch 59/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9104 - val_loss: 0.1841 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91726\n",
            "Epoch 60/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1924 - accuracy: 0.9111 - val_loss: 0.1832 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91726\n",
            "Epoch 61/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1965 - accuracy: 0.9106 - val_loss: 0.1829 - val_accuracy: 0.9151\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91726\n",
            "Epoch 62/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9114 - val_loss: 0.1824 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91726\n",
            "Epoch 63/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9135 - val_loss: 0.1817 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00063: val_accuracy improved from 0.91726 to 0.91750, saving model to model_es.hdf5\n",
            "Epoch 64/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9104 - val_loss: 0.1825 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91750\n",
            "Epoch 65/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9119 - val_loss: 0.1815 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.91750\n",
            "Epoch 66/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1870 - accuracy: 0.9129 - val_loss: 0.1817 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.91750\n",
            "Epoch 67/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9140 - val_loss: 0.1818 - val_accuracy: 0.9176\n",
            "\n",
            "Epoch 00067: val_accuracy improved from 0.91750 to 0.91758, saving model to model_es.hdf5\n",
            "Epoch 68/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9099 - val_loss: 0.1822 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.91758\n",
            "Epoch 69/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9123 - val_loss: 0.1817 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.91758\n",
            "Epoch 70/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9115 - val_loss: 0.1818 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.91758\n",
            "Epoch 71/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1896 - accuracy: 0.9117 - val_loss: 0.1822 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.91758\n",
            "Epoch 72/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1846 - accuracy: 0.9123 - val_loss: 0.1826 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.91758\n",
            "Epoch 73/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9118 - val_loss: 0.1810 - val_accuracy: 0.9167\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.91758\n",
            "Epoch 74/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9131 - val_loss: 0.1816 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.91758\n",
            "Epoch 75/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9125 - val_loss: 0.1802 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.91758\n",
            "Epoch 76/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9118 - val_loss: 0.1808 - val_accuracy: 0.9174\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.91758\n",
            "Epoch 77/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9121 - val_loss: 0.1818 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.91758\n",
            "Epoch 78/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9112 - val_loss: 0.1866 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.91758\n",
            "Epoch 79/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1839 - accuracy: 0.9137 - val_loss: 0.1834 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.91758\n",
            "Epoch 80/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9147 - val_loss: 0.1794 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.91758\n",
            "Epoch 81/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9106 - val_loss: 0.1797 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.91758\n",
            "Epoch 82/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9094 - val_loss: 0.1795 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.91758\n",
            "Epoch 83/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9133 - val_loss: 0.1820 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.91758\n",
            "Epoch 84/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9116 - val_loss: 0.1803 - val_accuracy: 0.9183\n",
            "\n",
            "Epoch 00084: val_accuracy improved from 0.91758 to 0.91831, saving model to model_es.hdf5\n",
            "Epoch 85/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9141 - val_loss: 0.1806 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.91831\n",
            "Epoch 86/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9132 - val_loss: 0.1793 - val_accuracy: 0.9175\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.91831\n",
            "Epoch 87/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9123 - val_loss: 0.1780 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.91831\n",
            "Epoch 88/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9141 - val_loss: 0.1793 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.91831\n",
            "Epoch 89/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1894 - accuracy: 0.9100 - val_loss: 0.1792 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.91831\n",
            "Epoch 90/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1824 - accuracy: 0.9147 - val_loss: 0.1803 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.91831\n",
            "Epoch 91/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9115 - val_loss: 0.1790 - val_accuracy: 0.9167\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.91831\n",
            "Epoch 92/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9108 - val_loss: 0.1800 - val_accuracy: 0.9168\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.91831\n",
            "Epoch 93/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1837 - accuracy: 0.9118 - val_loss: 0.1823 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.91831\n",
            "Epoch 94/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9115 - val_loss: 0.1876 - val_accuracy: 0.9156\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.91831\n",
            "Epoch 95/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1887 - accuracy: 0.9101 - val_loss: 0.1799 - val_accuracy: 0.9169\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.91831\n",
            "Epoch 96/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.9113 - val_loss: 0.1796 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.91831\n",
            "Epoch 97/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9140 - val_loss: 0.1814 - val_accuracy: 0.9167\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.91831\n",
            "Epoch 98/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9108 - val_loss: 0.1809 - val_accuracy: 0.9168\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.91831\n",
            "Epoch 99/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9085 - val_loss: 0.1829 - val_accuracy: 0.9165\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.91831\n",
            "Epoch 100/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9101 - val_loss: 0.1783 - val_accuracy: 0.9172\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.91831\n",
            "Epoch 101/256\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.9122 - val_loss: 0.1776 - val_accuracy: 0.9177\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.91831\n",
            "Epoch 00101: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xN6x2i09YxpS",
        "outputId": "c15cb9b8-2e1c-48f8-9453-9dd5a618ddd1"
      },
      "source": [
        "model_5_ = Sequential()\n",
        "model_5_.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_5_.add(Dense(4,activation='relu'))\n",
        "model_5_.add(Dense(4,activation='relu'))\n",
        "model_5_.add(Dense(1,activation='sigmoid'))\n",
        "model_5_.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model_5_.summary()\n",
        "history = model_5_.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=512)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_38 (Dense)             (None, 8)                 168       \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 229\n",
            "Trainable params: 229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4437 - accuracy: 0.8754 - val_loss: 0.2711 - val_accuracy: 0.9000\n",
            "Epoch 2/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2630 - accuracy: 0.9013 - val_loss: 0.2212 - val_accuracy: 0.9112\n",
            "Epoch 3/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2182 - accuracy: 0.9092 - val_loss: 0.2079 - val_accuracy: 0.9144\n",
            "Epoch 4/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2076 - accuracy: 0.9100 - val_loss: 0.2064 - val_accuracy: 0.9109\n",
            "Epoch 5/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2063 - accuracy: 0.9092 - val_loss: 0.1988 - val_accuracy: 0.9118\n",
            "Epoch 6/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2042 - accuracy: 0.9089 - val_loss: 0.1919 - val_accuracy: 0.9150\n",
            "Epoch 7/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2035 - accuracy: 0.9102 - val_loss: 0.1911 - val_accuracy: 0.9146\n",
            "Epoch 8/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1959 - accuracy: 0.9130 - val_loss: 0.1917 - val_accuracy: 0.9143\n",
            "Epoch 9/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9129 - val_loss: 0.1903 - val_accuracy: 0.9152\n",
            "Epoch 10/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1970 - accuracy: 0.9134 - val_loss: 0.1878 - val_accuracy: 0.9165\n",
            "Epoch 11/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9084 - val_loss: 0.1882 - val_accuracy: 0.9166\n",
            "Epoch 12/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1977 - accuracy: 0.9128 - val_loss: 0.1886 - val_accuracy: 0.9152\n",
            "Epoch 13/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1999 - accuracy: 0.9113 - val_loss: 0.1867 - val_accuracy: 0.9160\n",
            "Epoch 14/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9073 - val_loss: 0.1868 - val_accuracy: 0.9159\n",
            "Epoch 15/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9134 - val_loss: 0.1858 - val_accuracy: 0.9165\n",
            "Epoch 16/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9118 - val_loss: 0.1847 - val_accuracy: 0.9172\n",
            "Epoch 17/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9122 - val_loss: 0.1844 - val_accuracy: 0.9162\n",
            "Epoch 18/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9091 - val_loss: 0.1850 - val_accuracy: 0.9157\n",
            "Epoch 19/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9113 - val_loss: 0.1840 - val_accuracy: 0.9165\n",
            "Epoch 20/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9104 - val_loss: 0.1846 - val_accuracy: 0.9152\n",
            "Epoch 21/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9140 - val_loss: 0.1832 - val_accuracy: 0.9156\n",
            "Epoch 22/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9098 - val_loss: 0.1856 - val_accuracy: 0.9155\n",
            "Epoch 23/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9099 - val_loss: 0.1842 - val_accuracy: 0.9152\n",
            "Epoch 24/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9141 - val_loss: 0.1835 - val_accuracy: 0.9154\n",
            "Epoch 25/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9113 - val_loss: 0.1831 - val_accuracy: 0.9162\n",
            "Epoch 26/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9099 - val_loss: 0.1834 - val_accuracy: 0.9159\n",
            "Epoch 27/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9118 - val_loss: 0.1828 - val_accuracy: 0.9160\n",
            "Epoch 28/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9096 - val_loss: 0.1971 - val_accuracy: 0.9124\n",
            "Epoch 29/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9099 - val_loss: 0.1829 - val_accuracy: 0.9146\n",
            "Epoch 30/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9101 - val_loss: 0.1830 - val_accuracy: 0.9150\n",
            "Epoch 31/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1925 - accuracy: 0.9112 - val_loss: 0.1941 - val_accuracy: 0.9131\n",
            "Epoch 32/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9074 - val_loss: 0.1951 - val_accuracy: 0.9115\n",
            "Epoch 33/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9119 - val_loss: 0.1845 - val_accuracy: 0.9156\n",
            "Epoch 34/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9145 - val_loss: 0.1865 - val_accuracy: 0.9152\n",
            "Epoch 35/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1942 - accuracy: 0.9099 - val_loss: 0.1822 - val_accuracy: 0.9146\n",
            "Epoch 36/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9109 - val_loss: 0.1826 - val_accuracy: 0.9150\n",
            "Epoch 37/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9124 - val_loss: 0.1833 - val_accuracy: 0.9152\n",
            "Epoch 38/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9130 - val_loss: 0.1823 - val_accuracy: 0.9146\n",
            "Epoch 39/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9104 - val_loss: 0.1900 - val_accuracy: 0.9148\n",
            "Epoch 40/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9140 - val_loss: 0.1828 - val_accuracy: 0.9144\n",
            "Epoch 41/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9116 - val_loss: 0.1829 - val_accuracy: 0.9143\n",
            "Epoch 42/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9095 - val_loss: 0.1821 - val_accuracy: 0.9134\n",
            "Epoch 43/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9121 - val_loss: 0.1830 - val_accuracy: 0.9139\n",
            "Epoch 44/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9132 - val_loss: 0.1830 - val_accuracy: 0.9137\n",
            "Epoch 45/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9128 - val_loss: 0.1831 - val_accuracy: 0.9127\n",
            "Epoch 46/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9087 - val_loss: 0.1823 - val_accuracy: 0.9126\n",
            "Epoch 47/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9083 - val_loss: 0.1813 - val_accuracy: 0.9142\n",
            "Epoch 48/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9143 - val_loss: 0.1814 - val_accuracy: 0.9140\n",
            "Epoch 49/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9110 - val_loss: 0.1826 - val_accuracy: 0.9141\n",
            "Epoch 50/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9133 - val_loss: 0.1812 - val_accuracy: 0.9149\n",
            "Epoch 51/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9111 - val_loss: 0.1809 - val_accuracy: 0.9148\n",
            "Epoch 52/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9116 - val_loss: 0.1817 - val_accuracy: 0.9142\n",
            "Epoch 53/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9139 - val_loss: 0.1827 - val_accuracy: 0.9151\n",
            "Epoch 54/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9109 - val_loss: 0.1823 - val_accuracy: 0.9133\n",
            "Epoch 55/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9106 - val_loss: 0.1809 - val_accuracy: 0.9147\n",
            "Epoch 56/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9127 - val_loss: 0.1850 - val_accuracy: 0.9132\n",
            "Epoch 57/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9141 - val_loss: 0.1815 - val_accuracy: 0.9143\n",
            "Epoch 58/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9140 - val_loss: 0.1853 - val_accuracy: 0.9139\n",
            "Epoch 59/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9117 - val_loss: 0.1823 - val_accuracy: 0.9142\n",
            "Epoch 60/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9132 - val_loss: 0.1814 - val_accuracy: 0.9131\n",
            "Epoch 61/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9124 - val_loss: 0.1802 - val_accuracy: 0.9136\n",
            "Epoch 62/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9115 - val_loss: 0.1814 - val_accuracy: 0.9140\n",
            "Epoch 63/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9136 - val_loss: 0.1818 - val_accuracy: 0.9146\n",
            "Epoch 64/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9130 - val_loss: 0.1845 - val_accuracy: 0.9141\n",
            "Epoch 65/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9096 - val_loss: 0.1811 - val_accuracy: 0.9150\n",
            "Epoch 66/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9145 - val_loss: 0.1854 - val_accuracy: 0.9138\n",
            "Epoch 67/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9137 - val_loss: 0.1805 - val_accuracy: 0.9147\n",
            "Epoch 68/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9121 - val_loss: 0.2024 - val_accuracy: 0.9050\n",
            "Epoch 69/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9141 - val_loss: 0.1814 - val_accuracy: 0.9140\n",
            "Epoch 70/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9135 - val_loss: 0.1801 - val_accuracy: 0.9160\n",
            "Epoch 71/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9092 - val_loss: 0.1806 - val_accuracy: 0.9140\n",
            "Epoch 72/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9110 - val_loss: 0.1806 - val_accuracy: 0.9138\n",
            "Epoch 73/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9161 - val_loss: 0.1823 - val_accuracy: 0.9141\n",
            "Epoch 74/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9138 - val_loss: 0.1807 - val_accuracy: 0.9147\n",
            "Epoch 75/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9150 - val_loss: 0.1803 - val_accuracy: 0.9143\n",
            "Epoch 76/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9127 - val_loss: 0.1826 - val_accuracy: 0.9131\n",
            "Epoch 77/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9100 - val_loss: 0.1804 - val_accuracy: 0.9141\n",
            "Epoch 78/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9107 - val_loss: 0.1822 - val_accuracy: 0.9133\n",
            "Epoch 79/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9116 - val_loss: 0.1807 - val_accuracy: 0.9145\n",
            "Epoch 80/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9105 - val_loss: 0.1819 - val_accuracy: 0.9133\n",
            "Epoch 81/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9091 - val_loss: 0.1858 - val_accuracy: 0.9123\n",
            "Epoch 82/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9105 - val_loss: 0.1831 - val_accuracy: 0.9132\n",
            "Epoch 83/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9117 - val_loss: 0.1890 - val_accuracy: 0.9115\n",
            "Epoch 84/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9112 - val_loss: 0.1792 - val_accuracy: 0.9148\n",
            "Epoch 85/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9144 - val_loss: 0.1832 - val_accuracy: 0.9124\n",
            "Epoch 86/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9121 - val_loss: 0.1867 - val_accuracy: 0.9117\n",
            "Epoch 87/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9150 - val_loss: 0.1816 - val_accuracy: 0.9152\n",
            "Epoch 88/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9102 - val_loss: 0.1795 - val_accuracy: 0.9148\n",
            "Epoch 89/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9097 - val_loss: 0.1803 - val_accuracy: 0.9145\n",
            "Epoch 90/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9144 - val_loss: 0.1791 - val_accuracy: 0.9152\n",
            "Epoch 91/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9107 - val_loss: 0.1797 - val_accuracy: 0.9152\n",
            "Epoch 92/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9133 - val_loss: 0.1797 - val_accuracy: 0.9143\n",
            "Epoch 93/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9135 - val_loss: 0.1798 - val_accuracy: 0.9148\n",
            "Epoch 94/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9147 - val_loss: 0.1816 - val_accuracy: 0.9127\n",
            "Epoch 95/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9134 - val_loss: 0.1801 - val_accuracy: 0.9138\n",
            "Epoch 96/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9119 - val_loss: 0.1793 - val_accuracy: 0.9155\n",
            "Epoch 97/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9139 - val_loss: 0.1816 - val_accuracy: 0.9141\n",
            "Epoch 98/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9138 - val_loss: 0.1797 - val_accuracy: 0.9147\n",
            "Epoch 99/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9158 - val_loss: 0.1794 - val_accuracy: 0.9143\n",
            "Epoch 100/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9125 - val_loss: 0.1856 - val_accuracy: 0.9098\n",
            "Epoch 101/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9142 - val_loss: 0.1798 - val_accuracy: 0.9134\n",
            "Epoch 102/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9115 - val_loss: 0.1801 - val_accuracy: 0.9143\n",
            "Epoch 103/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9118 - val_loss: 0.1788 - val_accuracy: 0.9145\n",
            "Epoch 104/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9136 - val_loss: 0.1790 - val_accuracy: 0.9141\n",
            "Epoch 105/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9130 - val_loss: 0.1826 - val_accuracy: 0.9117\n",
            "Epoch 106/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9121 - val_loss: 0.1881 - val_accuracy: 0.9106\n",
            "Epoch 107/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9146 - val_loss: 0.1796 - val_accuracy: 0.9143\n",
            "Epoch 108/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9143 - val_loss: 0.1802 - val_accuracy: 0.9134\n",
            "Epoch 109/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9118 - val_loss: 0.1797 - val_accuracy: 0.9139\n",
            "Epoch 110/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9099 - val_loss: 0.1799 - val_accuracy: 0.9144\n",
            "Epoch 111/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9126 - val_loss: 0.1814 - val_accuracy: 0.9129\n",
            "Epoch 112/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1767 - accuracy: 0.9168 - val_loss: 0.1785 - val_accuracy: 0.9149\n",
            "Epoch 113/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9162 - val_loss: 0.1786 - val_accuracy: 0.9141\n",
            "Epoch 114/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9140 - val_loss: 0.1825 - val_accuracy: 0.9127\n",
            "Epoch 115/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9082 - val_loss: 0.1803 - val_accuracy: 0.9135\n",
            "Epoch 116/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9124 - val_loss: 0.1793 - val_accuracy: 0.9143\n",
            "Epoch 117/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9140 - val_loss: 0.1806 - val_accuracy: 0.9140\n",
            "Epoch 118/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9129 - val_loss: 0.1782 - val_accuracy: 0.9157\n",
            "Epoch 119/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9122 - val_loss: 0.1795 - val_accuracy: 0.9145\n",
            "Epoch 120/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9115 - val_loss: 0.1788 - val_accuracy: 0.9144\n",
            "Epoch 121/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9136 - val_loss: 0.1781 - val_accuracy: 0.9153\n",
            "Epoch 122/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9141 - val_loss: 0.1782 - val_accuracy: 0.9152\n",
            "Epoch 123/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9113 - val_loss: 0.1784 - val_accuracy: 0.9158\n",
            "Epoch 124/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9152 - val_loss: 0.1796 - val_accuracy: 0.9149\n",
            "Epoch 125/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9103 - val_loss: 0.1783 - val_accuracy: 0.9141\n",
            "Epoch 126/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9154 - val_loss: 0.1791 - val_accuracy: 0.9138\n",
            "Epoch 127/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9141 - val_loss: 0.1793 - val_accuracy: 0.9140\n",
            "Epoch 128/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9128 - val_loss: 0.1800 - val_accuracy: 0.9131\n",
            "Epoch 129/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9142 - val_loss: 0.1782 - val_accuracy: 0.9142\n",
            "Epoch 130/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9144 - val_loss: 0.1781 - val_accuracy: 0.9136\n",
            "Epoch 131/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9163 - val_loss: 0.1781 - val_accuracy: 0.9140\n",
            "Epoch 132/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9122 - val_loss: 0.1789 - val_accuracy: 0.9147\n",
            "Epoch 133/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9107 - val_loss: 0.1819 - val_accuracy: 0.9137\n",
            "Epoch 134/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9141 - val_loss: 0.1802 - val_accuracy: 0.9130\n",
            "Epoch 135/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9112 - val_loss: 0.1796 - val_accuracy: 0.9142\n",
            "Epoch 136/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9140 - val_loss: 0.1793 - val_accuracy: 0.9135\n",
            "Epoch 137/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9144 - val_loss: 0.1786 - val_accuracy: 0.9145\n",
            "Epoch 138/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9122 - val_loss: 0.1778 - val_accuracy: 0.9143\n",
            "Epoch 139/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9116 - val_loss: 0.1779 - val_accuracy: 0.9151\n",
            "Epoch 140/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9127 - val_loss: 0.1785 - val_accuracy: 0.9152\n",
            "Epoch 141/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9134 - val_loss: 0.1807 - val_accuracy: 0.9122\n",
            "Epoch 142/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9151 - val_loss: 0.1803 - val_accuracy: 0.9126\n",
            "Epoch 143/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9131 - val_loss: 0.1781 - val_accuracy: 0.9149\n",
            "Epoch 144/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9148 - val_loss: 0.1824 - val_accuracy: 0.9139\n",
            "Epoch 145/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9139 - val_loss: 0.1794 - val_accuracy: 0.9143\n",
            "Epoch 146/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9164 - val_loss: 0.1791 - val_accuracy: 0.9136\n",
            "Epoch 147/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9121 - val_loss: 0.1792 - val_accuracy: 0.9144\n",
            "Epoch 148/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9134 - val_loss: 0.1780 - val_accuracy: 0.9135\n",
            "Epoch 149/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9121 - val_loss: 0.1779 - val_accuracy: 0.9149\n",
            "Epoch 150/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9159 - val_loss: 0.1799 - val_accuracy: 0.9131\n",
            "Epoch 151/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9123 - val_loss: 0.1907 - val_accuracy: 0.9104\n",
            "Epoch 152/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9122 - val_loss: 0.1797 - val_accuracy: 0.9145\n",
            "Epoch 153/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9098 - val_loss: 0.1787 - val_accuracy: 0.9143\n",
            "Epoch 154/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9120 - val_loss: 0.1791 - val_accuracy: 0.9146\n",
            "Epoch 155/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9112 - val_loss: 0.1791 - val_accuracy: 0.9142\n",
            "Epoch 156/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9101 - val_loss: 0.1776 - val_accuracy: 0.9136\n",
            "Epoch 157/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9156 - val_loss: 0.1784 - val_accuracy: 0.9143\n",
            "Epoch 158/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9120 - val_loss: 0.1777 - val_accuracy: 0.9136\n",
            "Epoch 159/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9122 - val_loss: 0.1807 - val_accuracy: 0.9117\n",
            "Epoch 160/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9137 - val_loss: 0.1801 - val_accuracy: 0.9118\n",
            "Epoch 161/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9140 - val_loss: 0.1791 - val_accuracy: 0.9122\n",
            "Epoch 162/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9151 - val_loss: 0.1811 - val_accuracy: 0.9114\n",
            "Epoch 163/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9133 - val_loss: 0.1783 - val_accuracy: 0.9147\n",
            "Epoch 164/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9122 - val_loss: 0.1814 - val_accuracy: 0.9132\n",
            "Epoch 165/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9134 - val_loss: 0.1779 - val_accuracy: 0.9130\n",
            "Epoch 166/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9117 - val_loss: 0.1818 - val_accuracy: 0.9119\n",
            "Epoch 167/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9132 - val_loss: 0.1781 - val_accuracy: 0.9136\n",
            "Epoch 168/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9136 - val_loss: 0.1782 - val_accuracy: 0.9122\n",
            "Epoch 169/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9114 - val_loss: 0.1799 - val_accuracy: 0.9132\n",
            "Epoch 170/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9126 - val_loss: 0.1780 - val_accuracy: 0.9145\n",
            "Epoch 171/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9145 - val_loss: 0.1774 - val_accuracy: 0.9135\n",
            "Epoch 172/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9131 - val_loss: 0.1773 - val_accuracy: 0.9154\n",
            "Epoch 173/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9141 - val_loss: 0.1790 - val_accuracy: 0.9144\n",
            "Epoch 174/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9129 - val_loss: 0.1825 - val_accuracy: 0.9108\n",
            "Epoch 175/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9131 - val_loss: 0.1799 - val_accuracy: 0.9131\n",
            "Epoch 176/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9141 - val_loss: 0.1779 - val_accuracy: 0.9146\n",
            "Epoch 177/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9141 - val_loss: 0.1787 - val_accuracy: 0.9124\n",
            "Epoch 178/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9149 - val_loss: 0.1775 - val_accuracy: 0.9147\n",
            "Epoch 179/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9148 - val_loss: 0.1792 - val_accuracy: 0.9127\n",
            "Epoch 180/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9141 - val_loss: 0.1788 - val_accuracy: 0.9138\n",
            "Epoch 181/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9128 - val_loss: 0.1778 - val_accuracy: 0.9132\n",
            "Epoch 182/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9131 - val_loss: 0.1787 - val_accuracy: 0.9150\n",
            "Epoch 183/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9137 - val_loss: 0.1776 - val_accuracy: 0.9141\n",
            "Epoch 184/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9161 - val_loss: 0.1777 - val_accuracy: 0.9135\n",
            "Epoch 185/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9124 - val_loss: 0.1771 - val_accuracy: 0.9142\n",
            "Epoch 186/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9135 - val_loss: 0.1792 - val_accuracy: 0.9116\n",
            "Epoch 187/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9151 - val_loss: 0.1794 - val_accuracy: 0.9145\n",
            "Epoch 188/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9146 - val_loss: 0.1785 - val_accuracy: 0.9147\n",
            "Epoch 189/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9145 - val_loss: 0.1811 - val_accuracy: 0.9108\n",
            "Epoch 190/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9148 - val_loss: 0.1776 - val_accuracy: 0.9139\n",
            "Epoch 191/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9147 - val_loss: 0.1775 - val_accuracy: 0.9143\n",
            "Epoch 192/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9120 - val_loss: 0.1783 - val_accuracy: 0.9134\n",
            "Epoch 193/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9137 - val_loss: 0.1777 - val_accuracy: 0.9153\n",
            "Epoch 194/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9135 - val_loss: 0.1784 - val_accuracy: 0.9133\n",
            "Epoch 195/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9126 - val_loss: 0.1778 - val_accuracy: 0.9132\n",
            "Epoch 196/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9137 - val_loss: 0.1792 - val_accuracy: 0.9145\n",
            "Epoch 197/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9113 - val_loss: 0.1797 - val_accuracy: 0.9127\n",
            "Epoch 198/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9134 - val_loss: 0.1833 - val_accuracy: 0.9103\n",
            "Epoch 199/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9128 - val_loss: 0.1774 - val_accuracy: 0.9141\n",
            "Epoch 200/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9127 - val_loss: 0.1783 - val_accuracy: 0.9137\n",
            "Epoch 201/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9150 - val_loss: 0.1784 - val_accuracy: 0.9133\n",
            "Epoch 202/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9135 - val_loss: 0.1777 - val_accuracy: 0.9136\n",
            "Epoch 203/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9117 - val_loss: 0.1773 - val_accuracy: 0.9142\n",
            "Epoch 204/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1759 - accuracy: 0.9158 - val_loss: 0.1792 - val_accuracy: 0.9143\n",
            "Epoch 205/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1761 - accuracy: 0.9146 - val_loss: 0.1820 - val_accuracy: 0.9127\n",
            "Epoch 206/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9131 - val_loss: 0.1782 - val_accuracy: 0.9127\n",
            "Epoch 207/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9116 - val_loss: 0.1787 - val_accuracy: 0.9146\n",
            "Epoch 208/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9113 - val_loss: 0.1778 - val_accuracy: 0.9141\n",
            "Epoch 209/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9139 - val_loss: 0.1823 - val_accuracy: 0.9127\n",
            "Epoch 210/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9131 - val_loss: 0.1776 - val_accuracy: 0.9141\n",
            "Epoch 211/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9132 - val_loss: 0.1790 - val_accuracy: 0.9138\n",
            "Epoch 212/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9140 - val_loss: 0.1786 - val_accuracy: 0.9129\n",
            "Epoch 213/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9155 - val_loss: 0.1779 - val_accuracy: 0.9132\n",
            "Epoch 214/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9137 - val_loss: 0.1788 - val_accuracy: 0.9158\n",
            "Epoch 215/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9150 - val_loss: 0.1776 - val_accuracy: 0.9141\n",
            "Epoch 216/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1782 - accuracy: 0.9163 - val_loss: 0.1778 - val_accuracy: 0.9125\n",
            "Epoch 217/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9151 - val_loss: 0.1780 - val_accuracy: 0.9138\n",
            "Epoch 218/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9173 - val_loss: 0.1812 - val_accuracy: 0.9138\n",
            "Epoch 219/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9163 - val_loss: 0.1807 - val_accuracy: 0.9145\n",
            "Epoch 220/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9145 - val_loss: 0.1810 - val_accuracy: 0.9114\n",
            "Epoch 221/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9143 - val_loss: 0.1781 - val_accuracy: 0.9143\n",
            "Epoch 222/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9119 - val_loss: 0.1776 - val_accuracy: 0.9137\n",
            "Epoch 223/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9148 - val_loss: 0.1802 - val_accuracy: 0.9148\n",
            "Epoch 224/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9134 - val_loss: 0.1783 - val_accuracy: 0.9131\n",
            "Epoch 225/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9136 - val_loss: 0.1776 - val_accuracy: 0.9143\n",
            "Epoch 226/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9146 - val_loss: 0.1775 - val_accuracy: 0.9145\n",
            "Epoch 227/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9138 - val_loss: 0.1792 - val_accuracy: 0.9139\n",
            "Epoch 228/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9157 - val_loss: 0.1769 - val_accuracy: 0.9142\n",
            "Epoch 229/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.9160 - val_loss: 0.1772 - val_accuracy: 0.9135\n",
            "Epoch 230/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9152 - val_loss: 0.1799 - val_accuracy: 0.9126\n",
            "Epoch 231/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9103 - val_loss: 0.1767 - val_accuracy: 0.9143\n",
            "Epoch 232/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9135 - val_loss: 0.1802 - val_accuracy: 0.9117\n",
            "Epoch 233/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9154 - val_loss: 0.1770 - val_accuracy: 0.9145\n",
            "Epoch 234/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9176 - val_loss: 0.1772 - val_accuracy: 0.9152\n",
            "Epoch 235/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9149 - val_loss: 0.1779 - val_accuracy: 0.9129\n",
            "Epoch 236/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9152 - val_loss: 0.1771 - val_accuracy: 0.9141\n",
            "Epoch 237/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9162 - val_loss: 0.1768 - val_accuracy: 0.9147\n",
            "Epoch 238/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9145 - val_loss: 0.1787 - val_accuracy: 0.9143\n",
            "Epoch 239/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9158 - val_loss: 0.1775 - val_accuracy: 0.9142\n",
            "Epoch 240/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9155 - val_loss: 0.1772 - val_accuracy: 0.9152\n",
            "Epoch 241/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9155 - val_loss: 0.1776 - val_accuracy: 0.9143\n",
            "Epoch 242/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9129 - val_loss: 0.1780 - val_accuracy: 0.9139\n",
            "Epoch 243/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9143 - val_loss: 0.1785 - val_accuracy: 0.9126\n",
            "Epoch 244/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9141 - val_loss: 0.1767 - val_accuracy: 0.9150\n",
            "Epoch 245/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9141 - val_loss: 0.1804 - val_accuracy: 0.9126\n",
            "Epoch 246/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1732 - accuracy: 0.9174 - val_loss: 0.1819 - val_accuracy: 0.9119\n",
            "Epoch 247/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9132 - val_loss: 0.1764 - val_accuracy: 0.9147\n",
            "Epoch 248/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9157 - val_loss: 0.1784 - val_accuracy: 0.9143\n",
            "Epoch 249/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9119 - val_loss: 0.1779 - val_accuracy: 0.9150\n",
            "Epoch 250/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9126 - val_loss: 0.1818 - val_accuracy: 0.9118\n",
            "Epoch 251/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9144 - val_loss: 0.1770 - val_accuracy: 0.9145\n",
            "Epoch 252/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9148 - val_loss: 0.1772 - val_accuracy: 0.9137\n",
            "Epoch 253/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9123 - val_loss: 0.1776 - val_accuracy: 0.9131\n",
            "Epoch 254/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9142 - val_loss: 0.1799 - val_accuracy: 0.9141\n",
            "Epoch 255/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9133 - val_loss: 0.1780 - val_accuracy: 0.9143\n",
            "Epoch 256/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9155 - val_loss: 0.1806 - val_accuracy: 0.9129\n",
            "Epoch 257/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1739 - accuracy: 0.9159 - val_loss: 0.1775 - val_accuracy: 0.9148\n",
            "Epoch 258/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9118 - val_loss: 0.1766 - val_accuracy: 0.9141\n",
            "Epoch 259/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9128 - val_loss: 0.1785 - val_accuracy: 0.9150\n",
            "Epoch 260/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9135 - val_loss: 0.1804 - val_accuracy: 0.9126\n",
            "Epoch 261/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9121 - val_loss: 0.1775 - val_accuracy: 0.9148\n",
            "Epoch 262/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9112 - val_loss: 0.1804 - val_accuracy: 0.9125\n",
            "Epoch 263/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9138 - val_loss: 0.1790 - val_accuracy: 0.9136\n",
            "Epoch 264/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9169 - val_loss: 0.1776 - val_accuracy: 0.9137\n",
            "Epoch 265/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9145 - val_loss: 0.1764 - val_accuracy: 0.9147\n",
            "Epoch 266/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9136 - val_loss: 0.1815 - val_accuracy: 0.9118\n",
            "Epoch 267/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9163 - val_loss: 0.1798 - val_accuracy: 0.9130\n",
            "Epoch 268/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9137 - val_loss: 0.1843 - val_accuracy: 0.9108\n",
            "Epoch 269/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9149 - val_loss: 0.1770 - val_accuracy: 0.9140\n",
            "Epoch 270/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1753 - accuracy: 0.9178 - val_loss: 0.1771 - val_accuracy: 0.9137\n",
            "Epoch 271/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9148 - val_loss: 0.1765 - val_accuracy: 0.9143\n",
            "Epoch 272/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9133 - val_loss: 0.1766 - val_accuracy: 0.9145\n",
            "Epoch 273/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9139 - val_loss: 0.1767 - val_accuracy: 0.9143\n",
            "Epoch 274/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9149 - val_loss: 0.1816 - val_accuracy: 0.9136\n",
            "Epoch 275/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9165 - val_loss: 0.1782 - val_accuracy: 0.9137\n",
            "Epoch 276/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9139 - val_loss: 0.1774 - val_accuracy: 0.9140\n",
            "Epoch 277/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9154 - val_loss: 0.1764 - val_accuracy: 0.9151\n",
            "Epoch 278/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9151 - val_loss: 0.1916 - val_accuracy: 0.9091\n",
            "Epoch 279/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9150 - val_loss: 0.1778 - val_accuracy: 0.9143\n",
            "Epoch 280/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9134 - val_loss: 0.1766 - val_accuracy: 0.9153\n",
            "Epoch 281/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9161 - val_loss: 0.1777 - val_accuracy: 0.9135\n",
            "Epoch 282/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9141 - val_loss: 0.1763 - val_accuracy: 0.9146\n",
            "Epoch 283/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9144 - val_loss: 0.1775 - val_accuracy: 0.9149\n",
            "Epoch 284/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9159 - val_loss: 0.1776 - val_accuracy: 0.9137\n",
            "Epoch 285/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9167 - val_loss: 0.1791 - val_accuracy: 0.9133\n",
            "Epoch 286/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9152 - val_loss: 0.1771 - val_accuracy: 0.9145\n",
            "Epoch 287/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9148 - val_loss: 0.1770 - val_accuracy: 0.9152\n",
            "Epoch 288/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9146 - val_loss: 0.1793 - val_accuracy: 0.9135\n",
            "Epoch 289/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9117 - val_loss: 0.1780 - val_accuracy: 0.9142\n",
            "Epoch 290/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9159 - val_loss: 0.1762 - val_accuracy: 0.9143\n",
            "Epoch 291/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9145 - val_loss: 0.1784 - val_accuracy: 0.9137\n",
            "Epoch 292/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9135 - val_loss: 0.1865 - val_accuracy: 0.9139\n",
            "Epoch 293/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9154 - val_loss: 0.1783 - val_accuracy: 0.9124\n",
            "Epoch 294/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9133 - val_loss: 0.1768 - val_accuracy: 0.9143\n",
            "Epoch 295/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9152 - val_loss: 0.1782 - val_accuracy: 0.9142\n",
            "Epoch 296/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9134 - val_loss: 0.1780 - val_accuracy: 0.9153\n",
            "Epoch 297/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9156 - val_loss: 0.1779 - val_accuracy: 0.9142\n",
            "Epoch 298/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9165 - val_loss: 0.1767 - val_accuracy: 0.9149\n",
            "Epoch 299/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9176 - val_loss: 0.1770 - val_accuracy: 0.9155\n",
            "Epoch 300/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1761 - accuracy: 0.9158 - val_loss: 0.1785 - val_accuracy: 0.9141\n",
            "Epoch 301/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9136 - val_loss: 0.1799 - val_accuracy: 0.9144\n",
            "Epoch 302/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9148 - val_loss: 0.1774 - val_accuracy: 0.9141\n",
            "Epoch 303/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9137 - val_loss: 0.1768 - val_accuracy: 0.9139\n",
            "Epoch 304/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9171 - val_loss: 0.1782 - val_accuracy: 0.9139\n",
            "Epoch 305/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9127 - val_loss: 0.1791 - val_accuracy: 0.9131\n",
            "Epoch 306/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1755 - accuracy: 0.9169 - val_loss: 0.1767 - val_accuracy: 0.9145\n",
            "Epoch 307/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9147 - val_loss: 0.1783 - val_accuracy: 0.9139\n",
            "Epoch 308/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1745 - accuracy: 0.9169 - val_loss: 0.1766 - val_accuracy: 0.9140\n",
            "Epoch 309/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9127 - val_loss: 0.1793 - val_accuracy: 0.9132\n",
            "Epoch 310/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.9169 - val_loss: 0.1783 - val_accuracy: 0.9138\n",
            "Epoch 311/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9157 - val_loss: 0.1768 - val_accuracy: 0.9140\n",
            "Epoch 312/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9115 - val_loss: 0.1831 - val_accuracy: 0.9112\n",
            "Epoch 313/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9127 - val_loss: 0.1771 - val_accuracy: 0.9139\n",
            "Epoch 314/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9142 - val_loss: 0.1774 - val_accuracy: 0.9135\n",
            "Epoch 315/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.9194 - val_loss: 0.1777 - val_accuracy: 0.9129\n",
            "Epoch 316/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9172 - val_loss: 0.1766 - val_accuracy: 0.9135\n",
            "Epoch 317/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1759 - accuracy: 0.9166 - val_loss: 0.1774 - val_accuracy: 0.9135\n",
            "Epoch 318/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9121 - val_loss: 0.1787 - val_accuracy: 0.9134\n",
            "Epoch 319/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9156 - val_loss: 0.1763 - val_accuracy: 0.9148\n",
            "Epoch 320/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9139 - val_loss: 0.1769 - val_accuracy: 0.9139\n",
            "Epoch 321/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9129 - val_loss: 0.1856 - val_accuracy: 0.9126\n",
            "Epoch 322/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9140 - val_loss: 0.1763 - val_accuracy: 0.9135\n",
            "Epoch 323/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9141 - val_loss: 0.1765 - val_accuracy: 0.9143\n",
            "Epoch 324/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9161 - val_loss: 0.1777 - val_accuracy: 0.9139\n",
            "Epoch 325/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9153 - val_loss: 0.1775 - val_accuracy: 0.9143\n",
            "Epoch 326/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9154 - val_loss: 0.1764 - val_accuracy: 0.9139\n",
            "Epoch 327/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9120 - val_loss: 0.1765 - val_accuracy: 0.9131\n",
            "Epoch 328/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9124 - val_loss: 0.1790 - val_accuracy: 0.9131\n",
            "Epoch 329/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9132 - val_loss: 0.1781 - val_accuracy: 0.9135\n",
            "Epoch 330/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9142 - val_loss: 0.1809 - val_accuracy: 0.9132\n",
            "Epoch 331/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9141 - val_loss: 0.1773 - val_accuracy: 0.9150\n",
            "Epoch 332/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9144 - val_loss: 0.1779 - val_accuracy: 0.9131\n",
            "Epoch 333/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9154 - val_loss: 0.1771 - val_accuracy: 0.9141\n",
            "Epoch 334/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9141 - val_loss: 0.1781 - val_accuracy: 0.9143\n",
            "Epoch 335/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9147 - val_loss: 0.1763 - val_accuracy: 0.9133\n",
            "Epoch 336/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9140 - val_loss: 0.1832 - val_accuracy: 0.9129\n",
            "Epoch 337/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9170 - val_loss: 0.1764 - val_accuracy: 0.9139\n",
            "Epoch 338/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9130 - val_loss: 0.1786 - val_accuracy: 0.9138\n",
            "Epoch 339/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9139 - val_loss: 0.1773 - val_accuracy: 0.9136\n",
            "Epoch 340/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9143 - val_loss: 0.1780 - val_accuracy: 0.9134\n",
            "Epoch 341/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9129 - val_loss: 0.1799 - val_accuracy: 0.9128\n",
            "Epoch 342/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9109 - val_loss: 0.1761 - val_accuracy: 0.9135\n",
            "Epoch 343/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9121 - val_loss: 0.1767 - val_accuracy: 0.9144\n",
            "Epoch 344/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9145 - val_loss: 0.1758 - val_accuracy: 0.9143\n",
            "Epoch 345/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9145 - val_loss: 0.1766 - val_accuracy: 0.9136\n",
            "Epoch 346/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9154 - val_loss: 0.1778 - val_accuracy: 0.9132\n",
            "Epoch 347/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9138 - val_loss: 0.1775 - val_accuracy: 0.9135\n",
            "Epoch 348/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9151 - val_loss: 0.1769 - val_accuracy: 0.9136\n",
            "Epoch 349/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9155 - val_loss: 0.1782 - val_accuracy: 0.9132\n",
            "Epoch 350/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9147 - val_loss: 0.1765 - val_accuracy: 0.9139\n",
            "Epoch 351/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9153 - val_loss: 0.1771 - val_accuracy: 0.9139\n",
            "Epoch 352/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9121 - val_loss: 0.1761 - val_accuracy: 0.9147\n",
            "Epoch 353/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9132 - val_loss: 0.1794 - val_accuracy: 0.9143\n",
            "Epoch 354/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9161 - val_loss: 0.1762 - val_accuracy: 0.9138\n",
            "Epoch 355/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9166 - val_loss: 0.1769 - val_accuracy: 0.9138\n",
            "Epoch 356/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9155 - val_loss: 0.1766 - val_accuracy: 0.9141\n",
            "Epoch 357/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9131 - val_loss: 0.1783 - val_accuracy: 0.9136\n",
            "Epoch 358/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9166 - val_loss: 0.1795 - val_accuracy: 0.9139\n",
            "Epoch 359/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9174 - val_loss: 0.1762 - val_accuracy: 0.9138\n",
            "Epoch 360/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9174 - val_loss: 0.1764 - val_accuracy: 0.9138\n",
            "Epoch 361/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9142 - val_loss: 0.1762 - val_accuracy: 0.9148\n",
            "Epoch 362/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9122 - val_loss: 0.1798 - val_accuracy: 0.9140\n",
            "Epoch 363/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9146 - val_loss: 0.1793 - val_accuracy: 0.9139\n",
            "Epoch 364/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1729 - accuracy: 0.9175 - val_loss: 0.1775 - val_accuracy: 0.9129\n",
            "Epoch 365/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9155 - val_loss: 0.1766 - val_accuracy: 0.9150\n",
            "Epoch 366/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9150 - val_loss: 0.1765 - val_accuracy: 0.9139\n",
            "Epoch 367/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1749 - accuracy: 0.9174 - val_loss: 0.1767 - val_accuracy: 0.9141\n",
            "Epoch 368/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9153 - val_loss: 0.1764 - val_accuracy: 0.9140\n",
            "Epoch 369/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9154 - val_loss: 0.1769 - val_accuracy: 0.9146\n",
            "Epoch 370/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1782 - accuracy: 0.9156 - val_loss: 0.1767 - val_accuracy: 0.9143\n",
            "Epoch 371/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9138 - val_loss: 0.1762 - val_accuracy: 0.9143\n",
            "Epoch 372/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9127 - val_loss: 0.1792 - val_accuracy: 0.9143\n",
            "Epoch 373/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9110 - val_loss: 0.1763 - val_accuracy: 0.9148\n",
            "Epoch 374/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9157 - val_loss: 0.1803 - val_accuracy: 0.9137\n",
            "Epoch 375/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9137 - val_loss: 0.1764 - val_accuracy: 0.9141\n",
            "Epoch 376/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1749 - accuracy: 0.9177 - val_loss: 0.1769 - val_accuracy: 0.9139\n",
            "Epoch 377/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9156 - val_loss: 0.1772 - val_accuracy: 0.9145\n",
            "Epoch 378/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9123 - val_loss: 0.1764 - val_accuracy: 0.9140\n",
            "Epoch 379/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9141 - val_loss: 0.1764 - val_accuracy: 0.9136\n",
            "Epoch 380/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1750 - accuracy: 0.9180 - val_loss: 0.1776 - val_accuracy: 0.9145\n",
            "Epoch 381/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1751 - accuracy: 0.9181 - val_loss: 0.1759 - val_accuracy: 0.9143\n",
            "Epoch 382/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9154 - val_loss: 0.1816 - val_accuracy: 0.9124\n",
            "Epoch 383/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9156 - val_loss: 0.1770 - val_accuracy: 0.9143\n",
            "Epoch 384/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9129 - val_loss: 0.1760 - val_accuracy: 0.9142\n",
            "Epoch 385/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1754 - accuracy: 0.9167 - val_loss: 0.1767 - val_accuracy: 0.9146\n",
            "Epoch 386/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9148 - val_loss: 0.1763 - val_accuracy: 0.9142\n",
            "Epoch 387/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9165 - val_loss: 0.1757 - val_accuracy: 0.9148\n",
            "Epoch 388/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9157 - val_loss: 0.1760 - val_accuracy: 0.9148\n",
            "Epoch 389/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9141 - val_loss: 0.1763 - val_accuracy: 0.9152\n",
            "Epoch 390/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9128 - val_loss: 0.1826 - val_accuracy: 0.9133\n",
            "Epoch 391/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9171 - val_loss: 0.1762 - val_accuracy: 0.9142\n",
            "Epoch 392/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9183 - val_loss: 0.1759 - val_accuracy: 0.9150\n",
            "Epoch 393/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9132 - val_loss: 0.1763 - val_accuracy: 0.9139\n",
            "Epoch 394/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9133 - val_loss: 0.1759 - val_accuracy: 0.9152\n",
            "Epoch 395/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9159 - val_loss: 0.1784 - val_accuracy: 0.9152\n",
            "Epoch 396/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9153 - val_loss: 0.1818 - val_accuracy: 0.9135\n",
            "Epoch 397/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1759 - accuracy: 0.9163 - val_loss: 0.1760 - val_accuracy: 0.9148\n",
            "Epoch 398/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9140 - val_loss: 0.1850 - val_accuracy: 0.9126\n",
            "Epoch 399/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9123 - val_loss: 0.1832 - val_accuracy: 0.9123\n",
            "Epoch 400/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9111 - val_loss: 0.1769 - val_accuracy: 0.9145\n",
            "Epoch 401/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9161 - val_loss: 0.1766 - val_accuracy: 0.9146\n",
            "Epoch 402/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9153 - val_loss: 0.1764 - val_accuracy: 0.9152\n",
            "Epoch 403/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.9152 - val_loss: 0.1776 - val_accuracy: 0.9149\n",
            "Epoch 404/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9158 - val_loss: 0.1771 - val_accuracy: 0.9148\n",
            "Epoch 405/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9148 - val_loss: 0.1763 - val_accuracy: 0.9152\n",
            "Epoch 406/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9147 - val_loss: 0.1769 - val_accuracy: 0.9147\n",
            "Epoch 407/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9143 - val_loss: 0.1763 - val_accuracy: 0.9146\n",
            "Epoch 408/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9176 - val_loss: 0.1771 - val_accuracy: 0.9148\n",
            "Epoch 409/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9152 - val_loss: 0.1758 - val_accuracy: 0.9152\n",
            "Epoch 410/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9167 - val_loss: 0.1768 - val_accuracy: 0.9147\n",
            "Epoch 411/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9145 - val_loss: 0.1761 - val_accuracy: 0.9148\n",
            "Epoch 412/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9156 - val_loss: 0.1764 - val_accuracy: 0.9143\n",
            "Epoch 413/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9114 - val_loss: 0.1769 - val_accuracy: 0.9143\n",
            "Epoch 414/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9174 - val_loss: 0.1768 - val_accuracy: 0.9142\n",
            "Epoch 415/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9150 - val_loss: 0.1767 - val_accuracy: 0.9143\n",
            "Epoch 416/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9178 - val_loss: 0.1772 - val_accuracy: 0.9139\n",
            "Epoch 417/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1767 - accuracy: 0.9147 - val_loss: 0.1769 - val_accuracy: 0.9143\n",
            "Epoch 418/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9169 - val_loss: 0.1768 - val_accuracy: 0.9137\n",
            "Epoch 419/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9165 - val_loss: 0.1788 - val_accuracy: 0.9152\n",
            "Epoch 420/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9174 - val_loss: 0.1760 - val_accuracy: 0.9135\n",
            "Epoch 421/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9126 - val_loss: 0.1783 - val_accuracy: 0.9146\n",
            "Epoch 422/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1750 - accuracy: 0.9177 - val_loss: 0.1767 - val_accuracy: 0.9134\n",
            "Epoch 423/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9152 - val_loss: 0.1792 - val_accuracy: 0.9140\n",
            "Epoch 424/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9153 - val_loss: 0.1771 - val_accuracy: 0.9146\n",
            "Epoch 425/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9144 - val_loss: 0.1804 - val_accuracy: 0.9134\n",
            "Epoch 426/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9114 - val_loss: 0.1761 - val_accuracy: 0.9143\n",
            "Epoch 427/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9168 - val_loss: 0.1765 - val_accuracy: 0.9143\n",
            "Epoch 428/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9146 - val_loss: 0.1763 - val_accuracy: 0.9150\n",
            "Epoch 429/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9171 - val_loss: 0.1761 - val_accuracy: 0.9149\n",
            "Epoch 430/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9168 - val_loss: 0.1768 - val_accuracy: 0.9140\n",
            "Epoch 431/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1763 - accuracy: 0.9165 - val_loss: 0.1770 - val_accuracy: 0.9144\n",
            "Epoch 432/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9165 - val_loss: 0.1802 - val_accuracy: 0.9135\n",
            "Epoch 433/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9134 - val_loss: 0.1784 - val_accuracy: 0.9139\n",
            "Epoch 434/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9119 - val_loss: 0.1766 - val_accuracy: 0.9139\n",
            "Epoch 435/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9156 - val_loss: 0.1754 - val_accuracy: 0.9153\n",
            "Epoch 436/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9121 - val_loss: 0.1762 - val_accuracy: 0.9151\n",
            "Epoch 437/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1753 - accuracy: 0.9185 - val_loss: 0.1770 - val_accuracy: 0.9148\n",
            "Epoch 438/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9180 - val_loss: 0.1756 - val_accuracy: 0.9140\n",
            "Epoch 439/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9162 - val_loss: 0.1828 - val_accuracy: 0.9135\n",
            "Epoch 440/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9142 - val_loss: 0.1768 - val_accuracy: 0.9149\n",
            "Epoch 441/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9142 - val_loss: 0.1768 - val_accuracy: 0.9141\n",
            "Epoch 442/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9147 - val_loss: 0.1758 - val_accuracy: 0.9152\n",
            "Epoch 443/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9177 - val_loss: 0.1767 - val_accuracy: 0.9143\n",
            "Epoch 444/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9167 - val_loss: 0.1757 - val_accuracy: 0.9138\n",
            "Epoch 445/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9157 - val_loss: 0.1761 - val_accuracy: 0.9152\n",
            "Epoch 446/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9149 - val_loss: 0.1761 - val_accuracy: 0.9139\n",
            "Epoch 447/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9160 - val_loss: 0.1760 - val_accuracy: 0.9148\n",
            "Epoch 448/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9155 - val_loss: 0.1762 - val_accuracy: 0.9148\n",
            "Epoch 449/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1766 - accuracy: 0.9161 - val_loss: 0.1786 - val_accuracy: 0.9147\n",
            "Epoch 450/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9130 - val_loss: 0.1761 - val_accuracy: 0.9153\n",
            "Epoch 451/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9146 - val_loss: 0.1753 - val_accuracy: 0.9159\n",
            "Epoch 452/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1751 - accuracy: 0.9176 - val_loss: 0.1771 - val_accuracy: 0.9151\n",
            "Epoch 453/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9173 - val_loss: 0.1755 - val_accuracy: 0.9152\n",
            "Epoch 454/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.9151 - val_loss: 0.1772 - val_accuracy: 0.9147\n",
            "Epoch 455/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9154 - val_loss: 0.1756 - val_accuracy: 0.9143\n",
            "Epoch 456/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9165 - val_loss: 0.1752 - val_accuracy: 0.9154\n",
            "Epoch 457/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9151 - val_loss: 0.1762 - val_accuracy: 0.9141\n",
            "Epoch 458/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9140 - val_loss: 0.1825 - val_accuracy: 0.9140\n",
            "Epoch 459/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1727 - accuracy: 0.9176 - val_loss: 0.1769 - val_accuracy: 0.9144\n",
            "Epoch 460/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9150 - val_loss: 0.1759 - val_accuracy: 0.9147\n",
            "Epoch 461/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9146 - val_loss: 0.1757 - val_accuracy: 0.9148\n",
            "Epoch 462/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9163 - val_loss: 0.1769 - val_accuracy: 0.9152\n",
            "Epoch 463/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9160 - val_loss: 0.1761 - val_accuracy: 0.9148\n",
            "Epoch 464/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9169 - val_loss: 0.1769 - val_accuracy: 0.9155\n",
            "Epoch 465/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1782 - accuracy: 0.9154 - val_loss: 0.1771 - val_accuracy: 0.9156\n",
            "Epoch 466/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9182 - val_loss: 0.1755 - val_accuracy: 0.9144\n",
            "Epoch 467/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9154 - val_loss: 0.1761 - val_accuracy: 0.9153\n",
            "Epoch 468/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9149 - val_loss: 0.1792 - val_accuracy: 0.9143\n",
            "Epoch 469/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9133 - val_loss: 0.1812 - val_accuracy: 0.9139\n",
            "Epoch 470/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1763 - accuracy: 0.9157 - val_loss: 0.1759 - val_accuracy: 0.9160\n",
            "Epoch 471/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9127 - val_loss: 0.1760 - val_accuracy: 0.9158\n",
            "Epoch 472/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9167 - val_loss: 0.1773 - val_accuracy: 0.9146\n",
            "Epoch 473/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9134 - val_loss: 0.1764 - val_accuracy: 0.9148\n",
            "Epoch 474/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9134 - val_loss: 0.1780 - val_accuracy: 0.9144\n",
            "Epoch 475/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1755 - accuracy: 0.9182 - val_loss: 0.1763 - val_accuracy: 0.9149\n",
            "Epoch 476/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9161 - val_loss: 0.1759 - val_accuracy: 0.9149\n",
            "Epoch 477/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1728 - accuracy: 0.9177 - val_loss: 0.1757 - val_accuracy: 0.9153\n",
            "Epoch 478/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1768 - accuracy: 0.9172 - val_loss: 0.1758 - val_accuracy: 0.9147\n",
            "Epoch 479/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1758 - accuracy: 0.9161 - val_loss: 0.1767 - val_accuracy: 0.9152\n",
            "Epoch 480/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9129 - val_loss: 0.1757 - val_accuracy: 0.9155\n",
            "Epoch 481/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1752 - accuracy: 0.9162 - val_loss: 0.1754 - val_accuracy: 0.9157\n",
            "Epoch 482/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1733 - accuracy: 0.9187 - val_loss: 0.1761 - val_accuracy: 0.9141\n",
            "Epoch 483/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9174 - val_loss: 0.1778 - val_accuracy: 0.9143\n",
            "Epoch 484/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1753 - accuracy: 0.9167 - val_loss: 0.1841 - val_accuracy: 0.9127\n",
            "Epoch 485/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9168 - val_loss: 0.1754 - val_accuracy: 0.9160\n",
            "Epoch 486/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9138 - val_loss: 0.1792 - val_accuracy: 0.9142\n",
            "Epoch 487/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9149 - val_loss: 0.1757 - val_accuracy: 0.9150\n",
            "Epoch 488/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1755 - accuracy: 0.9171 - val_loss: 0.1754 - val_accuracy: 0.9149\n",
            "Epoch 489/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1758 - accuracy: 0.9164 - val_loss: 0.1759 - val_accuracy: 0.9155\n",
            "Epoch 490/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9139 - val_loss: 0.1753 - val_accuracy: 0.9150\n",
            "Epoch 491/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9160 - val_loss: 0.1752 - val_accuracy: 0.9147\n",
            "Epoch 492/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9170 - val_loss: 0.1757 - val_accuracy: 0.9146\n",
            "Epoch 493/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9182 - val_loss: 0.1765 - val_accuracy: 0.9148\n",
            "Epoch 494/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9157 - val_loss: 0.1750 - val_accuracy: 0.9147\n",
            "Epoch 495/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1752 - accuracy: 0.9156 - val_loss: 0.1757 - val_accuracy: 0.9149\n",
            "Epoch 496/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9166 - val_loss: 0.1759 - val_accuracy: 0.9149\n",
            "Epoch 497/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9168 - val_loss: 0.1783 - val_accuracy: 0.9137\n",
            "Epoch 498/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9173 - val_loss: 0.1837 - val_accuracy: 0.9131\n",
            "Epoch 499/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1767 - accuracy: 0.9144 - val_loss: 0.1756 - val_accuracy: 0.9144\n",
            "Epoch 500/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9151 - val_loss: 0.1759 - val_accuracy: 0.9147\n",
            "Epoch 501/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9137 - val_loss: 0.1815 - val_accuracy: 0.9134\n",
            "Epoch 502/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9189 - val_loss: 0.1759 - val_accuracy: 0.9152\n",
            "Epoch 503/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9163 - val_loss: 0.1769 - val_accuracy: 0.9151\n",
            "Epoch 504/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9133 - val_loss: 0.1777 - val_accuracy: 0.9146\n",
            "Epoch 505/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9147 - val_loss: 0.1779 - val_accuracy: 0.9149\n",
            "Epoch 506/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1752 - accuracy: 0.9179 - val_loss: 0.1758 - val_accuracy: 0.9156\n",
            "Epoch 507/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9161 - val_loss: 0.1788 - val_accuracy: 0.9139\n",
            "Epoch 508/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9161 - val_loss: 0.1772 - val_accuracy: 0.9146\n",
            "Epoch 509/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9156 - val_loss: 0.1755 - val_accuracy: 0.9150\n",
            "Epoch 510/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1757 - accuracy: 0.9154 - val_loss: 0.1779 - val_accuracy: 0.9152\n",
            "Epoch 511/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1751 - accuracy: 0.9182 - val_loss: 0.1783 - val_accuracy: 0.9156\n",
            "Epoch 512/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9163 - val_loss: 0.1762 - val_accuracy: 0.9156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a2K4-CiUSOMj",
        "outputId": "1747866a-ae2d-4eee-f097-09723b0513e8"
      },
      "source": [
        "model_5 = Sequential()\n",
        "model_5.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_5.add(Dense(4,activation='relu'))\n",
        "model_5.add(Dense(4,activation='relu'))\n",
        "model_5.add(Dense(1,activation='sigmoid'))\n",
        "model_5.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model_5.summary()\n",
        "history = model_5.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_28 (Dense)             (None, 8)                 168       \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 229\n",
            "Trainable params: 229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3876 - accuracy: 0.8834 - val_loss: 0.2625 - val_accuracy: 0.8910\n",
            "Epoch 2/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2615 - accuracy: 0.8864 - val_loss: 0.2159 - val_accuracy: 0.9109\n",
            "Epoch 3/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9078 - val_loss: 0.2035 - val_accuracy: 0.9119\n",
            "Epoch 4/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2127 - accuracy: 0.9061 - val_loss: 0.1983 - val_accuracy: 0.9114\n",
            "Epoch 5/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9110 - val_loss: 0.1976 - val_accuracy: 0.9112\n",
            "Epoch 6/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2045 - accuracy: 0.9074 - val_loss: 0.1961 - val_accuracy: 0.9135\n",
            "Epoch 7/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9115 - val_loss: 0.1958 - val_accuracy: 0.9135\n",
            "Epoch 8/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2021 - accuracy: 0.9084 - val_loss: 0.1991 - val_accuracy: 0.9115\n",
            "Epoch 9/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2032 - accuracy: 0.9070 - val_loss: 0.1959 - val_accuracy: 0.9131\n",
            "Epoch 10/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2031 - accuracy: 0.9081 - val_loss: 0.1956 - val_accuracy: 0.9131\n",
            "Epoch 11/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2006 - accuracy: 0.9115 - val_loss: 0.1969 - val_accuracy: 0.9109\n",
            "Epoch 12/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1976 - accuracy: 0.9112 - val_loss: 0.1955 - val_accuracy: 0.9120\n",
            "Epoch 13/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9131 - val_loss: 0.1945 - val_accuracy: 0.9135\n",
            "Epoch 14/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2056 - accuracy: 0.9061 - val_loss: 0.1982 - val_accuracy: 0.9130\n",
            "Epoch 15/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9139 - val_loss: 0.1948 - val_accuracy: 0.9128\n",
            "Epoch 16/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9108 - val_loss: 0.1985 - val_accuracy: 0.9109\n",
            "Epoch 17/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2037 - accuracy: 0.9074 - val_loss: 0.1972 - val_accuracy: 0.9125\n",
            "Epoch 18/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9127 - val_loss: 0.1974 - val_accuracy: 0.9117\n",
            "Epoch 19/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9113 - val_loss: 0.1947 - val_accuracy: 0.9126\n",
            "Epoch 20/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1995 - accuracy: 0.9113 - val_loss: 0.1986 - val_accuracy: 0.9113\n",
            "Epoch 21/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1983 - accuracy: 0.9116 - val_loss: 0.1947 - val_accuracy: 0.9128\n",
            "Epoch 22/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2013 - accuracy: 0.9090 - val_loss: 0.2018 - val_accuracy: 0.9104\n",
            "Epoch 23/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1984 - accuracy: 0.9109 - val_loss: 0.1955 - val_accuracy: 0.9116\n",
            "Epoch 24/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9102 - val_loss: 0.1942 - val_accuracy: 0.9131\n",
            "Epoch 25/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9106 - val_loss: 0.1945 - val_accuracy: 0.9131\n",
            "Epoch 26/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2012 - accuracy: 0.9113 - val_loss: 0.1939 - val_accuracy: 0.9133\n",
            "Epoch 27/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9122 - val_loss: 0.1949 - val_accuracy: 0.9122\n",
            "Epoch 28/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1976 - accuracy: 0.9106 - val_loss: 0.1950 - val_accuracy: 0.9130\n",
            "Epoch 29/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1988 - accuracy: 0.9094 - val_loss: 0.1935 - val_accuracy: 0.9137\n",
            "Epoch 30/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2011 - accuracy: 0.9085 - val_loss: 0.1934 - val_accuracy: 0.9129\n",
            "Epoch 31/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1950 - accuracy: 0.9139 - val_loss: 0.1935 - val_accuracy: 0.9136\n",
            "Epoch 32/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1920 - accuracy: 0.9127 - val_loss: 0.1941 - val_accuracy: 0.9133\n",
            "Epoch 33/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2019 - accuracy: 0.9108 - val_loss: 0.2055 - val_accuracy: 0.9090\n",
            "Epoch 34/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9119 - val_loss: 0.1926 - val_accuracy: 0.9131\n",
            "Epoch 35/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9118 - val_loss: 0.1926 - val_accuracy: 0.9131\n",
            "Epoch 36/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9114 - val_loss: 0.1927 - val_accuracy: 0.9132\n",
            "Epoch 37/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1964 - accuracy: 0.9125 - val_loss: 0.1968 - val_accuracy: 0.9126\n",
            "Epoch 38/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9096 - val_loss: 0.1932 - val_accuracy: 0.9133\n",
            "Epoch 39/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1994 - accuracy: 0.9098 - val_loss: 0.1922 - val_accuracy: 0.9131\n",
            "Epoch 40/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9101 - val_loss: 0.1913 - val_accuracy: 0.9131\n",
            "Epoch 41/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9139 - val_loss: 0.1914 - val_accuracy: 0.9119\n",
            "Epoch 42/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9113 - val_loss: 0.1919 - val_accuracy: 0.9123\n",
            "Epoch 43/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1946 - accuracy: 0.9118 - val_loss: 0.1903 - val_accuracy: 0.9131\n",
            "Epoch 44/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1955 - accuracy: 0.9125 - val_loss: 0.1903 - val_accuracy: 0.9132\n",
            "Epoch 45/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9129 - val_loss: 0.1899 - val_accuracy: 0.9121\n",
            "Epoch 46/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1948 - accuracy: 0.9113 - val_loss: 0.2016 - val_accuracy: 0.9097\n",
            "Epoch 47/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9125 - val_loss: 0.1890 - val_accuracy: 0.9118\n",
            "Epoch 48/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1905 - accuracy: 0.9115 - val_loss: 0.1893 - val_accuracy: 0.9122\n",
            "Epoch 49/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9103 - val_loss: 0.1912 - val_accuracy: 0.9114\n",
            "Epoch 50/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9125 - val_loss: 0.1891 - val_accuracy: 0.9131\n",
            "Epoch 51/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9137 - val_loss: 0.1908 - val_accuracy: 0.9117\n",
            "Epoch 52/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9131 - val_loss: 0.1946 - val_accuracy: 0.9088\n",
            "Epoch 53/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9107 - val_loss: 0.1887 - val_accuracy: 0.9124\n",
            "Epoch 54/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9131 - val_loss: 0.1868 - val_accuracy: 0.9135\n",
            "Epoch 55/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9117 - val_loss: 0.1867 - val_accuracy: 0.9127\n",
            "Epoch 56/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9139 - val_loss: 0.1889 - val_accuracy: 0.9125\n",
            "Epoch 57/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9130 - val_loss: 0.1866 - val_accuracy: 0.9138\n",
            "Epoch 58/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9116 - val_loss: 0.1863 - val_accuracy: 0.9122\n",
            "Epoch 59/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9144 - val_loss: 0.1857 - val_accuracy: 0.9131\n",
            "Epoch 60/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9117 - val_loss: 0.1869 - val_accuracy: 0.9122\n",
            "Epoch 61/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9113 - val_loss: 0.1918 - val_accuracy: 0.9134\n",
            "Epoch 62/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9120 - val_loss: 0.1893 - val_accuracy: 0.9118\n",
            "Epoch 63/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9140 - val_loss: 0.1890 - val_accuracy: 0.9120\n",
            "Epoch 64/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9170 - val_loss: 0.1892 - val_accuracy: 0.9114\n",
            "Epoch 65/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9100 - val_loss: 0.1849 - val_accuracy: 0.9131\n",
            "Epoch 66/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9139 - val_loss: 0.1883 - val_accuracy: 0.9122\n",
            "Epoch 67/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9128 - val_loss: 0.1896 - val_accuracy: 0.9110\n",
            "Epoch 68/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9126 - val_loss: 0.1851 - val_accuracy: 0.9124\n",
            "Epoch 69/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9112 - val_loss: 0.1846 - val_accuracy: 0.9121\n",
            "Epoch 70/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9131 - val_loss: 0.1961 - val_accuracy: 0.9091\n",
            "Epoch 71/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9143 - val_loss: 0.1847 - val_accuracy: 0.9125\n",
            "Epoch 72/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9100 - val_loss: 0.1864 - val_accuracy: 0.9135\n",
            "Epoch 73/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9121 - val_loss: 0.1863 - val_accuracy: 0.9125\n",
            "Epoch 74/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9140 - val_loss: 0.1865 - val_accuracy: 0.9135\n",
            "Epoch 75/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9141 - val_loss: 0.1840 - val_accuracy: 0.9123\n",
            "Epoch 76/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9141 - val_loss: 0.1855 - val_accuracy: 0.9116\n",
            "Epoch 77/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9138 - val_loss: 0.1894 - val_accuracy: 0.9137\n",
            "Epoch 78/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9115 - val_loss: 0.1891 - val_accuracy: 0.9120\n",
            "Epoch 79/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9120 - val_loss: 0.1854 - val_accuracy: 0.9137\n",
            "Epoch 80/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9125 - val_loss: 0.1849 - val_accuracy: 0.9129\n",
            "Epoch 81/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9139 - val_loss: 0.1838 - val_accuracy: 0.9116\n",
            "Epoch 82/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9137 - val_loss: 0.1881 - val_accuracy: 0.9115\n",
            "Epoch 83/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9115 - val_loss: 0.1846 - val_accuracy: 0.9135\n",
            "Epoch 84/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9110 - val_loss: 0.1855 - val_accuracy: 0.9128\n",
            "Epoch 85/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9123 - val_loss: 0.1846 - val_accuracy: 0.9139\n",
            "Epoch 86/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9163 - val_loss: 0.1831 - val_accuracy: 0.9124\n",
            "Epoch 87/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9161 - val_loss: 0.1849 - val_accuracy: 0.9136\n",
            "Epoch 88/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9104 - val_loss: 0.1827 - val_accuracy: 0.9145\n",
            "Epoch 89/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9118 - val_loss: 0.1829 - val_accuracy: 0.9131\n",
            "Epoch 90/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9150 - val_loss: 0.1839 - val_accuracy: 0.9122\n",
            "Epoch 91/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9116 - val_loss: 0.1836 - val_accuracy: 0.9142\n",
            "Epoch 92/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9135 - val_loss: 0.1837 - val_accuracy: 0.9130\n",
            "Epoch 93/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9135 - val_loss: 0.1845 - val_accuracy: 0.9135\n",
            "Epoch 94/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9165 - val_loss: 0.1826 - val_accuracy: 0.9131\n",
            "Epoch 95/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9184 - val_loss: 0.1835 - val_accuracy: 0.9124\n",
            "Epoch 96/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9128 - val_loss: 0.1856 - val_accuracy: 0.9144\n",
            "Epoch 97/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9133 - val_loss: 0.1871 - val_accuracy: 0.9114\n",
            "Epoch 98/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9138 - val_loss: 0.1842 - val_accuracy: 0.9118\n",
            "Epoch 99/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9145 - val_loss: 0.1862 - val_accuracy: 0.9122\n",
            "Epoch 100/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9136 - val_loss: 0.1870 - val_accuracy: 0.9106\n",
            "Epoch 101/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9123 - val_loss: 0.1830 - val_accuracy: 0.9129\n",
            "Epoch 102/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9134 - val_loss: 0.1822 - val_accuracy: 0.9127\n",
            "Epoch 103/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9149 - val_loss: 0.1830 - val_accuracy: 0.9132\n",
            "Epoch 104/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9142 - val_loss: 0.1834 - val_accuracy: 0.9130\n",
            "Epoch 105/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9139 - val_loss: 0.1869 - val_accuracy: 0.9119\n",
            "Epoch 106/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9166 - val_loss: 0.1854 - val_accuracy: 0.9117\n",
            "Epoch 107/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9155 - val_loss: 0.1826 - val_accuracy: 0.9130\n",
            "Epoch 108/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9134 - val_loss: 0.1900 - val_accuracy: 0.9141\n",
            "Epoch 109/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9125 - val_loss: 0.1824 - val_accuracy: 0.9135\n",
            "Epoch 110/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9141 - val_loss: 0.1828 - val_accuracy: 0.9137\n",
            "Epoch 111/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9112 - val_loss: 0.1831 - val_accuracy: 0.9138\n",
            "Epoch 112/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9124 - val_loss: 0.1828 - val_accuracy: 0.9122\n",
            "Epoch 113/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9133 - val_loss: 0.1815 - val_accuracy: 0.9137\n",
            "Epoch 114/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9124 - val_loss: 0.1824 - val_accuracy: 0.9129\n",
            "Epoch 115/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9123 - val_loss: 0.1873 - val_accuracy: 0.9113\n",
            "Epoch 116/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9165 - val_loss: 0.1838 - val_accuracy: 0.9142\n",
            "Epoch 117/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9145 - val_loss: 0.1821 - val_accuracy: 0.9128\n",
            "Epoch 118/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9151 - val_loss: 0.1829 - val_accuracy: 0.9131\n",
            "Epoch 119/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9143 - val_loss: 0.1817 - val_accuracy: 0.9143\n",
            "Epoch 120/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9137 - val_loss: 0.1866 - val_accuracy: 0.9118\n",
            "Epoch 121/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9172 - val_loss: 0.1852 - val_accuracy: 0.9131\n",
            "Epoch 122/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9134 - val_loss: 0.1833 - val_accuracy: 0.9139\n",
            "Epoch 123/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9146 - val_loss: 0.1839 - val_accuracy: 0.9119\n",
            "Epoch 124/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9114 - val_loss: 0.1841 - val_accuracy: 0.9134\n",
            "Epoch 125/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9132 - val_loss: 0.1817 - val_accuracy: 0.9138\n",
            "Epoch 126/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9145 - val_loss: 0.1820 - val_accuracy: 0.9140\n",
            "Epoch 127/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9155 - val_loss: 0.1825 - val_accuracy: 0.9137\n",
            "Epoch 128/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9180 - val_loss: 0.1815 - val_accuracy: 0.9136\n",
            "Epoch 129/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9163 - val_loss: 0.1810 - val_accuracy: 0.9147\n",
            "Epoch 130/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9143 - val_loss: 0.1815 - val_accuracy: 0.9136\n",
            "Epoch 131/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9138 - val_loss: 0.1811 - val_accuracy: 0.9139\n",
            "Epoch 132/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9135 - val_loss: 0.1816 - val_accuracy: 0.9125\n",
            "Epoch 133/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9149 - val_loss: 0.1826 - val_accuracy: 0.9133\n",
            "Epoch 134/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9155 - val_loss: 0.1835 - val_accuracy: 0.9133\n",
            "Epoch 135/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9128 - val_loss: 0.1822 - val_accuracy: 0.9137\n",
            "Epoch 136/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9153 - val_loss: 0.1849 - val_accuracy: 0.9110\n",
            "Epoch 137/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9142 - val_loss: 0.1820 - val_accuracy: 0.9137\n",
            "Epoch 138/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9143 - val_loss: 0.1816 - val_accuracy: 0.9125\n",
            "Epoch 139/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9141 - val_loss: 0.1846 - val_accuracy: 0.9143\n",
            "Epoch 140/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9154 - val_loss: 0.1819 - val_accuracy: 0.9145\n",
            "Epoch 141/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9159 - val_loss: 0.1819 - val_accuracy: 0.9139\n",
            "Epoch 142/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9186 - val_loss: 0.1817 - val_accuracy: 0.9131\n",
            "Epoch 143/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9158 - val_loss: 0.1850 - val_accuracy: 0.9138\n",
            "Epoch 144/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9157 - val_loss: 0.1828 - val_accuracy: 0.9135\n",
            "Epoch 145/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9128 - val_loss: 0.1820 - val_accuracy: 0.9126\n",
            "Epoch 146/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9150 - val_loss: 0.1810 - val_accuracy: 0.9126\n",
            "Epoch 147/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9124 - val_loss: 0.1807 - val_accuracy: 0.9135\n",
            "Epoch 148/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9118 - val_loss: 0.1812 - val_accuracy: 0.9129\n",
            "Epoch 149/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9143 - val_loss: 0.1826 - val_accuracy: 0.9122\n",
            "Epoch 150/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9161 - val_loss: 0.1814 - val_accuracy: 0.9131\n",
            "Epoch 151/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9145 - val_loss: 0.1827 - val_accuracy: 0.9148\n",
            "Epoch 152/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9153 - val_loss: 0.1814 - val_accuracy: 0.9143\n",
            "Epoch 153/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9140 - val_loss: 0.1815 - val_accuracy: 0.9132\n",
            "Epoch 154/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9151 - val_loss: 0.1815 - val_accuracy: 0.9135\n",
            "Epoch 155/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9144 - val_loss: 0.1814 - val_accuracy: 0.9133\n",
            "Epoch 156/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9161 - val_loss: 0.1805 - val_accuracy: 0.9131\n",
            "Epoch 157/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9135 - val_loss: 0.1813 - val_accuracy: 0.9129\n",
            "Epoch 158/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9146 - val_loss: 0.1820 - val_accuracy: 0.9121\n",
            "Epoch 159/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9141 - val_loss: 0.1842 - val_accuracy: 0.9140\n",
            "Epoch 160/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9169 - val_loss: 0.1844 - val_accuracy: 0.9111\n",
            "Epoch 161/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9125 - val_loss: 0.1832 - val_accuracy: 0.9128\n",
            "Epoch 162/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9149 - val_loss: 0.1807 - val_accuracy: 0.9135\n",
            "Epoch 163/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9163 - val_loss: 0.1803 - val_accuracy: 0.9141\n",
            "Epoch 164/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9141 - val_loss: 0.1823 - val_accuracy: 0.9125\n",
            "Epoch 165/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9176 - val_loss: 0.1805 - val_accuracy: 0.9140\n",
            "Epoch 166/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9161 - val_loss: 0.1804 - val_accuracy: 0.9124\n",
            "Epoch 167/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9165 - val_loss: 0.1822 - val_accuracy: 0.9131\n",
            "Epoch 168/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9158 - val_loss: 0.1804 - val_accuracy: 0.9131\n",
            "Epoch 169/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9155 - val_loss: 0.1806 - val_accuracy: 0.9137\n",
            "Epoch 170/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9157 - val_loss: 0.1810 - val_accuracy: 0.9137\n",
            "Epoch 171/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9126 - val_loss: 0.1802 - val_accuracy: 0.9144\n",
            "Epoch 172/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9168 - val_loss: 0.1824 - val_accuracy: 0.9126\n",
            "Epoch 173/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9146 - val_loss: 0.1830 - val_accuracy: 0.9131\n",
            "Epoch 174/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9165 - val_loss: 0.1810 - val_accuracy: 0.9139\n",
            "Epoch 175/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9132 - val_loss: 0.1822 - val_accuracy: 0.9118\n",
            "Epoch 176/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9164 - val_loss: 0.1806 - val_accuracy: 0.9131\n",
            "Epoch 177/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9138 - val_loss: 0.1802 - val_accuracy: 0.9134\n",
            "Epoch 178/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9127 - val_loss: 0.1842 - val_accuracy: 0.9125\n",
            "Epoch 179/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9167 - val_loss: 0.1800 - val_accuracy: 0.9135\n",
            "Epoch 180/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9144 - val_loss: 0.1820 - val_accuracy: 0.9135\n",
            "Epoch 181/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9139 - val_loss: 0.1813 - val_accuracy: 0.9135\n",
            "Epoch 182/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9153 - val_loss: 0.1810 - val_accuracy: 0.9131\n",
            "Epoch 183/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9142 - val_loss: 0.1832 - val_accuracy: 0.9119\n",
            "Epoch 184/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9146 - val_loss: 0.1809 - val_accuracy: 0.9126\n",
            "Epoch 185/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9151 - val_loss: 0.1800 - val_accuracy: 0.9132\n",
            "Epoch 186/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9169 - val_loss: 0.1804 - val_accuracy: 0.9136\n",
            "Epoch 187/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9139 - val_loss: 0.1799 - val_accuracy: 0.9123\n",
            "Epoch 188/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9149 - val_loss: 0.1819 - val_accuracy: 0.9129\n",
            "Epoch 189/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9171 - val_loss: 0.1817 - val_accuracy: 0.9134\n",
            "Epoch 190/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9181 - val_loss: 0.1800 - val_accuracy: 0.9143\n",
            "Epoch 191/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9148 - val_loss: 0.1809 - val_accuracy: 0.9139\n",
            "Epoch 192/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9174 - val_loss: 0.1815 - val_accuracy: 0.9137\n",
            "Epoch 193/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9153 - val_loss: 0.1804 - val_accuracy: 0.9128\n",
            "Epoch 194/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1777 - accuracy: 0.9178 - val_loss: 0.1809 - val_accuracy: 0.9137\n",
            "Epoch 195/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9154 - val_loss: 0.1808 - val_accuracy: 0.9131\n",
            "Epoch 196/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9128 - val_loss: 0.1830 - val_accuracy: 0.9139\n",
            "Epoch 197/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9133 - val_loss: 0.1809 - val_accuracy: 0.9143\n",
            "Epoch 198/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9168 - val_loss: 0.1862 - val_accuracy: 0.9112\n",
            "Epoch 199/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9165 - val_loss: 0.1823 - val_accuracy: 0.9122\n",
            "Epoch 200/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9139 - val_loss: 0.1806 - val_accuracy: 0.9131\n",
            "Epoch 201/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1755 - accuracy: 0.9201 - val_loss: 0.1798 - val_accuracy: 0.9133\n",
            "Epoch 202/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9123 - val_loss: 0.1802 - val_accuracy: 0.9127\n",
            "Epoch 203/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9148 - val_loss: 0.1819 - val_accuracy: 0.9127\n",
            "Epoch 204/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9136 - val_loss: 0.1792 - val_accuracy: 0.9135\n",
            "Epoch 205/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9152 - val_loss: 0.1824 - val_accuracy: 0.9130\n",
            "Epoch 206/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9178 - val_loss: 0.1808 - val_accuracy: 0.9133\n",
            "Epoch 207/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9164 - val_loss: 0.1836 - val_accuracy: 0.9124\n",
            "Epoch 208/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9168 - val_loss: 0.1845 - val_accuracy: 0.9148\n",
            "Epoch 209/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9143 - val_loss: 0.1810 - val_accuracy: 0.9141\n",
            "Epoch 210/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9158 - val_loss: 0.1841 - val_accuracy: 0.9109\n",
            "Epoch 211/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9162 - val_loss: 0.1804 - val_accuracy: 0.9140\n",
            "Epoch 212/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9142 - val_loss: 0.1796 - val_accuracy: 0.9139\n",
            "Epoch 213/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9138 - val_loss: 0.1797 - val_accuracy: 0.9139\n",
            "Epoch 214/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9155 - val_loss: 0.1809 - val_accuracy: 0.9126\n",
            "Epoch 215/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9168 - val_loss: 0.1807 - val_accuracy: 0.9131\n",
            "Epoch 216/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9148 - val_loss: 0.1801 - val_accuracy: 0.9135\n",
            "Epoch 217/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9144 - val_loss: 0.1801 - val_accuracy: 0.9135\n",
            "Epoch 218/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9148 - val_loss: 0.1813 - val_accuracy: 0.9122\n",
            "Epoch 219/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9161 - val_loss: 0.1810 - val_accuracy: 0.9143\n",
            "Epoch 220/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9144 - val_loss: 0.1821 - val_accuracy: 0.9118\n",
            "Epoch 221/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9134 - val_loss: 0.1802 - val_accuracy: 0.9139\n",
            "Epoch 222/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9159 - val_loss: 0.1797 - val_accuracy: 0.9142\n",
            "Epoch 223/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9150 - val_loss: 0.1809 - val_accuracy: 0.9141\n",
            "Epoch 224/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9155 - val_loss: 0.1801 - val_accuracy: 0.9129\n",
            "Epoch 225/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9158 - val_loss: 0.1808 - val_accuracy: 0.9127\n",
            "Epoch 226/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9178 - val_loss: 0.1798 - val_accuracy: 0.9135\n",
            "Epoch 227/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9145 - val_loss: 0.1809 - val_accuracy: 0.9134\n",
            "Epoch 228/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9181 - val_loss: 0.1790 - val_accuracy: 0.9136\n",
            "Epoch 229/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9159 - val_loss: 0.1797 - val_accuracy: 0.9137\n",
            "Epoch 230/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9131 - val_loss: 0.1835 - val_accuracy: 0.9144\n",
            "Epoch 231/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9164 - val_loss: 0.1792 - val_accuracy: 0.9140\n",
            "Epoch 232/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9145 - val_loss: 0.1793 - val_accuracy: 0.9146\n",
            "Epoch 233/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9160 - val_loss: 0.1827 - val_accuracy: 0.9130\n",
            "Epoch 234/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9186 - val_loss: 0.1794 - val_accuracy: 0.9135\n",
            "Epoch 235/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9155 - val_loss: 0.1832 - val_accuracy: 0.9122\n",
            "Epoch 236/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9135 - val_loss: 0.1819 - val_accuracy: 0.9143\n",
            "Epoch 237/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9143 - val_loss: 0.1824 - val_accuracy: 0.9127\n",
            "Epoch 238/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9181 - val_loss: 0.1834 - val_accuracy: 0.9119\n",
            "Epoch 239/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9154 - val_loss: 0.1790 - val_accuracy: 0.9148\n",
            "Epoch 240/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9141 - val_loss: 0.1830 - val_accuracy: 0.9129\n",
            "Epoch 241/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9129 - val_loss: 0.1833 - val_accuracy: 0.9125\n",
            "Epoch 242/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9173 - val_loss: 0.1790 - val_accuracy: 0.9132\n",
            "Epoch 243/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9145 - val_loss: 0.1811 - val_accuracy: 0.9145\n",
            "Epoch 244/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9140 - val_loss: 0.1805 - val_accuracy: 0.9138\n",
            "Epoch 245/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9153 - val_loss: 0.1809 - val_accuracy: 0.9143\n",
            "Epoch 246/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9155 - val_loss: 0.1797 - val_accuracy: 0.9136\n",
            "Epoch 247/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9154 - val_loss: 0.1798 - val_accuracy: 0.9136\n",
            "Epoch 248/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9144 - val_loss: 0.1797 - val_accuracy: 0.9130\n",
            "Epoch 249/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1714 - accuracy: 0.9212 - val_loss: 0.1795 - val_accuracy: 0.9136\n",
            "Epoch 250/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1749 - accuracy: 0.9179 - val_loss: 0.1794 - val_accuracy: 0.9128\n",
            "Epoch 251/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9138 - val_loss: 0.1790 - val_accuracy: 0.9138\n",
            "Epoch 252/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9155 - val_loss: 0.1797 - val_accuracy: 0.9136\n",
            "Epoch 253/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9162 - val_loss: 0.1792 - val_accuracy: 0.9140\n",
            "Epoch 254/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9173 - val_loss: 0.1792 - val_accuracy: 0.9141\n",
            "Epoch 255/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9133 - val_loss: 0.1788 - val_accuracy: 0.9144\n",
            "Epoch 256/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9169 - val_loss: 0.1842 - val_accuracy: 0.9114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M7OP4zKJvHXc",
        "outputId": "55f44284-343a-4b67-d00b-f6236a43dceb"
      },
      "source": [
        "#0.3\n",
        "model5 = Sequential()\n",
        "model5.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model5.add(Dense(4,activation='relu'))\n",
        "model5.add(Dense(4,activation='relu'))\n",
        "model5.add(Dense(1,activation='sigmoid'))\n",
        "model5.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model5.summary()\n",
        "history = model5.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 8)                 168       \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 229\n",
            "Trainable params: 229\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4105 - accuracy: 0.8506 - val_loss: 0.2543 - val_accuracy: 0.9025\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2489 - accuracy: 0.9014 - val_loss: 0.2130 - val_accuracy: 0.9125\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2177 - accuracy: 0.9082 - val_loss: 0.2047 - val_accuracy: 0.9133\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2070 - accuracy: 0.9116 - val_loss: 0.2047 - val_accuracy: 0.9126\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2039 - accuracy: 0.9096 - val_loss: 0.1992 - val_accuracy: 0.9136\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2006 - accuracy: 0.9097 - val_loss: 0.1971 - val_accuracy: 0.9135\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2009 - accuracy: 0.9113 - val_loss: 0.1966 - val_accuracy: 0.9127\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2023 - accuracy: 0.9093 - val_loss: 0.1962 - val_accuracy: 0.9122\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1985 - accuracy: 0.9090 - val_loss: 0.1932 - val_accuracy: 0.9118\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9098 - val_loss: 0.1917 - val_accuracy: 0.9151\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9129 - val_loss: 0.1913 - val_accuracy: 0.9133\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1970 - accuracy: 0.9100 - val_loss: 0.1906 - val_accuracy: 0.9146\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9108 - val_loss: 0.1902 - val_accuracy: 0.9147\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1963 - accuracy: 0.9087 - val_loss: 0.1880 - val_accuracy: 0.9154\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1924 - accuracy: 0.9112 - val_loss: 0.1881 - val_accuracy: 0.9152\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9110 - val_loss: 0.1878 - val_accuracy: 0.9140\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1908 - accuracy: 0.9108 - val_loss: 0.1878 - val_accuracy: 0.9135\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9120 - val_loss: 0.1869 - val_accuracy: 0.9144\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9098 - val_loss: 0.1870 - val_accuracy: 0.9156\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9129 - val_loss: 0.1896 - val_accuracy: 0.9153\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9114 - val_loss: 0.1854 - val_accuracy: 0.9149\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1929 - accuracy: 0.9107 - val_loss: 0.1862 - val_accuracy: 0.9156\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1945 - accuracy: 0.9099 - val_loss: 0.1856 - val_accuracy: 0.9165\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9137 - val_loss: 0.1898 - val_accuracy: 0.9147\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9101 - val_loss: 0.1864 - val_accuracy: 0.9165\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1938 - accuracy: 0.9099 - val_loss: 0.1893 - val_accuracy: 0.9142\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9126 - val_loss: 0.1843 - val_accuracy: 0.9174\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1941 - accuracy: 0.9095 - val_loss: 0.1865 - val_accuracy: 0.9153\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9075 - val_loss: 0.1874 - val_accuracy: 0.9148\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9111 - val_loss: 0.1856 - val_accuracy: 0.9151\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9118 - val_loss: 0.1832 - val_accuracy: 0.9164\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9103 - val_loss: 0.1851 - val_accuracy: 0.9169\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9133 - val_loss: 0.1843 - val_accuracy: 0.9155\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9120 - val_loss: 0.1832 - val_accuracy: 0.9163\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9153 - val_loss: 0.1865 - val_accuracy: 0.9144\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1923 - accuracy: 0.9099 - val_loss: 0.1827 - val_accuracy: 0.9155\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1891 - accuracy: 0.9112 - val_loss: 0.1848 - val_accuracy: 0.9159\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1890 - accuracy: 0.9119 - val_loss: 0.1834 - val_accuracy: 0.9161\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1877 - accuracy: 0.9111 - val_loss: 0.1834 - val_accuracy: 0.9160\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1891 - accuracy: 0.9114 - val_loss: 0.1823 - val_accuracy: 0.9164\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9110 - val_loss: 0.1826 - val_accuracy: 0.9156\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9120 - val_loss: 0.1843 - val_accuracy: 0.9150\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9128 - val_loss: 0.1887 - val_accuracy: 0.9116\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9106 - val_loss: 0.1816 - val_accuracy: 0.9158\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9107 - val_loss: 0.1838 - val_accuracy: 0.9137\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9091 - val_loss: 0.1872 - val_accuracy: 0.9122\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9125 - val_loss: 0.1823 - val_accuracy: 0.9160\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9143 - val_loss: 0.1812 - val_accuracy: 0.9155\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1837 - accuracy: 0.9136 - val_loss: 0.1824 - val_accuracy: 0.9155\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1923 - accuracy: 0.9113 - val_loss: 0.1812 - val_accuracy: 0.9160\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1809 - accuracy: 0.9116 - val_loss: 0.1812 - val_accuracy: 0.9155\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1871 - accuracy: 0.9121 - val_loss: 0.1808 - val_accuracy: 0.9156\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1844 - accuracy: 0.9131 - val_loss: 0.1835 - val_accuracy: 0.9165\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1893 - accuracy: 0.9108 - val_loss: 0.1863 - val_accuracy: 0.9149\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9133 - val_loss: 0.1814 - val_accuracy: 0.9166\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9109 - val_loss: 0.1817 - val_accuracy: 0.9167\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1857 - accuracy: 0.9132 - val_loss: 0.1823 - val_accuracy: 0.9166\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9129 - val_loss: 0.1841 - val_accuracy: 0.9127\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1846 - accuracy: 0.9125 - val_loss: 0.1810 - val_accuracy: 0.9164\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1870 - accuracy: 0.9081 - val_loss: 0.1816 - val_accuracy: 0.9168\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9152 - val_loss: 0.1801 - val_accuracy: 0.9162\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9134 - val_loss: 0.1810 - val_accuracy: 0.9162\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9102 - val_loss: 0.1845 - val_accuracy: 0.9135\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9113 - val_loss: 0.1892 - val_accuracy: 0.9152\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9117 - val_loss: 0.1806 - val_accuracy: 0.9143\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9136 - val_loss: 0.1798 - val_accuracy: 0.9167\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9106 - val_loss: 0.1802 - val_accuracy: 0.9165\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9143 - val_loss: 0.1806 - val_accuracy: 0.9164\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9106 - val_loss: 0.1804 - val_accuracy: 0.9164\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9126 - val_loss: 0.1811 - val_accuracy: 0.9166\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9110 - val_loss: 0.1818 - val_accuracy: 0.9165\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9106 - val_loss: 0.1805 - val_accuracy: 0.9169\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1839 - accuracy: 0.9127 - val_loss: 0.1793 - val_accuracy: 0.9169\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1827 - accuracy: 0.9122 - val_loss: 0.1820 - val_accuracy: 0.9161\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1834 - accuracy: 0.9131 - val_loss: 0.1872 - val_accuracy: 0.9167\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1864 - accuracy: 0.9136 - val_loss: 0.1833 - val_accuracy: 0.9138\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9127 - val_loss: 0.1809 - val_accuracy: 0.9160\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9106 - val_loss: 0.1797 - val_accuracy: 0.9165\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.9148 - val_loss: 0.1795 - val_accuracy: 0.9160\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1838 - accuracy: 0.9109 - val_loss: 0.1845 - val_accuracy: 0.9164\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1820 - accuracy: 0.9148 - val_loss: 0.1825 - val_accuracy: 0.9164\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9118 - val_loss: 0.1811 - val_accuracy: 0.9167\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1797 - accuracy: 0.9129 - val_loss: 0.1819 - val_accuracy: 0.9159\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9136 - val_loss: 0.1800 - val_accuracy: 0.9163\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9110 - val_loss: 0.1829 - val_accuracy: 0.9166\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9110 - val_loss: 0.1799 - val_accuracy: 0.9156\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1840 - accuracy: 0.9106 - val_loss: 0.1801 - val_accuracy: 0.9166\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9135 - val_loss: 0.1821 - val_accuracy: 0.9160\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9140 - val_loss: 0.1812 - val_accuracy: 0.9168\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9117 - val_loss: 0.1797 - val_accuracy: 0.9146\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9129 - val_loss: 0.1802 - val_accuracy: 0.9152\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1789 - accuracy: 0.9156 - val_loss: 0.1807 - val_accuracy: 0.9165\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1806 - accuracy: 0.9134 - val_loss: 0.1803 - val_accuracy: 0.9159\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1781 - accuracy: 0.9179 - val_loss: 0.1795 - val_accuracy: 0.9162\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9124 - val_loss: 0.1849 - val_accuracy: 0.9160\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9145 - val_loss: 0.1843 - val_accuracy: 0.9165\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9151 - val_loss: 0.1811 - val_accuracy: 0.9167\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1802 - accuracy: 0.9143 - val_loss: 0.1812 - val_accuracy: 0.9135\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9125 - val_loss: 0.1798 - val_accuracy: 0.9167\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1823 - accuracy: 0.9143 - val_loss: 0.1799 - val_accuracy: 0.9167\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9135 - val_loss: 0.1796 - val_accuracy: 0.9161\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.9128 - val_loss: 0.1811 - val_accuracy: 0.9156\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1792 - accuracy: 0.9152 - val_loss: 0.1826 - val_accuracy: 0.9157\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1795 - accuracy: 0.9143 - val_loss: 0.1792 - val_accuracy: 0.9158\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.9126 - val_loss: 0.1824 - val_accuracy: 0.9149\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1808 - accuracy: 0.9143 - val_loss: 0.1795 - val_accuracy: 0.9160\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9102 - val_loss: 0.1789 - val_accuracy: 0.9150\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9136 - val_loss: 0.1794 - val_accuracy: 0.9148\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.9146 - val_loss: 0.1850 - val_accuracy: 0.9148\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1816 - accuracy: 0.9145 - val_loss: 0.1790 - val_accuracy: 0.9162\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9132 - val_loss: 0.1821 - val_accuracy: 0.9165\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.9146 - val_loss: 0.1820 - val_accuracy: 0.9162\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1827 - accuracy: 0.9118 - val_loss: 0.1804 - val_accuracy: 0.9150\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9121 - val_loss: 0.1790 - val_accuracy: 0.9156\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9102 - val_loss: 0.1800 - val_accuracy: 0.9146\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9156 - val_loss: 0.1792 - val_accuracy: 0.9165\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9132 - val_loss: 0.1815 - val_accuracy: 0.9131\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9132 - val_loss: 0.1780 - val_accuracy: 0.9160\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9133 - val_loss: 0.1799 - val_accuracy: 0.9161\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.9134 - val_loss: 0.1801 - val_accuracy: 0.9161\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1794 - accuracy: 0.9138 - val_loss: 0.1788 - val_accuracy: 0.9160\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9120 - val_loss: 0.1788 - val_accuracy: 0.9162\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1814 - accuracy: 0.9114 - val_loss: 0.1801 - val_accuracy: 0.9164\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1823 - accuracy: 0.9140 - val_loss: 0.1813 - val_accuracy: 0.9168\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9124 - val_loss: 0.1785 - val_accuracy: 0.9152\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.9166 - val_loss: 0.1835 - val_accuracy: 0.9164\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9148 - val_loss: 0.1787 - val_accuracy: 0.9152\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9118 - val_loss: 0.1789 - val_accuracy: 0.9161\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FMIp1A27zxsZ",
        "outputId": "229b8446-9d5d-4fdf-8345-0e44786c6993"
      },
      "source": [
        "#0.3\n",
        "model6 = Sequential()\n",
        "model6.add(Dense(16, input_dim = 20,activation='relu'))\n",
        "model6.add(Dense(8,activation='relu'))\n",
        "model6.add(Dense(4,activation='relu'))\n",
        "model6.add(Dense(4,activation='relu'))\n",
        "model6.add(Dense(4,activation='relu'))\n",
        "model6.add(Dense(1,activation='sigmoid'))\n",
        "model6.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model6.summary()\n",
        "history = model6.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 16)                336       \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 553\n",
            "Trainable params: 553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4608 - accuracy: 0.8185 - val_loss: 0.2667 - val_accuracy: 0.8924\n",
            "Epoch 2/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2538 - accuracy: 0.8992 - val_loss: 0.2047 - val_accuracy: 0.9135\n",
            "Epoch 3/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2096 - accuracy: 0.9094 - val_loss: 0.1959 - val_accuracy: 0.9143\n",
            "Epoch 4/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2050 - accuracy: 0.9086 - val_loss: 0.1964 - val_accuracy: 0.9132\n",
            "Epoch 5/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2046 - accuracy: 0.9068 - val_loss: 0.1945 - val_accuracy: 0.9134\n",
            "Epoch 6/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2000 - accuracy: 0.9113 - val_loss: 0.1946 - val_accuracy: 0.9133\n",
            "Epoch 7/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1979 - accuracy: 0.9110 - val_loss: 0.1926 - val_accuracy: 0.9138\n",
            "Epoch 8/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9092 - val_loss: 0.2091 - val_accuracy: 0.9117\n",
            "Epoch 9/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1981 - accuracy: 0.9105 - val_loss: 0.1926 - val_accuracy: 0.9139\n",
            "Epoch 10/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2017 - accuracy: 0.9078 - val_loss: 0.1997 - val_accuracy: 0.9118\n",
            "Epoch 11/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1960 - accuracy: 0.9108 - val_loss: 0.1916 - val_accuracy: 0.9135\n",
            "Epoch 12/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1980 - accuracy: 0.9092 - val_loss: 0.1916 - val_accuracy: 0.9136\n",
            "Epoch 13/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1971 - accuracy: 0.9105 - val_loss: 0.1905 - val_accuracy: 0.9124\n",
            "Epoch 14/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1977 - accuracy: 0.9105 - val_loss: 0.1892 - val_accuracy: 0.9143\n",
            "Epoch 15/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1952 - accuracy: 0.9093 - val_loss: 0.1930 - val_accuracy: 0.9126\n",
            "Epoch 16/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1983 - accuracy: 0.9088 - val_loss: 0.1895 - val_accuracy: 0.9129\n",
            "Epoch 17/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1960 - accuracy: 0.9100 - val_loss: 0.1888 - val_accuracy: 0.9135\n",
            "Epoch 18/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9101 - val_loss: 0.1898 - val_accuracy: 0.9127\n",
            "Epoch 19/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9135 - val_loss: 0.1938 - val_accuracy: 0.9122\n",
            "Epoch 20/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1921 - accuracy: 0.9091 - val_loss: 0.1897 - val_accuracy: 0.9135\n",
            "Epoch 21/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1961 - accuracy: 0.9097 - val_loss: 0.1874 - val_accuracy: 0.9138\n",
            "Epoch 22/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1896 - accuracy: 0.9108 - val_loss: 0.1889 - val_accuracy: 0.9142\n",
            "Epoch 23/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9121 - val_loss: 0.1904 - val_accuracy: 0.9131\n",
            "Epoch 24/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9120 - val_loss: 0.1885 - val_accuracy: 0.9126\n",
            "Epoch 25/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1870 - accuracy: 0.9148 - val_loss: 0.1867 - val_accuracy: 0.9135\n",
            "Epoch 26/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1967 - accuracy: 0.9060 - val_loss: 0.1865 - val_accuracy: 0.9135\n",
            "Epoch 27/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9114 - val_loss: 0.1980 - val_accuracy: 0.9120\n",
            "Epoch 28/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9128 - val_loss: 0.1880 - val_accuracy: 0.9128\n",
            "Epoch 29/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1935 - accuracy: 0.9101 - val_loss: 0.1858 - val_accuracy: 0.9131\n",
            "Epoch 30/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1934 - accuracy: 0.9110 - val_loss: 0.1858 - val_accuracy: 0.9134\n",
            "Epoch 31/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9076 - val_loss: 0.1862 - val_accuracy: 0.9125\n",
            "Epoch 32/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9113 - val_loss: 0.1857 - val_accuracy: 0.9121\n",
            "Epoch 33/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1920 - accuracy: 0.9081 - val_loss: 0.1847 - val_accuracy: 0.9129\n",
            "Epoch 34/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9105 - val_loss: 0.1861 - val_accuracy: 0.9135\n",
            "Epoch 35/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1910 - accuracy: 0.9120 - val_loss: 0.1849 - val_accuracy: 0.9110\n",
            "Epoch 36/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9108 - val_loss: 0.1881 - val_accuracy: 0.9126\n",
            "Epoch 37/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9104 - val_loss: 0.1862 - val_accuracy: 0.9133\n",
            "Epoch 38/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9100 - val_loss: 0.1861 - val_accuracy: 0.9127\n",
            "Epoch 39/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9114 - val_loss: 0.1851 - val_accuracy: 0.9116\n",
            "Epoch 40/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1903 - accuracy: 0.9099 - val_loss: 0.1868 - val_accuracy: 0.9131\n",
            "Epoch 41/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9113 - val_loss: 0.1873 - val_accuracy: 0.9131\n",
            "Epoch 42/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9104 - val_loss: 0.1890 - val_accuracy: 0.9120\n",
            "Epoch 43/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9094 - val_loss: 0.1850 - val_accuracy: 0.9133\n",
            "Epoch 44/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9112 - val_loss: 0.1843 - val_accuracy: 0.9122\n",
            "Epoch 45/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.9139 - val_loss: 0.1863 - val_accuracy: 0.9131\n",
            "Epoch 46/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9122 - val_loss: 0.1839 - val_accuracy: 0.9139\n",
            "Epoch 47/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9133 - val_loss: 0.1841 - val_accuracy: 0.9120\n",
            "Epoch 48/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9136 - val_loss: 0.1860 - val_accuracy: 0.9139\n",
            "Epoch 49/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9136 - val_loss: 0.1827 - val_accuracy: 0.9137\n",
            "Epoch 50/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1846 - accuracy: 0.9140 - val_loss: 0.1918 - val_accuracy: 0.9126\n",
            "Epoch 51/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9146 - val_loss: 0.1862 - val_accuracy: 0.9135\n",
            "Epoch 52/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9114 - val_loss: 0.1850 - val_accuracy: 0.9132\n",
            "Epoch 53/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1876 - accuracy: 0.9126 - val_loss: 0.1833 - val_accuracy: 0.9135\n",
            "Epoch 54/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1814 - accuracy: 0.9158 - val_loss: 0.1869 - val_accuracy: 0.9124\n",
            "Epoch 55/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.9183 - val_loss: 0.1852 - val_accuracy: 0.9136\n",
            "Epoch 56/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1838 - accuracy: 0.9120 - val_loss: 0.1838 - val_accuracy: 0.9138\n",
            "Epoch 57/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1833 - accuracy: 0.9131 - val_loss: 0.1828 - val_accuracy: 0.9126\n",
            "Epoch 58/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9134 - val_loss: 0.1829 - val_accuracy: 0.9134\n",
            "Epoch 59/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9104 - val_loss: 0.1851 - val_accuracy: 0.9134\n",
            "Epoch 60/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9123 - val_loss: 0.1839 - val_accuracy: 0.9125\n",
            "Epoch 61/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1830 - accuracy: 0.9137 - val_loss: 0.1846 - val_accuracy: 0.9141\n",
            "Epoch 62/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1810 - accuracy: 0.9139 - val_loss: 0.1842 - val_accuracy: 0.9125\n",
            "Epoch 63/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9124 - val_loss: 0.1820 - val_accuracy: 0.9127\n",
            "Epoch 64/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9133 - val_loss: 0.1881 - val_accuracy: 0.9123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7yflao8Rx9_v",
        "outputId": "5bb26cf1-6086-4ea9-9733-b2891b80a789"
      },
      "source": [
        "#0.3\n",
        "model7 = Sequential()\n",
        "model7.add(Dense(16, input_dim = 20,activation='relu'))\n",
        "model7.add(Dense(8,activation='relu'))\n",
        "model7.add(Dense(4,activation='relu'))\n",
        "model7.add(Dense(4,activation='relu'))\n",
        "model7.add(Dense(4,activation='relu'))\n",
        "model7.add(Dense(1,activation='sigmoid'))\n",
        "model7.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model7.summary()\n",
        "history = model7.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_37 (Dense)             (None, 16)                336       \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 553\n",
            "Trainable params: 553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4924 - accuracy: 0.8549 - val_loss: 0.2509 - val_accuracy: 0.9044\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2433 - accuracy: 0.9033 - val_loss: 0.2105 - val_accuracy: 0.9114\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2128 - accuracy: 0.9098 - val_loss: 0.2043 - val_accuracy: 0.9134\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2087 - accuracy: 0.9084 - val_loss: 0.2083 - val_accuracy: 0.9092\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2046 - accuracy: 0.9096 - val_loss: 0.1966 - val_accuracy: 0.9137\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2076 - accuracy: 0.9084 - val_loss: 0.1947 - val_accuracy: 0.9138\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1979 - accuracy: 0.9109 - val_loss: 0.1926 - val_accuracy: 0.9135\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9124 - val_loss: 0.1932 - val_accuracy: 0.9131\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2007 - accuracy: 0.9105 - val_loss: 0.1933 - val_accuracy: 0.9130\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1972 - accuracy: 0.9112 - val_loss: 0.1913 - val_accuracy: 0.9126\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1935 - accuracy: 0.9117 - val_loss: 0.1927 - val_accuracy: 0.9118\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1957 - accuracy: 0.9106 - val_loss: 0.1900 - val_accuracy: 0.9141\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9118 - val_loss: 0.1882 - val_accuracy: 0.9139\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9117 - val_loss: 0.1951 - val_accuracy: 0.9117\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9117 - val_loss: 0.1954 - val_accuracy: 0.9114\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9117 - val_loss: 0.1868 - val_accuracy: 0.9130\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9110 - val_loss: 0.1876 - val_accuracy: 0.9133\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9117 - val_loss: 0.1844 - val_accuracy: 0.9143\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9109 - val_loss: 0.1916 - val_accuracy: 0.9126\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1918 - accuracy: 0.9107 - val_loss: 0.1844 - val_accuracy: 0.9142\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9108 - val_loss: 0.1880 - val_accuracy: 0.9141\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9145 - val_loss: 0.1872 - val_accuracy: 0.9143\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9112 - val_loss: 0.1826 - val_accuracy: 0.9149\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9136 - val_loss: 0.1834 - val_accuracy: 0.9143\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1871 - accuracy: 0.9121 - val_loss: 0.1825 - val_accuracy: 0.9140\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1876 - accuracy: 0.9124 - val_loss: 0.1859 - val_accuracy: 0.9131\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9073 - val_loss: 0.1828 - val_accuracy: 0.9138\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9093 - val_loss: 0.1835 - val_accuracy: 0.9146\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9108 - val_loss: 0.1828 - val_accuracy: 0.9128\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9120 - val_loss: 0.1821 - val_accuracy: 0.9151\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1891 - accuracy: 0.9108 - val_loss: 0.1829 - val_accuracy: 0.9149\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1844 - accuracy: 0.9143 - val_loss: 0.1842 - val_accuracy: 0.9140\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9136 - val_loss: 0.1850 - val_accuracy: 0.9128\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9139 - val_loss: 0.1824 - val_accuracy: 0.9133\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9151 - val_loss: 0.1861 - val_accuracy: 0.9140\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.9125 - val_loss: 0.1815 - val_accuracy: 0.9139\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1892 - accuracy: 0.9097 - val_loss: 0.1838 - val_accuracy: 0.9148\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9110 - val_loss: 0.1845 - val_accuracy: 0.9133\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9129 - val_loss: 0.1823 - val_accuracy: 0.9141\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9140 - val_loss: 0.1813 - val_accuracy: 0.9145\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1835 - accuracy: 0.9143 - val_loss: 0.1837 - val_accuracy: 0.9136\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1823 - accuracy: 0.9169 - val_loss: 0.1861 - val_accuracy: 0.9135\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9130 - val_loss: 0.1834 - val_accuracy: 0.9143\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1823 - accuracy: 0.9143 - val_loss: 0.1889 - val_accuracy: 0.9107\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9124 - val_loss: 0.1830 - val_accuracy: 0.9142\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1840 - accuracy: 0.9134 - val_loss: 0.1824 - val_accuracy: 0.9151\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9125 - val_loss: 0.1842 - val_accuracy: 0.9140\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9117 - val_loss: 0.1822 - val_accuracy: 0.9139\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9119 - val_loss: 0.1849 - val_accuracy: 0.9138\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1844 - accuracy: 0.9124 - val_loss: 0.1838 - val_accuracy: 0.9138\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1773 - accuracy: 0.9157 - val_loss: 0.1826 - val_accuracy: 0.9139\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9159 - val_loss: 0.1839 - val_accuracy: 0.9143\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9091 - val_loss: 0.1864 - val_accuracy: 0.9140\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9125 - val_loss: 0.1874 - val_accuracy: 0.9127\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1810 - accuracy: 0.9151 - val_loss: 0.1815 - val_accuracy: 0.9139\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9144 - val_loss: 0.1820 - val_accuracy: 0.9128\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9157 - val_loss: 0.1814 - val_accuracy: 0.9145\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9108 - val_loss: 0.1894 - val_accuracy: 0.9149\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.9127 - val_loss: 0.2030 - val_accuracy: 0.9136\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1839 - accuracy: 0.9125 - val_loss: 0.1830 - val_accuracy: 0.9131\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1776 - accuracy: 0.9149 - val_loss: 0.1806 - val_accuracy: 0.9127\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9142 - val_loss: 0.1818 - val_accuracy: 0.9145\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1787 - accuracy: 0.9141 - val_loss: 0.1815 - val_accuracy: 0.9143\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9132 - val_loss: 0.1806 - val_accuracy: 0.9130\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9127 - val_loss: 0.1818 - val_accuracy: 0.9139\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1808 - accuracy: 0.9143 - val_loss: 0.1911 - val_accuracy: 0.9117\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9150 - val_loss: 0.1827 - val_accuracy: 0.9133\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.9134 - val_loss: 0.1830 - val_accuracy: 0.9138\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9152 - val_loss: 0.1804 - val_accuracy: 0.9139\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1812 - accuracy: 0.9141 - val_loss: 0.1799 - val_accuracy: 0.9135\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.9108 - val_loss: 0.1831 - val_accuracy: 0.9120\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1803 - accuracy: 0.9164 - val_loss: 0.1849 - val_accuracy: 0.9142\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9144 - val_loss: 0.1825 - val_accuracy: 0.9137\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9128 - val_loss: 0.1811 - val_accuracy: 0.9135\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1803 - accuracy: 0.9150 - val_loss: 0.1836 - val_accuracy: 0.9138\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1794 - accuracy: 0.9152 - val_loss: 0.1809 - val_accuracy: 0.9134\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1730 - accuracy: 0.9179 - val_loss: 0.1840 - val_accuracy: 0.9156\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9144 - val_loss: 0.1857 - val_accuracy: 0.9123\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9139 - val_loss: 0.1805 - val_accuracy: 0.9134\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1774 - accuracy: 0.9126 - val_loss: 0.1843 - val_accuracy: 0.9131\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1763 - accuracy: 0.9155 - val_loss: 0.1810 - val_accuracy: 0.9131\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9143 - val_loss: 0.1840 - val_accuracy: 0.9118\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1721 - accuracy: 0.9185 - val_loss: 0.1806 - val_accuracy: 0.9134\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1807 - accuracy: 0.9130 - val_loss: 0.1807 - val_accuracy: 0.9137\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1801 - accuracy: 0.9137 - val_loss: 0.1840 - val_accuracy: 0.9149\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1742 - accuracy: 0.9183 - val_loss: 0.1807 - val_accuracy: 0.9144\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1749 - accuracy: 0.9156 - val_loss: 0.1899 - val_accuracy: 0.9095\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9120 - val_loss: 0.1794 - val_accuracy: 0.9148\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.9133 - val_loss: 0.1792 - val_accuracy: 0.9147\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.9162 - val_loss: 0.1814 - val_accuracy: 0.9136\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1826 - accuracy: 0.9109 - val_loss: 0.1802 - val_accuracy: 0.9131\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1742 - accuracy: 0.9152 - val_loss: 0.1793 - val_accuracy: 0.9147\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1751 - accuracy: 0.9153 - val_loss: 0.1836 - val_accuracy: 0.9126\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9142 - val_loss: 0.1829 - val_accuracy: 0.9154\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9152 - val_loss: 0.1813 - val_accuracy: 0.9146\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1826 - accuracy: 0.9113 - val_loss: 0.1812 - val_accuracy: 0.9131\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9161 - val_loss: 0.1799 - val_accuracy: 0.9160\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9179 - val_loss: 0.1804 - val_accuracy: 0.9123\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1766 - accuracy: 0.9145 - val_loss: 0.1814 - val_accuracy: 0.9128\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1784 - accuracy: 0.9141 - val_loss: 0.1790 - val_accuracy: 0.9152\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.9134 - val_loss: 0.1795 - val_accuracy: 0.9148\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1780 - accuracy: 0.9160 - val_loss: 0.1788 - val_accuracy: 0.9156\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9134 - val_loss: 0.1862 - val_accuracy: 0.9117\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1740 - accuracy: 0.9168 - val_loss: 0.1788 - val_accuracy: 0.9137\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9145 - val_loss: 0.1824 - val_accuracy: 0.9130\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1736 - accuracy: 0.9163 - val_loss: 0.1811 - val_accuracy: 0.9141\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1758 - accuracy: 0.9164 - val_loss: 0.1829 - val_accuracy: 0.9156\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1736 - accuracy: 0.9182 - val_loss: 0.1800 - val_accuracy: 0.9131\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9163 - val_loss: 0.1811 - val_accuracy: 0.9135\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1725 - accuracy: 0.9191 - val_loss: 0.1785 - val_accuracy: 0.9144\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1799 - accuracy: 0.9151 - val_loss: 0.1805 - val_accuracy: 0.9147\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1752 - accuracy: 0.9158 - val_loss: 0.1789 - val_accuracy: 0.9139\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9138 - val_loss: 0.1806 - val_accuracy: 0.9146\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9159 - val_loss: 0.1822 - val_accuracy: 0.9158\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.9154 - val_loss: 0.1795 - val_accuracy: 0.9148\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1748 - accuracy: 0.9174 - val_loss: 0.1809 - val_accuracy: 0.9147\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1762 - accuracy: 0.9137 - val_loss: 0.1804 - val_accuracy: 0.9149\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1826 - accuracy: 0.9104 - val_loss: 0.1799 - val_accuracy: 0.9146\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1705 - accuracy: 0.9173 - val_loss: 0.1807 - val_accuracy: 0.9142\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.9142 - val_loss: 0.1805 - val_accuracy: 0.9129\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.9150 - val_loss: 0.1801 - val_accuracy: 0.9157\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1753 - accuracy: 0.9164 - val_loss: 0.1813 - val_accuracy: 0.9150\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1782 - accuracy: 0.9139 - val_loss: 0.1805 - val_accuracy: 0.9136\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1712 - accuracy: 0.9189 - val_loss: 0.1815 - val_accuracy: 0.9144\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1718 - accuracy: 0.9167 - val_loss: 0.1788 - val_accuracy: 0.9152\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9145 - val_loss: 0.1836 - val_accuracy: 0.9145\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1769 - accuracy: 0.9162 - val_loss: 0.1803 - val_accuracy: 0.9130\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1717 - accuracy: 0.9181 - val_loss: 0.1825 - val_accuracy: 0.9132\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1w1B327ozFlW",
        "outputId": "44fcdcf3-e84d-4f54-aebb-775be47a9ed2"
      },
      "source": [
        "#0.3\n",
        "model8 = Sequential()\n",
        "model8.add(Dense(16, input_dim = 20,activation='relu'))\n",
        "model8.add(Dense(8,activation='relu'))\n",
        "model8.add(Dense(4,activation='relu'))\n",
        "model8.add(Dense(4,activation='relu'))\n",
        "model8.add(Dense(4,activation='relu'))\n",
        "model8.add(Dense(4,activation='relu'))\n",
        "model8.add(Dense(4,activation='relu'))\n",
        "model8.add(Dense(4,activation='relu'))\n",
        "model8.add(Dense(1,activation='sigmoid'))\n",
        "model8.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model8.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4442 - accuracy: 0.8781 - val_loss: 0.2640 - val_accuracy: 0.8957\n",
            "Epoch 2/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2691 - accuracy: 0.8901 - val_loss: 0.2035 - val_accuracy: 0.9124\n",
            "Epoch 3/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2151 - accuracy: 0.9088 - val_loss: 0.1953 - val_accuracy: 0.9141\n",
            "Epoch 4/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2025 - accuracy: 0.9102 - val_loss: 0.1938 - val_accuracy: 0.9145\n",
            "Epoch 5/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2030 - accuracy: 0.9102 - val_loss: 0.1921 - val_accuracy: 0.9143\n",
            "Epoch 6/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2052 - accuracy: 0.9091 - val_loss: 0.1924 - val_accuracy: 0.9147\n",
            "Epoch 7/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9126 - val_loss: 0.1901 - val_accuracy: 0.9153\n",
            "Epoch 8/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2020 - accuracy: 0.9070 - val_loss: 0.1925 - val_accuracy: 0.9143\n",
            "Epoch 9/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1991 - accuracy: 0.9067 - val_loss: 0.1890 - val_accuracy: 0.9144\n",
            "Epoch 10/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9106 - val_loss: 0.1904 - val_accuracy: 0.9116\n",
            "Epoch 11/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9116 - val_loss: 0.1861 - val_accuracy: 0.9139\n",
            "Epoch 12/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9120 - val_loss: 0.1870 - val_accuracy: 0.9152\n",
            "Epoch 13/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1931 - accuracy: 0.9067 - val_loss: 0.1863 - val_accuracy: 0.9135\n",
            "Epoch 14/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1918 - accuracy: 0.9102 - val_loss: 0.1912 - val_accuracy: 0.9138\n",
            "Epoch 15/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1927 - accuracy: 0.9094 - val_loss: 0.1856 - val_accuracy: 0.9148\n",
            "Epoch 16/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1893 - accuracy: 0.9094 - val_loss: 0.1855 - val_accuracy: 0.9154\n",
            "Epoch 17/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9099 - val_loss: 0.1848 - val_accuracy: 0.9151\n",
            "Epoch 18/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9086 - val_loss: 0.1857 - val_accuracy: 0.9136\n",
            "Epoch 19/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9113 - val_loss: 0.1857 - val_accuracy: 0.9139\n",
            "Epoch 20/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1890 - accuracy: 0.9087 - val_loss: 0.1840 - val_accuracy: 0.9148\n",
            "Epoch 21/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1877 - accuracy: 0.9106 - val_loss: 0.1875 - val_accuracy: 0.9153\n",
            "Epoch 22/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9115 - val_loss: 0.1833 - val_accuracy: 0.9156\n",
            "Epoch 23/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9115 - val_loss: 0.1826 - val_accuracy: 0.9153\n",
            "Epoch 24/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9097 - val_loss: 0.1839 - val_accuracy: 0.9148\n",
            "Epoch 25/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9104 - val_loss: 0.1835 - val_accuracy: 0.9143\n",
            "Epoch 26/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9111 - val_loss: 0.1829 - val_accuracy: 0.9156\n",
            "Epoch 27/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9115 - val_loss: 0.1889 - val_accuracy: 0.9122\n",
            "Epoch 28/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9119 - val_loss: 0.1947 - val_accuracy: 0.9131\n",
            "Epoch 29/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9116 - val_loss: 0.1830 - val_accuracy: 0.9142\n",
            "Epoch 30/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1876 - accuracy: 0.9102 - val_loss: 0.1857 - val_accuracy: 0.9126\n",
            "Epoch 31/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9099 - val_loss: 0.1840 - val_accuracy: 0.9136\n",
            "Epoch 32/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1921 - accuracy: 0.9090 - val_loss: 0.1826 - val_accuracy: 0.9154\n",
            "Epoch 33/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9146 - val_loss: 0.1844 - val_accuracy: 0.9132\n",
            "Epoch 34/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9110 - val_loss: 0.1862 - val_accuracy: 0.9132\n",
            "Epoch 35/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1894 - accuracy: 0.9126 - val_loss: 0.1854 - val_accuracy: 0.9154\n",
            "Epoch 36/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9091 - val_loss: 0.1820 - val_accuracy: 0.9156\n",
            "Epoch 37/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9147 - val_loss: 0.1877 - val_accuracy: 0.9127\n",
            "Epoch 38/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9105 - val_loss: 0.1825 - val_accuracy: 0.9165\n",
            "Epoch 39/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1857 - accuracy: 0.9112 - val_loss: 0.1856 - val_accuracy: 0.9134\n",
            "Epoch 40/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9125 - val_loss: 0.1804 - val_accuracy: 0.9151\n",
            "Epoch 41/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1817 - accuracy: 0.9134 - val_loss: 0.1824 - val_accuracy: 0.9151\n",
            "Epoch 42/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1877 - accuracy: 0.9099 - val_loss: 0.1834 - val_accuracy: 0.9136\n",
            "Epoch 43/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9110 - val_loss: 0.1898 - val_accuracy: 0.9130\n",
            "Epoch 44/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9088 - val_loss: 0.1813 - val_accuracy: 0.9165\n",
            "Epoch 45/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9117 - val_loss: 0.1820 - val_accuracy: 0.9154\n",
            "Epoch 46/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9104 - val_loss: 0.1811 - val_accuracy: 0.9161\n",
            "Epoch 47/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9116 - val_loss: 0.1819 - val_accuracy: 0.9145\n",
            "Epoch 48/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9104 - val_loss: 0.1843 - val_accuracy: 0.9153\n",
            "Epoch 49/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9112 - val_loss: 0.1810 - val_accuracy: 0.9165\n",
            "Epoch 50/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9129 - val_loss: 0.1830 - val_accuracy: 0.9160\n",
            "Epoch 51/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9135 - val_loss: 0.1818 - val_accuracy: 0.9155\n",
            "Epoch 52/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.9114 - val_loss: 0.1813 - val_accuracy: 0.9179\n",
            "Epoch 53/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1811 - accuracy: 0.9121 - val_loss: 0.1819 - val_accuracy: 0.9152\n",
            "Epoch 54/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1790 - accuracy: 0.9144 - val_loss: 0.1810 - val_accuracy: 0.9165\n",
            "Epoch 55/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9107 - val_loss: 0.1829 - val_accuracy: 0.9166\n",
            "Epoch 56/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.9122 - val_loss: 0.1853 - val_accuracy: 0.9139\n",
            "Epoch 57/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1796 - accuracy: 0.9136 - val_loss: 0.1818 - val_accuracy: 0.9169\n",
            "Epoch 58/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1838 - accuracy: 0.9130 - val_loss: 0.1887 - val_accuracy: 0.9120\n",
            "Epoch 59/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1829 - accuracy: 0.9121 - val_loss: 0.1806 - val_accuracy: 0.9167\n",
            "Epoch 60/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1805 - accuracy: 0.9121 - val_loss: 0.1807 - val_accuracy: 0.9163\n",
            "Epoch 61/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1793 - accuracy: 0.9141 - val_loss: 0.1804 - val_accuracy: 0.9158\n",
            "Epoch 62/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9114 - val_loss: 0.1816 - val_accuracy: 0.9159\n",
            "Epoch 63/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1812 - accuracy: 0.9112 - val_loss: 0.1804 - val_accuracy: 0.9163\n",
            "Epoch 64/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1792 - accuracy: 0.9138 - val_loss: 0.1828 - val_accuracy: 0.9151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1wJKg0xh6CWx",
        "outputId": "374eefc9-8ca0-41bc-90f8-f44252f250ea"
      },
      "source": [
        "model11 = Sequential()\n",
        "model11.add(Dense(64, input_dim = 20,activation='relu'))\n",
        "model11.add(Dense(32,activation='relu'))\n",
        "model11.add(Dense(16,activation='relu'))\n",
        "model11.add(Dense(8,activation='relu'))\n",
        "model11.add(Dense(4,activation='relu'))\n",
        "model11.add(Dense(1,activation='sigmoid'))\n",
        "model11.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model11.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "901/901 [==============================] - 3s 2ms/step - loss: 0.3343 - accuracy: 0.8849 - val_loss: 0.2040 - val_accuracy: 0.9114\n",
            "Epoch 2/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2147 - accuracy: 0.9085 - val_loss: 0.1950 - val_accuracy: 0.9146\n",
            "Epoch 3/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2056 - accuracy: 0.9111 - val_loss: 0.1986 - val_accuracy: 0.9107\n",
            "Epoch 4/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2030 - accuracy: 0.9100 - val_loss: 0.1875 - val_accuracy: 0.9147\n",
            "Epoch 5/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9117 - val_loss: 0.1884 - val_accuracy: 0.9105\n",
            "Epoch 6/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9127 - val_loss: 0.1831 - val_accuracy: 0.9130\n",
            "Epoch 7/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9110 - val_loss: 0.1901 - val_accuracy: 0.9121\n",
            "Epoch 8/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9150 - val_loss: 0.1829 - val_accuracy: 0.9126\n",
            "Epoch 9/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9145 - val_loss: 0.1839 - val_accuracy: 0.9138\n",
            "Epoch 10/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9118 - val_loss: 0.1932 - val_accuracy: 0.9101\n",
            "Epoch 11/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9108 - val_loss: 0.1805 - val_accuracy: 0.9152\n",
            "Epoch 12/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9123 - val_loss: 0.1805 - val_accuracy: 0.9140\n",
            "Epoch 13/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9115 - val_loss: 0.1802 - val_accuracy: 0.9137\n",
            "Epoch 14/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9130 - val_loss: 0.1788 - val_accuracy: 0.9158\n",
            "Epoch 15/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9167 - val_loss: 0.1902 - val_accuracy: 0.9081\n",
            "Epoch 16/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9134 - val_loss: 0.1865 - val_accuracy: 0.9122\n",
            "Epoch 17/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9136 - val_loss: 0.1835 - val_accuracy: 0.9122\n",
            "Epoch 18/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9143 - val_loss: 0.1812 - val_accuracy: 0.9151\n",
            "Epoch 19/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9162 - val_loss: 0.1791 - val_accuracy: 0.9148\n",
            "Epoch 20/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9144 - val_loss: 0.1815 - val_accuracy: 0.9143\n",
            "Epoch 21/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9126 - val_loss: 0.1810 - val_accuracy: 0.9148\n",
            "Epoch 22/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9163 - val_loss: 0.1808 - val_accuracy: 0.9129\n",
            "Epoch 23/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9160 - val_loss: 0.1811 - val_accuracy: 0.9150\n",
            "Epoch 24/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9171 - val_loss: 0.1818 - val_accuracy: 0.9161\n",
            "Epoch 25/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9166 - val_loss: 0.1903 - val_accuracy: 0.9082\n",
            "Epoch 26/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1743 - accuracy: 0.9170 - val_loss: 0.1909 - val_accuracy: 0.9101\n",
            "Epoch 27/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9147 - val_loss: 0.1861 - val_accuracy: 0.9140\n",
            "Epoch 28/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1747 - accuracy: 0.9183 - val_loss: 0.1816 - val_accuracy: 0.9149\n",
            "Epoch 29/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9151 - val_loss: 0.1809 - val_accuracy: 0.9159\n",
            "Epoch 30/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9178 - val_loss: 0.1798 - val_accuracy: 0.9151\n",
            "Epoch 31/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9158 - val_loss: 0.1814 - val_accuracy: 0.9129\n",
            "Epoch 32/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1740 - accuracy: 0.9175 - val_loss: 0.1840 - val_accuracy: 0.9125\n",
            "Epoch 33/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1714 - accuracy: 0.9195 - val_loss: 0.1843 - val_accuracy: 0.9138\n",
            "Epoch 34/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1719 - accuracy: 0.9193 - val_loss: 0.1812 - val_accuracy: 0.9149\n",
            "Epoch 35/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1728 - accuracy: 0.9220 - val_loss: 0.1809 - val_accuracy: 0.9138\n",
            "Epoch 36/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9181 - val_loss: 0.1826 - val_accuracy: 0.9160\n",
            "Epoch 37/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1751 - accuracy: 0.9175 - val_loss: 0.1875 - val_accuracy: 0.9116\n",
            "Epoch 38/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1756 - accuracy: 0.9186 - val_loss: 0.1819 - val_accuracy: 0.9156\n",
            "Epoch 39/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1714 - accuracy: 0.9194 - val_loss: 0.1848 - val_accuracy: 0.9122\n",
            "Epoch 40/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1735 - accuracy: 0.9175 - val_loss: 0.1810 - val_accuracy: 0.9137\n",
            "Epoch 41/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1668 - accuracy: 0.9224 - val_loss: 0.1811 - val_accuracy: 0.9142\n",
            "Epoch 42/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1754 - accuracy: 0.9183 - val_loss: 0.1801 - val_accuracy: 0.9155\n",
            "Epoch 43/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1703 - accuracy: 0.9199 - val_loss: 0.1810 - val_accuracy: 0.9154\n",
            "Epoch 44/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1735 - accuracy: 0.9173 - val_loss: 0.1823 - val_accuracy: 0.9166\n",
            "Epoch 45/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1678 - accuracy: 0.9221 - val_loss: 0.1817 - val_accuracy: 0.9146\n",
            "Epoch 46/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1715 - accuracy: 0.9219 - val_loss: 0.1822 - val_accuracy: 0.9130\n",
            "Epoch 47/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1707 - accuracy: 0.9204 - val_loss: 0.1901 - val_accuracy: 0.9103\n",
            "Epoch 48/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1647 - accuracy: 0.9226 - val_loss: 0.1822 - val_accuracy: 0.9133\n",
            "Epoch 49/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1662 - accuracy: 0.9218 - val_loss: 0.1857 - val_accuracy: 0.9148\n",
            "Epoch 50/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1664 - accuracy: 0.9225 - val_loss: 0.1832 - val_accuracy: 0.9143\n",
            "Epoch 51/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1663 - accuracy: 0.9226 - val_loss: 0.1811 - val_accuracy: 0.9141\n",
            "Epoch 52/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1665 - accuracy: 0.9235 - val_loss: 0.1903 - val_accuracy: 0.9116\n",
            "Epoch 53/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1655 - accuracy: 0.9225 - val_loss: 0.1819 - val_accuracy: 0.9131\n",
            "Epoch 54/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1637 - accuracy: 0.9242 - val_loss: 0.1837 - val_accuracy: 0.9145\n",
            "Epoch 55/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1643 - accuracy: 0.9232 - val_loss: 0.1866 - val_accuracy: 0.9111\n",
            "Epoch 56/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1655 - accuracy: 0.9241 - val_loss: 0.1853 - val_accuracy: 0.9135\n",
            "Epoch 57/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1669 - accuracy: 0.9215 - val_loss: 0.1852 - val_accuracy: 0.9139\n",
            "Epoch 58/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1607 - accuracy: 0.9239 - val_loss: 0.1851 - val_accuracy: 0.9128\n",
            "Epoch 59/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1641 - accuracy: 0.9215 - val_loss: 0.1872 - val_accuracy: 0.9138\n",
            "Epoch 60/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1588 - accuracy: 0.9271 - val_loss: 0.1878 - val_accuracy: 0.9123\n",
            "Epoch 61/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1626 - accuracy: 0.9242 - val_loss: 0.1868 - val_accuracy: 0.9139\n",
            "Epoch 62/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1599 - accuracy: 0.9256 - val_loss: 0.1903 - val_accuracy: 0.9123\n",
            "Epoch 63/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1611 - accuracy: 0.9254 - val_loss: 0.1894 - val_accuracy: 0.9135\n",
            "Epoch 64/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1633 - accuracy: 0.9245 - val_loss: 0.1864 - val_accuracy: 0.9112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yA2G3hqA2sB3",
        "outputId": "2a35ea10-20cf-46f9-8b11-54793441dbbd"
      },
      "source": [
        "model10 = Sequential()\n",
        "model10.add(Dense(32, input_dim = 20,activation='relu'))\n",
        "model10.add(Dense(8,activation='relu'))\n",
        "model10.add(Dense(4,activation='relu'))\n",
        "model10.add(Dense(1,activation='sigmoid'))\n",
        "model10.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model10.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3130 - accuracy: 0.8896 - val_loss: 0.2250 - val_accuracy: 0.9100\n",
            "Epoch 2/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2249 - accuracy: 0.9063 - val_loss: 0.2057 - val_accuracy: 0.9145\n",
            "Epoch 3/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2170 - accuracy: 0.9049 - val_loss: 0.2043 - val_accuracy: 0.9127\n",
            "Epoch 4/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2101 - accuracy: 0.9077 - val_loss: 0.1971 - val_accuracy: 0.9153\n",
            "Epoch 5/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2086 - accuracy: 0.9081 - val_loss: 0.1965 - val_accuracy: 0.9164\n",
            "Epoch 6/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2013 - accuracy: 0.9111 - val_loss: 0.1920 - val_accuracy: 0.9141\n",
            "Epoch 7/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2019 - accuracy: 0.9112 - val_loss: 0.1926 - val_accuracy: 0.9144\n",
            "Epoch 8/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1979 - accuracy: 0.9114 - val_loss: 0.1926 - val_accuracy: 0.9145\n",
            "Epoch 9/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1998 - accuracy: 0.9101 - val_loss: 0.1911 - val_accuracy: 0.9154\n",
            "Epoch 10/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1918 - accuracy: 0.9134 - val_loss: 0.1860 - val_accuracy: 0.9159\n",
            "Epoch 11/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1931 - accuracy: 0.9105 - val_loss: 0.1874 - val_accuracy: 0.9137\n",
            "Epoch 12/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1904 - accuracy: 0.9104 - val_loss: 0.1856 - val_accuracy: 0.9151\n",
            "Epoch 13/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1914 - accuracy: 0.9116 - val_loss: 0.1836 - val_accuracy: 0.9165\n",
            "Epoch 14/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9074 - val_loss: 0.1845 - val_accuracy: 0.9168\n",
            "Epoch 15/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1907 - accuracy: 0.9114 - val_loss: 0.1841 - val_accuracy: 0.9155\n",
            "Epoch 16/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1859 - accuracy: 0.9131 - val_loss: 0.1896 - val_accuracy: 0.9156\n",
            "Epoch 17/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1852 - accuracy: 0.9120 - val_loss: 0.1826 - val_accuracy: 0.9172\n",
            "Epoch 18/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1836 - accuracy: 0.9133 - val_loss: 0.1821 - val_accuracy: 0.9160\n",
            "Epoch 19/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1867 - accuracy: 0.9107 - val_loss: 0.1834 - val_accuracy: 0.9175\n",
            "Epoch 20/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1865 - accuracy: 0.9111 - val_loss: 0.1851 - val_accuracy: 0.9135\n",
            "Epoch 21/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1892 - accuracy: 0.9107 - val_loss: 0.1819 - val_accuracy: 0.9171\n",
            "Epoch 22/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1832 - accuracy: 0.9105 - val_loss: 0.1813 - val_accuracy: 0.9162\n",
            "Epoch 23/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1809 - accuracy: 0.9124 - val_loss: 0.1851 - val_accuracy: 0.9154\n",
            "Epoch 24/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9083 - val_loss: 0.1817 - val_accuracy: 0.9163\n",
            "Epoch 25/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1820 - accuracy: 0.9111 - val_loss: 0.1836 - val_accuracy: 0.9135\n",
            "Epoch 26/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1815 - accuracy: 0.9110 - val_loss: 0.1891 - val_accuracy: 0.9155\n",
            "Epoch 27/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1805 - accuracy: 0.9148 - val_loss: 0.1818 - val_accuracy: 0.9165\n",
            "Epoch 28/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1843 - accuracy: 0.9097 - val_loss: 0.1811 - val_accuracy: 0.9168\n",
            "Epoch 29/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1839 - accuracy: 0.9117 - val_loss: 0.1921 - val_accuracy: 0.9156\n",
            "Epoch 30/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1840 - accuracy: 0.9113 - val_loss: 0.1832 - val_accuracy: 0.9156\n",
            "Epoch 31/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9115 - val_loss: 0.1839 - val_accuracy: 0.9146\n",
            "Epoch 32/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1777 - accuracy: 0.9149 - val_loss: 0.1832 - val_accuracy: 0.9149\n",
            "Epoch 33/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1858 - accuracy: 0.9086 - val_loss: 0.1846 - val_accuracy: 0.9155\n",
            "Epoch 34/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1815 - accuracy: 0.9112 - val_loss: 0.1826 - val_accuracy: 0.9158\n",
            "Epoch 35/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1794 - accuracy: 0.9126 - val_loss: 0.1843 - val_accuracy: 0.9159\n",
            "Epoch 36/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1788 - accuracy: 0.9133 - val_loss: 0.1821 - val_accuracy: 0.9159\n",
            "Epoch 37/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9134 - val_loss: 0.1820 - val_accuracy: 0.9156\n",
            "Epoch 38/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1778 - accuracy: 0.9143 - val_loss: 0.1817 - val_accuracy: 0.9163\n",
            "Epoch 39/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1783 - accuracy: 0.9120 - val_loss: 0.1837 - val_accuracy: 0.9166\n",
            "Epoch 40/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1785 - accuracy: 0.9139 - val_loss: 0.1809 - val_accuracy: 0.9165\n",
            "Epoch 41/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1740 - accuracy: 0.9184 - val_loss: 0.1812 - val_accuracy: 0.9167\n",
            "Epoch 42/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1773 - accuracy: 0.9145 - val_loss: 0.1825 - val_accuracy: 0.9157\n",
            "Epoch 43/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.9160 - val_loss: 0.1834 - val_accuracy: 0.9162\n",
            "Epoch 44/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1761 - accuracy: 0.9166 - val_loss: 0.1804 - val_accuracy: 0.9163\n",
            "Epoch 45/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1756 - accuracy: 0.9171 - val_loss: 0.1815 - val_accuracy: 0.9148\n",
            "Epoch 46/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1775 - accuracy: 0.9157 - val_loss: 0.1841 - val_accuracy: 0.9137\n",
            "Epoch 47/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1798 - accuracy: 0.9116 - val_loss: 0.1818 - val_accuracy: 0.9167\n",
            "Epoch 48/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9092 - val_loss: 0.1826 - val_accuracy: 0.9164\n",
            "Epoch 49/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.9142 - val_loss: 0.1848 - val_accuracy: 0.9162\n",
            "Epoch 50/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1759 - accuracy: 0.9164 - val_loss: 0.1806 - val_accuracy: 0.9165\n",
            "Epoch 51/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1746 - accuracy: 0.9140 - val_loss: 0.1840 - val_accuracy: 0.9165\n",
            "Epoch 52/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1741 - accuracy: 0.9123 - val_loss: 0.1819 - val_accuracy: 0.9158\n",
            "Epoch 53/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9127 - val_loss: 0.1818 - val_accuracy: 0.9151\n",
            "Epoch 54/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1747 - accuracy: 0.9150 - val_loss: 0.1820 - val_accuracy: 0.9163\n",
            "Epoch 55/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1768 - accuracy: 0.9123 - val_loss: 0.1812 - val_accuracy: 0.9156\n",
            "Epoch 56/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1770 - accuracy: 0.9157 - val_loss: 0.1798 - val_accuracy: 0.9160\n",
            "Epoch 57/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1748 - accuracy: 0.9151 - val_loss: 0.1835 - val_accuracy: 0.9135\n",
            "Epoch 58/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1785 - accuracy: 0.9139 - val_loss: 0.1825 - val_accuracy: 0.9163\n",
            "Epoch 59/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1694 - accuracy: 0.9184 - val_loss: 0.1806 - val_accuracy: 0.9169\n",
            "Epoch 60/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1743 - accuracy: 0.9170 - val_loss: 0.1812 - val_accuracy: 0.9173\n",
            "Epoch 61/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1739 - accuracy: 0.9156 - val_loss: 0.1834 - val_accuracy: 0.9179\n",
            "Epoch 62/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1735 - accuracy: 0.9153 - val_loss: 0.1816 - val_accuracy: 0.9177\n",
            "Epoch 63/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1714 - accuracy: 0.9185 - val_loss: 0.1813 - val_accuracy: 0.9161\n",
            "Epoch 64/64\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1712 - accuracy: 0.9169 - val_loss: 0.1825 - val_accuracy: 0.9162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpYBGhvIdU3i",
        "outputId": "8d031817-b1be-4396-cc27-4598b5fbc8f4"
      },
      "source": [
        "model_o1 = Sequential()\n",
        "model_o1.add(Dense(128, input_dim = 20,activation='relu'))\n",
        "model_o1.add(Dense(64,activation='relu'))\n",
        "model_o1.add(Dense(32,activation='relu'))\n",
        "model_o1.add(Dense(16,activation='relu'))\n",
        "model_o1.add(Dense(8,activation='relu'))\n",
        "model_o1.add(Dense(4,activation='relu'))\n",
        "model_o1.add(Dense(1,activation='sigmoid'))\n",
        "model_o1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_o1.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=1024)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1024\n",
            "901/901 [==============================] - 3s 2ms/step - loss: 0.9015 - accuracy: 0.8632 - val_loss: 0.2646 - val_accuracy: 0.8925\n",
            "Epoch 2/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2786 - accuracy: 0.8857 - val_loss: 0.2506 - val_accuracy: 0.8926\n",
            "Epoch 3/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2643 - accuracy: 0.8844 - val_loss: 0.2318 - val_accuracy: 0.9013\n",
            "Epoch 4/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2433 - accuracy: 0.8966 - val_loss: 0.2112 - val_accuracy: 0.9054\n",
            "Epoch 5/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2412 - accuracy: 0.8913 - val_loss: 0.2201 - val_accuracy: 0.8991\n",
            "Epoch 6/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2281 - accuracy: 0.8960 - val_loss: 0.2054 - val_accuracy: 0.9021\n",
            "Epoch 7/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2269 - accuracy: 0.8999 - val_loss: 0.2061 - val_accuracy: 0.9029\n",
            "Epoch 8/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2156 - accuracy: 0.9025 - val_loss: 0.2024 - val_accuracy: 0.9097\n",
            "Epoch 9/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2182 - accuracy: 0.9029 - val_loss: 0.2245 - val_accuracy: 0.8952\n",
            "Epoch 10/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2234 - accuracy: 0.9000 - val_loss: 0.2228 - val_accuracy: 0.9050\n",
            "Epoch 11/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2164 - accuracy: 0.9014 - val_loss: 0.2350 - val_accuracy: 0.8851\n",
            "Epoch 12/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2185 - accuracy: 0.9005 - val_loss: 0.2194 - val_accuracy: 0.8982\n",
            "Epoch 13/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2202 - accuracy: 0.9009 - val_loss: 0.2074 - val_accuracy: 0.9075\n",
            "Epoch 14/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2125 - accuracy: 0.9035 - val_loss: 0.1992 - val_accuracy: 0.9101\n",
            "Epoch 15/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2158 - accuracy: 0.9029 - val_loss: 0.2032 - val_accuracy: 0.9075\n",
            "Epoch 16/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2113 - accuracy: 0.9062 - val_loss: 0.2137 - val_accuracy: 0.9048\n",
            "Epoch 17/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2140 - accuracy: 0.9031 - val_loss: 0.2482 - val_accuracy: 0.8858\n",
            "Epoch 18/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2113 - accuracy: 0.9033 - val_loss: 0.2009 - val_accuracy: 0.9095\n",
            "Epoch 19/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2155 - accuracy: 0.9045 - val_loss: 0.1991 - val_accuracy: 0.9105\n",
            "Epoch 20/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2125 - accuracy: 0.9058 - val_loss: 0.1982 - val_accuracy: 0.9115\n",
            "Epoch 21/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2103 - accuracy: 0.9026 - val_loss: 0.1986 - val_accuracy: 0.9092\n",
            "Epoch 22/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2144 - accuracy: 0.9029 - val_loss: 0.2003 - val_accuracy: 0.9099\n",
            "Epoch 23/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2134 - accuracy: 0.9018 - val_loss: 0.2102 - val_accuracy: 0.9109\n",
            "Epoch 24/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2117 - accuracy: 0.9059 - val_loss: 0.1991 - val_accuracy: 0.9093\n",
            "Epoch 25/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2108 - accuracy: 0.9031 - val_loss: 0.1966 - val_accuracy: 0.9115\n",
            "Epoch 26/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2074 - accuracy: 0.9067 - val_loss: 0.1991 - val_accuracy: 0.9116\n",
            "Epoch 27/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2091 - accuracy: 0.9031 - val_loss: 0.2288 - val_accuracy: 0.8807\n",
            "Epoch 28/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2099 - accuracy: 0.9032 - val_loss: 0.1978 - val_accuracy: 0.9116\n",
            "Epoch 29/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2155 - accuracy: 0.9018 - val_loss: 0.2024 - val_accuracy: 0.9082\n",
            "Epoch 30/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2066 - accuracy: 0.9054 - val_loss: 0.1978 - val_accuracy: 0.9095\n",
            "Epoch 31/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2102 - accuracy: 0.9035 - val_loss: 0.1974 - val_accuracy: 0.9085\n",
            "Epoch 32/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2052 - accuracy: 0.9078 - val_loss: 0.1969 - val_accuracy: 0.9107\n",
            "Epoch 33/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2082 - accuracy: 0.9058 - val_loss: 0.2003 - val_accuracy: 0.9054\n",
            "Epoch 34/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2081 - accuracy: 0.9074 - val_loss: 0.1974 - val_accuracy: 0.9110\n",
            "Epoch 35/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2083 - accuracy: 0.9055 - val_loss: 0.2073 - val_accuracy: 0.9018\n",
            "Epoch 36/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2093 - accuracy: 0.9062 - val_loss: 0.1983 - val_accuracy: 0.9132\n",
            "Epoch 37/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9089 - val_loss: 0.2028 - val_accuracy: 0.9069\n",
            "Epoch 38/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2108 - accuracy: 0.9049 - val_loss: 0.1957 - val_accuracy: 0.9123\n",
            "Epoch 39/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2069 - accuracy: 0.9075 - val_loss: 0.1963 - val_accuracy: 0.9128\n",
            "Epoch 40/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2076 - accuracy: 0.9058 - val_loss: 0.2058 - val_accuracy: 0.9105\n",
            "Epoch 41/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2065 - accuracy: 0.9056 - val_loss: 0.1939 - val_accuracy: 0.9118\n",
            "Epoch 42/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2072 - accuracy: 0.9074 - val_loss: 0.1963 - val_accuracy: 0.9129\n",
            "Epoch 43/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9094 - val_loss: 0.1939 - val_accuracy: 0.9123\n",
            "Epoch 44/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2109 - accuracy: 0.9024 - val_loss: 0.2285 - val_accuracy: 0.9041\n",
            "Epoch 45/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2051 - accuracy: 0.9065 - val_loss: 0.1956 - val_accuracy: 0.9129\n",
            "Epoch 46/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2067 - accuracy: 0.9057 - val_loss: 0.2009 - val_accuracy: 0.9127\n",
            "Epoch 47/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2023 - accuracy: 0.9073 - val_loss: 0.1974 - val_accuracy: 0.9106\n",
            "Epoch 48/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2063 - accuracy: 0.9096 - val_loss: 0.1960 - val_accuracy: 0.9094\n",
            "Epoch 49/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2046 - accuracy: 0.9060 - val_loss: 0.1995 - val_accuracy: 0.9105\n",
            "Epoch 50/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2062 - accuracy: 0.9064 - val_loss: 0.1960 - val_accuracy: 0.9114\n",
            "Epoch 51/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2036 - accuracy: 0.9072 - val_loss: 0.2073 - val_accuracy: 0.9114\n",
            "Epoch 52/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2047 - accuracy: 0.9076 - val_loss: 0.1936 - val_accuracy: 0.9118\n",
            "Epoch 53/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9099 - val_loss: 0.1950 - val_accuracy: 0.9119\n",
            "Epoch 54/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2053 - accuracy: 0.9076 - val_loss: 0.2002 - val_accuracy: 0.9109\n",
            "Epoch 55/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2029 - accuracy: 0.9057 - val_loss: 0.1920 - val_accuracy: 0.9126\n",
            "Epoch 56/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2015 - accuracy: 0.9095 - val_loss: 0.1996 - val_accuracy: 0.9131\n",
            "Epoch 57/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9091 - val_loss: 0.1969 - val_accuracy: 0.9109\n",
            "Epoch 58/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2051 - accuracy: 0.9087 - val_loss: 0.1949 - val_accuracy: 0.9113\n",
            "Epoch 59/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2053 - accuracy: 0.9064 - val_loss: 0.1954 - val_accuracy: 0.9127\n",
            "Epoch 60/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2058 - accuracy: 0.9076 - val_loss: 0.1935 - val_accuracy: 0.9114\n",
            "Epoch 61/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9091 - val_loss: 0.1981 - val_accuracy: 0.9143\n",
            "Epoch 62/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2025 - accuracy: 0.9107 - val_loss: 0.2087 - val_accuracy: 0.9128\n",
            "Epoch 63/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2079 - accuracy: 0.9081 - val_loss: 0.1962 - val_accuracy: 0.9087\n",
            "Epoch 64/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2081 - accuracy: 0.9029 - val_loss: 0.1972 - val_accuracy: 0.9077\n",
            "Epoch 65/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9141 - val_loss: 0.1984 - val_accuracy: 0.9132\n",
            "Epoch 66/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2052 - accuracy: 0.9055 - val_loss: 0.1995 - val_accuracy: 0.9102\n",
            "Epoch 67/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2061 - accuracy: 0.9059 - val_loss: 0.1939 - val_accuracy: 0.9117\n",
            "Epoch 68/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2026 - accuracy: 0.9066 - val_loss: 0.1967 - val_accuracy: 0.9108\n",
            "Epoch 69/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9070 - val_loss: 0.1974 - val_accuracy: 0.9126\n",
            "Epoch 70/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9073 - val_loss: 0.1923 - val_accuracy: 0.9116\n",
            "Epoch 71/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2005 - accuracy: 0.9089 - val_loss: 0.2064 - val_accuracy: 0.9076\n",
            "Epoch 72/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9082 - val_loss: 0.1945 - val_accuracy: 0.9109\n",
            "Epoch 73/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9099 - val_loss: 0.1961 - val_accuracy: 0.9132\n",
            "Epoch 74/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2003 - accuracy: 0.9106 - val_loss: 0.1965 - val_accuracy: 0.9108\n",
            "Epoch 75/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9093 - val_loss: 0.2001 - val_accuracy: 0.9109\n",
            "Epoch 76/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9098 - val_loss: 0.1992 - val_accuracy: 0.9127\n",
            "Epoch 77/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9080 - val_loss: 0.1938 - val_accuracy: 0.9132\n",
            "Epoch 78/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9088 - val_loss: 0.1929 - val_accuracy: 0.9127\n",
            "Epoch 79/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2033 - accuracy: 0.9062 - val_loss: 0.1916 - val_accuracy: 0.9131\n",
            "Epoch 80/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2037 - accuracy: 0.9070 - val_loss: 0.1928 - val_accuracy: 0.9127\n",
            "Epoch 81/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2019 - accuracy: 0.9102 - val_loss: 0.1935 - val_accuracy: 0.9127\n",
            "Epoch 82/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2056 - accuracy: 0.9062 - val_loss: 0.2012 - val_accuracy: 0.9112\n",
            "Epoch 83/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9079 - val_loss: 0.2120 - val_accuracy: 0.9088\n",
            "Epoch 84/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9084 - val_loss: 0.1942 - val_accuracy: 0.9098\n",
            "Epoch 85/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2065 - accuracy: 0.9062 - val_loss: 0.2007 - val_accuracy: 0.9113\n",
            "Epoch 86/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2019 - accuracy: 0.9076 - val_loss: 0.1958 - val_accuracy: 0.9139\n",
            "Epoch 87/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9066 - val_loss: 0.1957 - val_accuracy: 0.9139\n",
            "Epoch 88/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2034 - accuracy: 0.9071 - val_loss: 0.1944 - val_accuracy: 0.9116\n",
            "Epoch 89/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9104 - val_loss: 0.1922 - val_accuracy: 0.9125\n",
            "Epoch 90/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2048 - accuracy: 0.9067 - val_loss: 0.1952 - val_accuracy: 0.9139\n",
            "Epoch 91/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9095 - val_loss: 0.1916 - val_accuracy: 0.9125\n",
            "Epoch 92/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9083 - val_loss: 0.1924 - val_accuracy: 0.9117\n",
            "Epoch 93/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9119 - val_loss: 0.1930 - val_accuracy: 0.9123\n",
            "Epoch 94/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9092 - val_loss: 0.2045 - val_accuracy: 0.9102\n",
            "Epoch 95/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2051 - accuracy: 0.9088 - val_loss: 0.1929 - val_accuracy: 0.9118\n",
            "Epoch 96/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9075 - val_loss: 0.1951 - val_accuracy: 0.9134\n",
            "Epoch 97/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9113 - val_loss: 0.1925 - val_accuracy: 0.9135\n",
            "Epoch 98/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9088 - val_loss: 0.1916 - val_accuracy: 0.9118\n",
            "Epoch 99/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2038 - accuracy: 0.9089 - val_loss: 0.1914 - val_accuracy: 0.9130\n",
            "Epoch 100/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9098 - val_loss: 0.1923 - val_accuracy: 0.9123\n",
            "Epoch 101/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9077 - val_loss: 0.2001 - val_accuracy: 0.9118\n",
            "Epoch 102/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9101 - val_loss: 0.1968 - val_accuracy: 0.9132\n",
            "Epoch 103/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2025 - accuracy: 0.9071 - val_loss: 0.1944 - val_accuracy: 0.9126\n",
            "Epoch 104/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9090 - val_loss: 0.1944 - val_accuracy: 0.9123\n",
            "Epoch 105/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2025 - accuracy: 0.9064 - val_loss: 0.1949 - val_accuracy: 0.9119\n",
            "Epoch 106/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9094 - val_loss: 0.1953 - val_accuracy: 0.9113\n",
            "Epoch 107/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9086 - val_loss: 0.1903 - val_accuracy: 0.9133\n",
            "Epoch 108/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2046 - accuracy: 0.9055 - val_loss: 0.1939 - val_accuracy: 0.9131\n",
            "Epoch 109/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1995 - accuracy: 0.9103 - val_loss: 0.2010 - val_accuracy: 0.9119\n",
            "Epoch 110/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9080 - val_loss: 0.1927 - val_accuracy: 0.9140\n",
            "Epoch 111/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9083 - val_loss: 0.1963 - val_accuracy: 0.9101\n",
            "Epoch 112/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2042 - accuracy: 0.9074 - val_loss: 0.2019 - val_accuracy: 0.9085\n",
            "Epoch 113/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9055 - val_loss: 0.1975 - val_accuracy: 0.9111\n",
            "Epoch 114/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9099 - val_loss: 0.1942 - val_accuracy: 0.9135\n",
            "Epoch 115/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9082 - val_loss: 0.1987 - val_accuracy: 0.9130\n",
            "Epoch 116/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1959 - accuracy: 0.9100 - val_loss: 0.1952 - val_accuracy: 0.9136\n",
            "Epoch 117/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9074 - val_loss: 0.1915 - val_accuracy: 0.9137\n",
            "Epoch 118/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9102 - val_loss: 0.1999 - val_accuracy: 0.9101\n",
            "Epoch 119/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9095 - val_loss: 0.1933 - val_accuracy: 0.9124\n",
            "Epoch 120/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9111 - val_loss: 0.1916 - val_accuracy: 0.9136\n",
            "Epoch 121/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9126 - val_loss: 0.1921 - val_accuracy: 0.9135\n",
            "Epoch 122/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9061 - val_loss: 0.1978 - val_accuracy: 0.9117\n",
            "Epoch 123/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2026 - accuracy: 0.9077 - val_loss: 0.1937 - val_accuracy: 0.9122\n",
            "Epoch 124/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9064 - val_loss: 0.1919 - val_accuracy: 0.9128\n",
            "Epoch 125/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9093 - val_loss: 0.2037 - val_accuracy: 0.9106\n",
            "Epoch 126/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2034 - accuracy: 0.9076 - val_loss: 0.2054 - val_accuracy: 0.9108\n",
            "Epoch 127/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9065 - val_loss: 0.1904 - val_accuracy: 0.9127\n",
            "Epoch 128/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9084 - val_loss: 0.1918 - val_accuracy: 0.9126\n",
            "Epoch 129/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9090 - val_loss: 0.1960 - val_accuracy: 0.9135\n",
            "Epoch 130/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9089 - val_loss: 0.1933 - val_accuracy: 0.9129\n",
            "Epoch 131/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2015 - accuracy: 0.9075 - val_loss: 0.1925 - val_accuracy: 0.9129\n",
            "Epoch 132/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9094 - val_loss: 0.1941 - val_accuracy: 0.9113\n",
            "Epoch 133/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9083 - val_loss: 0.1917 - val_accuracy: 0.9137\n",
            "Epoch 134/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9099 - val_loss: 0.1916 - val_accuracy: 0.9130\n",
            "Epoch 135/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9087 - val_loss: 0.1916 - val_accuracy: 0.9118\n",
            "Epoch 136/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1995 - accuracy: 0.9085 - val_loss: 0.1921 - val_accuracy: 0.9126\n",
            "Epoch 137/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9087 - val_loss: 0.2009 - val_accuracy: 0.9077\n",
            "Epoch 138/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9143 - val_loss: 0.1986 - val_accuracy: 0.9127\n",
            "Epoch 139/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9096 - val_loss: 0.1961 - val_accuracy: 0.9112\n",
            "Epoch 140/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9122 - val_loss: 0.1921 - val_accuracy: 0.9132\n",
            "Epoch 141/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9081 - val_loss: 0.1921 - val_accuracy: 0.9136\n",
            "Epoch 142/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1978 - accuracy: 0.9072 - val_loss: 0.1976 - val_accuracy: 0.9132\n",
            "Epoch 143/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9087 - val_loss: 0.1933 - val_accuracy: 0.9132\n",
            "Epoch 144/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9093 - val_loss: 0.1908 - val_accuracy: 0.9128\n",
            "Epoch 145/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9124 - val_loss: 0.1896 - val_accuracy: 0.9122\n",
            "Epoch 146/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2008 - accuracy: 0.9073 - val_loss: 0.1913 - val_accuracy: 0.9136\n",
            "Epoch 147/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9063 - val_loss: 0.1968 - val_accuracy: 0.9122\n",
            "Epoch 148/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9090 - val_loss: 0.1930 - val_accuracy: 0.9115\n",
            "Epoch 149/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2020 - accuracy: 0.9089 - val_loss: 0.1916 - val_accuracy: 0.9137\n",
            "Epoch 150/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2069 - accuracy: 0.9055 - val_loss: 0.1911 - val_accuracy: 0.9140\n",
            "Epoch 151/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9090 - val_loss: 0.1914 - val_accuracy: 0.9109\n",
            "Epoch 152/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2004 - accuracy: 0.9098 - val_loss: 0.1929 - val_accuracy: 0.9114\n",
            "Epoch 153/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9107 - val_loss: 0.1900 - val_accuracy: 0.9136\n",
            "Epoch 154/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9084 - val_loss: 0.1919 - val_accuracy: 0.9116\n",
            "Epoch 155/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2020 - accuracy: 0.9080 - val_loss: 0.1927 - val_accuracy: 0.9126\n",
            "Epoch 156/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9095 - val_loss: 0.1901 - val_accuracy: 0.9119\n",
            "Epoch 157/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9097 - val_loss: 0.1935 - val_accuracy: 0.9135\n",
            "Epoch 158/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1997 - accuracy: 0.9081 - val_loss: 0.1903 - val_accuracy: 0.9123\n",
            "Epoch 159/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1974 - accuracy: 0.9102 - val_loss: 0.1898 - val_accuracy: 0.9136\n",
            "Epoch 160/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1974 - accuracy: 0.9116 - val_loss: 0.1925 - val_accuracy: 0.9119\n",
            "Epoch 161/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9101 - val_loss: 0.2044 - val_accuracy: 0.9098\n",
            "Epoch 162/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9070 - val_loss: 0.2044 - val_accuracy: 0.9129\n",
            "Epoch 163/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1995 - accuracy: 0.9087 - val_loss: 0.1899 - val_accuracy: 0.9138\n",
            "Epoch 164/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9096 - val_loss: 0.1931 - val_accuracy: 0.9122\n",
            "Epoch 165/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2000 - accuracy: 0.9089 - val_loss: 0.1929 - val_accuracy: 0.9109\n",
            "Epoch 166/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9100 - val_loss: 0.1921 - val_accuracy: 0.9136\n",
            "Epoch 167/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2046 - accuracy: 0.9045 - val_loss: 0.1920 - val_accuracy: 0.9105\n",
            "Epoch 168/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1978 - accuracy: 0.9120 - val_loss: 0.1922 - val_accuracy: 0.9143\n",
            "Epoch 169/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9108 - val_loss: 0.1898 - val_accuracy: 0.9143\n",
            "Epoch 170/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9097 - val_loss: 0.1912 - val_accuracy: 0.9143\n",
            "Epoch 171/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9115 - val_loss: 0.1907 - val_accuracy: 0.9125\n",
            "Epoch 172/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2008 - accuracy: 0.9087 - val_loss: 0.1948 - val_accuracy: 0.9120\n",
            "Epoch 173/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9089 - val_loss: 0.1907 - val_accuracy: 0.9131\n",
            "Epoch 174/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9093 - val_loss: 0.1942 - val_accuracy: 0.9132\n",
            "Epoch 175/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1988 - accuracy: 0.9094 - val_loss: 0.1963 - val_accuracy: 0.9120\n",
            "Epoch 176/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9131 - val_loss: 0.1962 - val_accuracy: 0.9132\n",
            "Epoch 177/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9114 - val_loss: 0.1908 - val_accuracy: 0.9134\n",
            "Epoch 178/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9108 - val_loss: 0.1921 - val_accuracy: 0.9111\n",
            "Epoch 179/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9096 - val_loss: 0.1900 - val_accuracy: 0.9143\n",
            "Epoch 180/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9110 - val_loss: 0.2002 - val_accuracy: 0.9101\n",
            "Epoch 181/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9086 - val_loss: 0.1948 - val_accuracy: 0.9112\n",
            "Epoch 182/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9094 - val_loss: 0.1907 - val_accuracy: 0.9135\n",
            "Epoch 183/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9091 - val_loss: 0.1912 - val_accuracy: 0.9139\n",
            "Epoch 184/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9075 - val_loss: 0.1899 - val_accuracy: 0.9139\n",
            "Epoch 185/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9102 - val_loss: 0.1899 - val_accuracy: 0.9130\n",
            "Epoch 186/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9101 - val_loss: 0.1898 - val_accuracy: 0.9126\n",
            "Epoch 187/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9115 - val_loss: 0.1912 - val_accuracy: 0.9118\n",
            "Epoch 188/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9116 - val_loss: 0.1904 - val_accuracy: 0.9134\n",
            "Epoch 189/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9117 - val_loss: 0.1910 - val_accuracy: 0.9137\n",
            "Epoch 190/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9114 - val_loss: 0.1911 - val_accuracy: 0.9128\n",
            "Epoch 191/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9123 - val_loss: 0.1996 - val_accuracy: 0.9105\n",
            "Epoch 192/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9078 - val_loss: 0.1902 - val_accuracy: 0.9127\n",
            "Epoch 193/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9086 - val_loss: 0.1902 - val_accuracy: 0.9118\n",
            "Epoch 194/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9085 - val_loss: 0.1902 - val_accuracy: 0.9132\n",
            "Epoch 195/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9076 - val_loss: 0.1908 - val_accuracy: 0.9135\n",
            "Epoch 196/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9099 - val_loss: 0.2069 - val_accuracy: 0.9097\n",
            "Epoch 197/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9077 - val_loss: 0.1902 - val_accuracy: 0.9130\n",
            "Epoch 198/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9123 - val_loss: 0.1909 - val_accuracy: 0.9136\n",
            "Epoch 199/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9131 - val_loss: 0.1915 - val_accuracy: 0.9127\n",
            "Epoch 200/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9125 - val_loss: 0.1963 - val_accuracy: 0.9125\n",
            "Epoch 201/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9071 - val_loss: 0.1916 - val_accuracy: 0.9113\n",
            "Epoch 202/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1970 - accuracy: 0.9096 - val_loss: 0.1969 - val_accuracy: 0.9111\n",
            "Epoch 203/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2008 - accuracy: 0.9087 - val_loss: 0.1912 - val_accuracy: 0.9119\n",
            "Epoch 204/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9071 - val_loss: 0.1944 - val_accuracy: 0.9140\n",
            "Epoch 205/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9083 - val_loss: 0.1900 - val_accuracy: 0.9135\n",
            "Epoch 206/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9107 - val_loss: 0.1920 - val_accuracy: 0.9132\n",
            "Epoch 207/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9108 - val_loss: 0.1996 - val_accuracy: 0.9126\n",
            "Epoch 208/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9126 - val_loss: 0.1899 - val_accuracy: 0.9131\n",
            "Epoch 209/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9113 - val_loss: 0.1904 - val_accuracy: 0.9128\n",
            "Epoch 210/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9095 - val_loss: 0.1915 - val_accuracy: 0.9127\n",
            "Epoch 211/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9116 - val_loss: 0.1917 - val_accuracy: 0.9143\n",
            "Epoch 212/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9099 - val_loss: 0.1923 - val_accuracy: 0.9138\n",
            "Epoch 213/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9091 - val_loss: 0.1896 - val_accuracy: 0.9138\n",
            "Epoch 214/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9100 - val_loss: 0.2061 - val_accuracy: 0.9102\n",
            "Epoch 215/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9081 - val_loss: 0.1904 - val_accuracy: 0.9132\n",
            "Epoch 216/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9150 - val_loss: 0.1918 - val_accuracy: 0.9135\n",
            "Epoch 217/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9124 - val_loss: 0.1993 - val_accuracy: 0.9134\n",
            "Epoch 218/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9130 - val_loss: 0.1956 - val_accuracy: 0.9137\n",
            "Epoch 219/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9120 - val_loss: 0.2062 - val_accuracy: 0.9097\n",
            "Epoch 220/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9101 - val_loss: 0.1900 - val_accuracy: 0.9137\n",
            "Epoch 221/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9128 - val_loss: 0.1926 - val_accuracy: 0.9139\n",
            "Epoch 222/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9093 - val_loss: 0.1894 - val_accuracy: 0.9129\n",
            "Epoch 223/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9130 - val_loss: 0.1904 - val_accuracy: 0.9122\n",
            "Epoch 224/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2006 - accuracy: 0.9046 - val_loss: 0.1914 - val_accuracy: 0.9120\n",
            "Epoch 225/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1995 - accuracy: 0.9081 - val_loss: 0.1908 - val_accuracy: 0.9128\n",
            "Epoch 226/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9113 - val_loss: 0.1923 - val_accuracy: 0.9131\n",
            "Epoch 227/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9069 - val_loss: 0.1908 - val_accuracy: 0.9140\n",
            "Epoch 228/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9095 - val_loss: 0.1946 - val_accuracy: 0.9145\n",
            "Epoch 229/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9078 - val_loss: 0.1959 - val_accuracy: 0.9135\n",
            "Epoch 230/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9091 - val_loss: 0.1896 - val_accuracy: 0.9124\n",
            "Epoch 231/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9088 - val_loss: 0.1959 - val_accuracy: 0.9135\n",
            "Epoch 232/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9085 - val_loss: 0.1936 - val_accuracy: 0.9144\n",
            "Epoch 233/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9060 - val_loss: 0.1930 - val_accuracy: 0.9143\n",
            "Epoch 234/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9115 - val_loss: 0.1894 - val_accuracy: 0.9131\n",
            "Epoch 235/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9090 - val_loss: 0.1910 - val_accuracy: 0.9143\n",
            "Epoch 236/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9134 - val_loss: 0.1946 - val_accuracy: 0.9139\n",
            "Epoch 237/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9121 - val_loss: 0.1944 - val_accuracy: 0.9114\n",
            "Epoch 238/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9095 - val_loss: 0.1899 - val_accuracy: 0.9143\n",
            "Epoch 239/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1995 - accuracy: 0.9086 - val_loss: 0.1945 - val_accuracy: 0.9106\n",
            "Epoch 240/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9093 - val_loss: 0.1902 - val_accuracy: 0.9132\n",
            "Epoch 241/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9100 - val_loss: 0.1910 - val_accuracy: 0.9136\n",
            "Epoch 242/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9109 - val_loss: 0.1909 - val_accuracy: 0.9139\n",
            "Epoch 243/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9118 - val_loss: 0.1898 - val_accuracy: 0.9149\n",
            "Epoch 244/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9114 - val_loss: 0.2007 - val_accuracy: 0.9108\n",
            "Epoch 245/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9077 - val_loss: 0.1901 - val_accuracy: 0.9143\n",
            "Epoch 246/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9100 - val_loss: 0.1893 - val_accuracy: 0.9131\n",
            "Epoch 247/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2008 - accuracy: 0.9100 - val_loss: 0.2037 - val_accuracy: 0.9103\n",
            "Epoch 248/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2036 - accuracy: 0.9059 - val_loss: 0.1917 - val_accuracy: 0.9126\n",
            "Epoch 249/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9092 - val_loss: 0.1898 - val_accuracy: 0.9148\n",
            "Epoch 250/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9100 - val_loss: 0.1902 - val_accuracy: 0.9123\n",
            "Epoch 251/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9129 - val_loss: 0.1896 - val_accuracy: 0.9131\n",
            "Epoch 252/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9126 - val_loss: 0.1899 - val_accuracy: 0.9128\n",
            "Epoch 253/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9092 - val_loss: 0.2065 - val_accuracy: 0.9092\n",
            "Epoch 254/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9091 - val_loss: 0.1949 - val_accuracy: 0.9129\n",
            "Epoch 255/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9100 - val_loss: 0.1887 - val_accuracy: 0.9140\n",
            "Epoch 256/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9076 - val_loss: 0.1961 - val_accuracy: 0.9116\n",
            "Epoch 257/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9083 - val_loss: 0.1911 - val_accuracy: 0.9143\n",
            "Epoch 258/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9111 - val_loss: 0.1942 - val_accuracy: 0.9137\n",
            "Epoch 259/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9102 - val_loss: 0.1903 - val_accuracy: 0.9125\n",
            "Epoch 260/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9113 - val_loss: 0.1909 - val_accuracy: 0.9141\n",
            "Epoch 261/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9086 - val_loss: 0.1926 - val_accuracy: 0.9131\n",
            "Epoch 262/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9086 - val_loss: 0.1890 - val_accuracy: 0.9139\n",
            "Epoch 263/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9104 - val_loss: 0.1905 - val_accuracy: 0.9142\n",
            "Epoch 264/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9102 - val_loss: 0.1932 - val_accuracy: 0.9115\n",
            "Epoch 265/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9087 - val_loss: 0.1915 - val_accuracy: 0.9148\n",
            "Epoch 266/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1982 - accuracy: 0.9088 - val_loss: 0.1905 - val_accuracy: 0.9117\n",
            "Epoch 267/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9092 - val_loss: 0.1932 - val_accuracy: 0.9134\n",
            "Epoch 268/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9100 - val_loss: 0.1946 - val_accuracy: 0.9118\n",
            "Epoch 269/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9103 - val_loss: 0.1941 - val_accuracy: 0.9118\n",
            "Epoch 270/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1963 - accuracy: 0.9087 - val_loss: 0.1912 - val_accuracy: 0.9144\n",
            "Epoch 271/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1938 - accuracy: 0.9100 - val_loss: 0.1904 - val_accuracy: 0.9148\n",
            "Epoch 272/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9106 - val_loss: 0.1940 - val_accuracy: 0.9101\n",
            "Epoch 273/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9083 - val_loss: 0.1909 - val_accuracy: 0.9147\n",
            "Epoch 274/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9111 - val_loss: 0.1932 - val_accuracy: 0.9123\n",
            "Epoch 275/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9073 - val_loss: 0.1986 - val_accuracy: 0.9122\n",
            "Epoch 276/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9125 - val_loss: 0.1904 - val_accuracy: 0.9143\n",
            "Epoch 277/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1988 - accuracy: 0.9112 - val_loss: 0.1903 - val_accuracy: 0.9137\n",
            "Epoch 278/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1968 - accuracy: 0.9109 - val_loss: 0.1898 - val_accuracy: 0.9140\n",
            "Epoch 279/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9094 - val_loss: 0.1948 - val_accuracy: 0.9135\n",
            "Epoch 280/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9098 - val_loss: 0.1906 - val_accuracy: 0.9121\n",
            "Epoch 281/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9120 - val_loss: 0.1908 - val_accuracy: 0.9135\n",
            "Epoch 282/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9079 - val_loss: 0.1889 - val_accuracy: 0.9148\n",
            "Epoch 283/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9138 - val_loss: 0.1896 - val_accuracy: 0.9153\n",
            "Epoch 284/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9121 - val_loss: 0.1932 - val_accuracy: 0.9123\n",
            "Epoch 285/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9114 - val_loss: 0.1899 - val_accuracy: 0.9140\n",
            "Epoch 286/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1928 - accuracy: 0.9116 - val_loss: 0.1891 - val_accuracy: 0.9137\n",
            "Epoch 287/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9085 - val_loss: 0.1902 - val_accuracy: 0.9126\n",
            "Epoch 288/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9107 - val_loss: 0.1883 - val_accuracy: 0.9151\n",
            "Epoch 289/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9117 - val_loss: 0.1888 - val_accuracy: 0.9150\n",
            "Epoch 290/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9106 - val_loss: 0.1888 - val_accuracy: 0.9138\n",
            "Epoch 291/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9083 - val_loss: 0.1922 - val_accuracy: 0.9143\n",
            "Epoch 292/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9112 - val_loss: 0.1895 - val_accuracy: 0.9141\n",
            "Epoch 293/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9105 - val_loss: 0.1899 - val_accuracy: 0.9150\n",
            "Epoch 294/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9142 - val_loss: 0.1909 - val_accuracy: 0.9121\n",
            "Epoch 295/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1960 - accuracy: 0.9106 - val_loss: 0.1975 - val_accuracy: 0.9129\n",
            "Epoch 296/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1904 - accuracy: 0.9157 - val_loss: 0.1883 - val_accuracy: 0.9147\n",
            "Epoch 297/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1993 - accuracy: 0.9087 - val_loss: 0.1959 - val_accuracy: 0.9133\n",
            "Epoch 298/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1940 - accuracy: 0.9102 - val_loss: 0.1982 - val_accuracy: 0.9138\n",
            "Epoch 299/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1961 - accuracy: 0.9105 - val_loss: 0.1896 - val_accuracy: 0.9140\n",
            "Epoch 300/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1968 - accuracy: 0.9117 - val_loss: 0.1945 - val_accuracy: 0.9133\n",
            "Epoch 301/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9110 - val_loss: 0.1943 - val_accuracy: 0.9139\n",
            "Epoch 302/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9066 - val_loss: 0.1929 - val_accuracy: 0.9120\n",
            "Epoch 303/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1913 - accuracy: 0.9135 - val_loss: 0.1915 - val_accuracy: 0.9130\n",
            "Epoch 304/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9093 - val_loss: 0.1896 - val_accuracy: 0.9149\n",
            "Epoch 305/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1933 - accuracy: 0.9106 - val_loss: 0.1907 - val_accuracy: 0.9132\n",
            "Epoch 306/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1976 - accuracy: 0.9101 - val_loss: 0.1921 - val_accuracy: 0.9118\n",
            "Epoch 307/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1962 - accuracy: 0.9108 - val_loss: 0.1886 - val_accuracy: 0.9139\n",
            "Epoch 308/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1927 - accuracy: 0.9103 - val_loss: 0.1961 - val_accuracy: 0.9114\n",
            "Epoch 309/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1914 - accuracy: 0.9121 - val_loss: 0.1892 - val_accuracy: 0.9146\n",
            "Epoch 310/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9115 - val_loss: 0.1876 - val_accuracy: 0.9149\n",
            "Epoch 311/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1926 - accuracy: 0.9135 - val_loss: 0.1910 - val_accuracy: 0.9129\n",
            "Epoch 312/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9102 - val_loss: 0.1905 - val_accuracy: 0.9141\n",
            "Epoch 313/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1999 - accuracy: 0.9089 - val_loss: 0.1883 - val_accuracy: 0.9136\n",
            "Epoch 314/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9138 - val_loss: 0.1899 - val_accuracy: 0.9133\n",
            "Epoch 315/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1926 - accuracy: 0.9121 - val_loss: 0.1955 - val_accuracy: 0.9135\n",
            "Epoch 316/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9109 - val_loss: 0.1904 - val_accuracy: 0.9131\n",
            "Epoch 317/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1928 - accuracy: 0.9104 - val_loss: 0.1892 - val_accuracy: 0.9123\n",
            "Epoch 318/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9102 - val_loss: 0.1916 - val_accuracy: 0.9135\n",
            "Epoch 319/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9143 - val_loss: 0.1894 - val_accuracy: 0.9111\n",
            "Epoch 320/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 0.1892 - val_accuracy: 0.9152\n",
            "Epoch 321/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1904 - accuracy: 0.9117 - val_loss: 0.1890 - val_accuracy: 0.9149\n",
            "Epoch 322/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9128 - val_loss: 0.1895 - val_accuracy: 0.9138\n",
            "Epoch 323/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1956 - accuracy: 0.9116 - val_loss: 0.1912 - val_accuracy: 0.9142\n",
            "Epoch 324/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9103 - val_loss: 0.1898 - val_accuracy: 0.9120\n",
            "Epoch 325/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9110 - val_loss: 0.1933 - val_accuracy: 0.9121\n",
            "Epoch 326/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1992 - accuracy: 0.9099 - val_loss: 0.1880 - val_accuracy: 0.9127\n",
            "Epoch 327/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9124 - val_loss: 0.1897 - val_accuracy: 0.9126\n",
            "Epoch 328/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9084 - val_loss: 0.1893 - val_accuracy: 0.9145\n",
            "Epoch 329/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9140 - val_loss: 0.1953 - val_accuracy: 0.9101\n",
            "Epoch 330/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9106 - val_loss: 0.1880 - val_accuracy: 0.9145\n",
            "Epoch 331/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1949 - accuracy: 0.9091 - val_loss: 0.1886 - val_accuracy: 0.9131\n",
            "Epoch 332/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9110 - val_loss: 0.1910 - val_accuracy: 0.9124\n",
            "Epoch 333/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9079 - val_loss: 0.1906 - val_accuracy: 0.9150\n",
            "Epoch 334/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1915 - accuracy: 0.9149 - val_loss: 0.1893 - val_accuracy: 0.9160\n",
            "Epoch 335/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1919 - accuracy: 0.9106 - val_loss: 0.1971 - val_accuracy: 0.9140\n",
            "Epoch 336/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1945 - accuracy: 0.9106 - val_loss: 0.1900 - val_accuracy: 0.9152\n",
            "Epoch 337/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9139 - val_loss: 0.1889 - val_accuracy: 0.9142\n",
            "Epoch 338/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9084 - val_loss: 0.1895 - val_accuracy: 0.9137\n",
            "Epoch 339/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9106 - val_loss: 0.1871 - val_accuracy: 0.9150\n",
            "Epoch 340/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1933 - accuracy: 0.9092 - val_loss: 0.1904 - val_accuracy: 0.9145\n",
            "Epoch 341/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1961 - accuracy: 0.9088 - val_loss: 0.1901 - val_accuracy: 0.9138\n",
            "Epoch 342/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1959 - accuracy: 0.9076 - val_loss: 0.1879 - val_accuracy: 0.9142\n",
            "Epoch 343/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9105 - val_loss: 0.1912 - val_accuracy: 0.9142\n",
            "Epoch 344/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9101 - val_loss: 0.1883 - val_accuracy: 0.9138\n",
            "Epoch 345/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1941 - accuracy: 0.9088 - val_loss: 0.1901 - val_accuracy: 0.9121\n",
            "Epoch 346/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1866 - accuracy: 0.9143 - val_loss: 0.1887 - val_accuracy: 0.9131\n",
            "Epoch 347/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1970 - accuracy: 0.9087 - val_loss: 0.1897 - val_accuracy: 0.9130\n",
            "Epoch 348/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9137 - val_loss: 0.1884 - val_accuracy: 0.9135\n",
            "Epoch 349/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9098 - val_loss: 0.1901 - val_accuracy: 0.9143\n",
            "Epoch 350/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9126 - val_loss: 0.1910 - val_accuracy: 0.9148\n",
            "Epoch 351/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1945 - accuracy: 0.9104 - val_loss: 0.1897 - val_accuracy: 0.9137\n",
            "Epoch 352/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9135 - val_loss: 0.1897 - val_accuracy: 0.9139\n",
            "Epoch 353/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9120 - val_loss: 0.1884 - val_accuracy: 0.9139\n",
            "Epoch 354/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9122 - val_loss: 0.1963 - val_accuracy: 0.9114\n",
            "Epoch 355/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9122 - val_loss: 0.1889 - val_accuracy: 0.9149\n",
            "Epoch 356/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1925 - accuracy: 0.9124 - val_loss: 0.1887 - val_accuracy: 0.9128\n",
            "Epoch 357/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1910 - accuracy: 0.9123 - val_loss: 0.1901 - val_accuracy: 0.9121\n",
            "Epoch 358/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1990 - accuracy: 0.9064 - val_loss: 0.1874 - val_accuracy: 0.9153\n",
            "Epoch 359/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1943 - accuracy: 0.9120 - val_loss: 0.1905 - val_accuracy: 0.9131\n",
            "Epoch 360/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9112 - val_loss: 0.1905 - val_accuracy: 0.9135\n",
            "Epoch 361/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9110 - val_loss: 0.1931 - val_accuracy: 0.9129\n",
            "Epoch 362/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9096 - val_loss: 0.1871 - val_accuracy: 0.9146\n",
            "Epoch 363/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1904 - accuracy: 0.9129 - val_loss: 0.1944 - val_accuracy: 0.9129\n",
            "Epoch 364/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1921 - accuracy: 0.9129 - val_loss: 0.1873 - val_accuracy: 0.9138\n",
            "Epoch 365/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1918 - accuracy: 0.9111 - val_loss: 0.1895 - val_accuracy: 0.9146\n",
            "Epoch 366/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1947 - accuracy: 0.9089 - val_loss: 0.1933 - val_accuracy: 0.9132\n",
            "Epoch 367/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1919 - accuracy: 0.9149 - val_loss: 0.1896 - val_accuracy: 0.9154\n",
            "Epoch 368/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1939 - accuracy: 0.9102 - val_loss: 0.1902 - val_accuracy: 0.9130\n",
            "Epoch 369/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1915 - accuracy: 0.9085 - val_loss: 0.1883 - val_accuracy: 0.9133\n",
            "Epoch 370/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1936 - accuracy: 0.9111 - val_loss: 0.1902 - val_accuracy: 0.9139\n",
            "Epoch 371/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9105 - val_loss: 0.1962 - val_accuracy: 0.9131\n",
            "Epoch 372/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1961 - accuracy: 0.9104 - val_loss: 0.1901 - val_accuracy: 0.9146\n",
            "Epoch 373/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9127 - val_loss: 0.1893 - val_accuracy: 0.9139\n",
            "Epoch 374/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1910 - accuracy: 0.9138 - val_loss: 0.1967 - val_accuracy: 0.9130\n",
            "Epoch 375/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1947 - accuracy: 0.9123 - val_loss: 0.1932 - val_accuracy: 0.9139\n",
            "Epoch 376/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1956 - accuracy: 0.9111 - val_loss: 0.1904 - val_accuracy: 0.9136\n",
            "Epoch 377/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9157 - val_loss: 0.1873 - val_accuracy: 0.9156\n",
            "Epoch 378/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1932 - accuracy: 0.9135 - val_loss: 0.1890 - val_accuracy: 0.9145\n",
            "Epoch 379/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1879 - accuracy: 0.9144 - val_loss: 0.1897 - val_accuracy: 0.9140\n",
            "Epoch 380/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1937 - accuracy: 0.9134 - val_loss: 0.1881 - val_accuracy: 0.9144\n",
            "Epoch 381/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1959 - accuracy: 0.9126 - val_loss: 0.1961 - val_accuracy: 0.9080\n",
            "Epoch 382/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1879 - accuracy: 0.9129 - val_loss: 0.1991 - val_accuracy: 0.9117\n",
            "Epoch 383/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1946 - accuracy: 0.9117 - val_loss: 0.1893 - val_accuracy: 0.9135\n",
            "Epoch 384/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9119 - val_loss: 0.1895 - val_accuracy: 0.9157\n",
            "Epoch 385/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1920 - accuracy: 0.9121 - val_loss: 0.1892 - val_accuracy: 0.9135\n",
            "Epoch 386/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1919 - accuracy: 0.9129 - val_loss: 0.1897 - val_accuracy: 0.9160\n",
            "Epoch 387/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1894 - accuracy: 0.9120 - val_loss: 0.1893 - val_accuracy: 0.9149\n",
            "Epoch 388/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1939 - accuracy: 0.9113 - val_loss: 0.1923 - val_accuracy: 0.9139\n",
            "Epoch 389/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9120 - val_loss: 0.1882 - val_accuracy: 0.9138\n",
            "Epoch 390/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9104 - val_loss: 0.1872 - val_accuracy: 0.9142\n",
            "Epoch 391/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1885 - accuracy: 0.9132 - val_loss: 0.1927 - val_accuracy: 0.9130\n",
            "Epoch 392/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1928 - accuracy: 0.9105 - val_loss: 0.1965 - val_accuracy: 0.9091\n",
            "Epoch 393/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1909 - accuracy: 0.9133 - val_loss: 0.1970 - val_accuracy: 0.9105\n",
            "Epoch 394/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1940 - accuracy: 0.9121 - val_loss: 0.1952 - val_accuracy: 0.9114\n",
            "Epoch 395/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9116 - val_loss: 0.1894 - val_accuracy: 0.9135\n",
            "Epoch 396/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1918 - accuracy: 0.9118 - val_loss: 0.1918 - val_accuracy: 0.9140\n",
            "Epoch 397/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1893 - accuracy: 0.9145 - val_loss: 0.1912 - val_accuracy: 0.9140\n",
            "Epoch 398/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9101 - val_loss: 0.1947 - val_accuracy: 0.9135\n",
            "Epoch 399/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1957 - accuracy: 0.9113 - val_loss: 0.1910 - val_accuracy: 0.9135\n",
            "Epoch 400/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1947 - accuracy: 0.9105 - val_loss: 0.1875 - val_accuracy: 0.9141\n",
            "Epoch 401/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1934 - accuracy: 0.9112 - val_loss: 0.1897 - val_accuracy: 0.9142\n",
            "Epoch 402/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9109 - val_loss: 0.1902 - val_accuracy: 0.9130\n",
            "Epoch 403/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9132 - val_loss: 0.1911 - val_accuracy: 0.9122\n",
            "Epoch 404/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9112 - val_loss: 0.1935 - val_accuracy: 0.9147\n",
            "Epoch 405/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1889 - accuracy: 0.9142 - val_loss: 0.1923 - val_accuracy: 0.9142\n",
            "Epoch 406/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9126 - val_loss: 0.1930 - val_accuracy: 0.9118\n",
            "Epoch 407/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1945 - accuracy: 0.9099 - val_loss: 0.1881 - val_accuracy: 0.9151\n",
            "Epoch 408/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1905 - accuracy: 0.9122 - val_loss: 0.1886 - val_accuracy: 0.9163\n",
            "Epoch 409/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1904 - accuracy: 0.9132 - val_loss: 0.1994 - val_accuracy: 0.9110\n",
            "Epoch 410/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1968 - accuracy: 0.9116 - val_loss: 0.1874 - val_accuracy: 0.9145\n",
            "Epoch 411/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1959 - accuracy: 0.9100 - val_loss: 0.1906 - val_accuracy: 0.9126\n",
            "Epoch 412/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1922 - accuracy: 0.9114 - val_loss: 0.1947 - val_accuracy: 0.9131\n",
            "Epoch 413/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1932 - accuracy: 0.9124 - val_loss: 0.1883 - val_accuracy: 0.9148\n",
            "Epoch 414/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1908 - accuracy: 0.9126 - val_loss: 0.1897 - val_accuracy: 0.9139\n",
            "Epoch 415/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1940 - accuracy: 0.9123 - val_loss: 0.1979 - val_accuracy: 0.9074\n",
            "Epoch 416/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1922 - accuracy: 0.9115 - val_loss: 0.1887 - val_accuracy: 0.9135\n",
            "Epoch 417/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1928 - accuracy: 0.9102 - val_loss: 0.1913 - val_accuracy: 0.9128\n",
            "Epoch 418/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1962 - accuracy: 0.9104 - val_loss: 0.1879 - val_accuracy: 0.9152\n",
            "Epoch 419/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1880 - accuracy: 0.9129 - val_loss: 0.2010 - val_accuracy: 0.9077\n",
            "Epoch 420/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1899 - accuracy: 0.9135 - val_loss: 0.1894 - val_accuracy: 0.9135\n",
            "Epoch 421/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1919 - accuracy: 0.9115 - val_loss: 0.1871 - val_accuracy: 0.9155\n",
            "Epoch 422/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1882 - accuracy: 0.9112 - val_loss: 0.1880 - val_accuracy: 0.9140\n",
            "Epoch 423/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1884 - accuracy: 0.9139 - val_loss: 0.1892 - val_accuracy: 0.9144\n",
            "Epoch 424/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1868 - accuracy: 0.9158 - val_loss: 0.1880 - val_accuracy: 0.9131\n",
            "Epoch 425/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1915 - accuracy: 0.9121 - val_loss: 0.1929 - val_accuracy: 0.9154\n",
            "Epoch 426/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9134 - val_loss: 0.1893 - val_accuracy: 0.9143\n",
            "Epoch 427/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1933 - accuracy: 0.9109 - val_loss: 0.1889 - val_accuracy: 0.9145\n",
            "Epoch 428/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9134 - val_loss: 0.1921 - val_accuracy: 0.9122\n",
            "Epoch 429/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1899 - accuracy: 0.9112 - val_loss: 0.1881 - val_accuracy: 0.9136\n",
            "Epoch 430/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1944 - accuracy: 0.9124 - val_loss: 0.1875 - val_accuracy: 0.9148\n",
            "Epoch 431/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1894 - accuracy: 0.9136 - val_loss: 0.1892 - val_accuracy: 0.9135\n",
            "Epoch 432/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1883 - accuracy: 0.9128 - val_loss: 0.1927 - val_accuracy: 0.9139\n",
            "Epoch 433/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1890 - accuracy: 0.9121 - val_loss: 0.1894 - val_accuracy: 0.9135\n",
            "Epoch 434/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1883 - accuracy: 0.9143 - val_loss: 0.1970 - val_accuracy: 0.9113\n",
            "Epoch 435/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1909 - accuracy: 0.9108 - val_loss: 0.1915 - val_accuracy: 0.9121\n",
            "Epoch 436/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1938 - accuracy: 0.9104 - val_loss: 0.1914 - val_accuracy: 0.9133\n",
            "Epoch 437/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1931 - accuracy: 0.9120 - val_loss: 0.1895 - val_accuracy: 0.9134\n",
            "Epoch 438/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1930 - accuracy: 0.9121 - val_loss: 0.1897 - val_accuracy: 0.9143\n",
            "Epoch 439/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9128 - val_loss: 0.2034 - val_accuracy: 0.9041\n",
            "Epoch 440/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1909 - accuracy: 0.9131 - val_loss: 0.1877 - val_accuracy: 0.9135\n",
            "Epoch 441/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9144 - val_loss: 0.1938 - val_accuracy: 0.9134\n",
            "Epoch 442/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1951 - accuracy: 0.9116 - val_loss: 0.2134 - val_accuracy: 0.9059\n",
            "Epoch 443/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1920 - accuracy: 0.9135 - val_loss: 0.1879 - val_accuracy: 0.9142\n",
            "Epoch 444/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1913 - accuracy: 0.9128 - val_loss: 0.1878 - val_accuracy: 0.9151\n",
            "Epoch 445/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1931 - accuracy: 0.9106 - val_loss: 0.1862 - val_accuracy: 0.9144\n",
            "Epoch 446/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1920 - accuracy: 0.9109 - val_loss: 0.1898 - val_accuracy: 0.9138\n",
            "Epoch 447/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9135 - val_loss: 0.1922 - val_accuracy: 0.9131\n",
            "Epoch 448/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1926 - accuracy: 0.9115 - val_loss: 0.1868 - val_accuracy: 0.9150\n",
            "Epoch 449/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1895 - accuracy: 0.9144 - val_loss: 0.2005 - val_accuracy: 0.9066\n",
            "Epoch 450/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1884 - accuracy: 0.9148 - val_loss: 0.1922 - val_accuracy: 0.9111\n",
            "Epoch 451/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1883 - accuracy: 0.9138 - val_loss: 0.1887 - val_accuracy: 0.9140\n",
            "Epoch 452/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1885 - accuracy: 0.9144 - val_loss: 0.1894 - val_accuracy: 0.9136\n",
            "Epoch 453/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1939 - accuracy: 0.9103 - val_loss: 0.1892 - val_accuracy: 0.9143\n",
            "Epoch 454/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1906 - accuracy: 0.9123 - val_loss: 0.1916 - val_accuracy: 0.9135\n",
            "Epoch 455/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1899 - accuracy: 0.9141 - val_loss: 0.1902 - val_accuracy: 0.9135\n",
            "Epoch 456/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1913 - accuracy: 0.9093 - val_loss: 0.1876 - val_accuracy: 0.9143\n",
            "Epoch 457/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 0.1884 - val_accuracy: 0.9142\n",
            "Epoch 458/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1890 - accuracy: 0.9139 - val_loss: 0.1900 - val_accuracy: 0.9139\n",
            "Epoch 459/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1898 - accuracy: 0.9143 - val_loss: 0.1874 - val_accuracy: 0.9138\n",
            "Epoch 460/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1913 - accuracy: 0.9114 - val_loss: 0.1872 - val_accuracy: 0.9148\n",
            "Epoch 461/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1899 - accuracy: 0.9132 - val_loss: 0.1926 - val_accuracy: 0.9135\n",
            "Epoch 462/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1932 - accuracy: 0.9123 - val_loss: 0.1996 - val_accuracy: 0.9071\n",
            "Epoch 463/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1937 - accuracy: 0.9118 - val_loss: 0.1914 - val_accuracy: 0.9131\n",
            "Epoch 464/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1893 - accuracy: 0.9144 - val_loss: 0.1872 - val_accuracy: 0.9137\n",
            "Epoch 465/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1881 - accuracy: 0.9116 - val_loss: 0.1922 - val_accuracy: 0.9131\n",
            "Epoch 466/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1901 - accuracy: 0.9139 - val_loss: 0.1879 - val_accuracy: 0.9134\n",
            "Epoch 467/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1914 - accuracy: 0.9110 - val_loss: 0.1988 - val_accuracy: 0.9126\n",
            "Epoch 468/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1949 - accuracy: 0.9117 - val_loss: 0.1990 - val_accuracy: 0.9069\n",
            "Epoch 469/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1869 - accuracy: 0.9152 - val_loss: 0.1874 - val_accuracy: 0.9141\n",
            "Epoch 470/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1907 - accuracy: 0.9106 - val_loss: 0.1901 - val_accuracy: 0.9135\n",
            "Epoch 471/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9149 - val_loss: 0.1886 - val_accuracy: 0.9139\n",
            "Epoch 472/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9138 - val_loss: 0.1890 - val_accuracy: 0.9142\n",
            "Epoch 473/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1936 - accuracy: 0.9124 - val_loss: 0.1897 - val_accuracy: 0.9135\n",
            "Epoch 474/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1923 - accuracy: 0.9115 - val_loss: 0.1952 - val_accuracy: 0.9092\n",
            "Epoch 475/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1934 - accuracy: 0.9134 - val_loss: 0.1894 - val_accuracy: 0.9135\n",
            "Epoch 476/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1898 - accuracy: 0.9121 - val_loss: 0.1907 - val_accuracy: 0.9139\n",
            "Epoch 477/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1924 - accuracy: 0.9101 - val_loss: 0.1913 - val_accuracy: 0.9137\n",
            "Epoch 478/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1912 - accuracy: 0.9114 - val_loss: 0.1895 - val_accuracy: 0.9128\n",
            "Epoch 479/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1864 - accuracy: 0.9128 - val_loss: 0.1926 - val_accuracy: 0.9117\n",
            "Epoch 480/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1900 - accuracy: 0.9115 - val_loss: 0.1888 - val_accuracy: 0.9129\n",
            "Epoch 481/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1963 - accuracy: 0.9094 - val_loss: 0.1911 - val_accuracy: 0.9126\n",
            "Epoch 482/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1921 - accuracy: 0.9122 - val_loss: 0.1920 - val_accuracy: 0.9109\n",
            "Epoch 483/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1913 - accuracy: 0.9127 - val_loss: 0.1899 - val_accuracy: 0.9141\n",
            "Epoch 484/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1901 - accuracy: 0.9127 - val_loss: 0.1889 - val_accuracy: 0.9126\n",
            "Epoch 485/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1909 - accuracy: 0.9124 - val_loss: 0.1923 - val_accuracy: 0.9126\n",
            "Epoch 486/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1937 - accuracy: 0.9113 - val_loss: 0.1876 - val_accuracy: 0.9152\n",
            "Epoch 487/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1875 - accuracy: 0.9161 - val_loss: 0.1959 - val_accuracy: 0.9080\n",
            "Epoch 488/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1936 - accuracy: 0.9118 - val_loss: 0.1909 - val_accuracy: 0.9129\n",
            "Epoch 489/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1899 - accuracy: 0.9126 - val_loss: 0.1873 - val_accuracy: 0.9138\n",
            "Epoch 490/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1929 - accuracy: 0.9122 - val_loss: 0.1888 - val_accuracy: 0.9129\n",
            "Epoch 491/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1866 - accuracy: 0.9145 - val_loss: 0.1898 - val_accuracy: 0.9141\n",
            "Epoch 492/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1868 - accuracy: 0.9159 - val_loss: 0.1893 - val_accuracy: 0.9139\n",
            "Epoch 493/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1848 - accuracy: 0.9148 - val_loss: 0.1929 - val_accuracy: 0.9147\n",
            "Epoch 494/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9155 - val_loss: 0.1875 - val_accuracy: 0.9143\n",
            "Epoch 495/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1974 - accuracy: 0.9106 - val_loss: 0.1897 - val_accuracy: 0.9131\n",
            "Epoch 496/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1855 - accuracy: 0.9127 - val_loss: 0.1900 - val_accuracy: 0.9142\n",
            "Epoch 497/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1899 - accuracy: 0.9135 - val_loss: 0.1894 - val_accuracy: 0.9131\n",
            "Epoch 498/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1910 - accuracy: 0.9131 - val_loss: 0.1926 - val_accuracy: 0.9135\n",
            "Epoch 499/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9130 - val_loss: 0.1916 - val_accuracy: 0.9125\n",
            "Epoch 500/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1871 - accuracy: 0.9148 - val_loss: 0.1885 - val_accuracy: 0.9134\n",
            "Epoch 501/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9150 - val_loss: 0.1882 - val_accuracy: 0.9141\n",
            "Epoch 502/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9162 - val_loss: 0.2091 - val_accuracy: 0.9039\n",
            "Epoch 503/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1936 - accuracy: 0.9089 - val_loss: 0.1911 - val_accuracy: 0.9117\n",
            "Epoch 504/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9117 - val_loss: 0.1877 - val_accuracy: 0.9153\n",
            "Epoch 505/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1836 - accuracy: 0.9166 - val_loss: 0.1899 - val_accuracy: 0.9148\n",
            "Epoch 506/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1865 - accuracy: 0.9141 - val_loss: 0.1895 - val_accuracy: 0.9142\n",
            "Epoch 507/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9142 - val_loss: 0.1868 - val_accuracy: 0.9131\n",
            "Epoch 508/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1901 - accuracy: 0.9138 - val_loss: 0.1885 - val_accuracy: 0.9127\n",
            "Epoch 509/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1906 - accuracy: 0.9119 - val_loss: 0.1919 - val_accuracy: 0.9126\n",
            "Epoch 510/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1926 - accuracy: 0.9095 - val_loss: 0.1875 - val_accuracy: 0.9129\n",
            "Epoch 511/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1906 - accuracy: 0.9136 - val_loss: 0.1917 - val_accuracy: 0.9126\n",
            "Epoch 512/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1897 - accuracy: 0.9126 - val_loss: 0.1879 - val_accuracy: 0.9148\n",
            "Epoch 513/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9116 - val_loss: 0.1939 - val_accuracy: 0.9134\n",
            "Epoch 514/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1860 - accuracy: 0.9145 - val_loss: 0.1902 - val_accuracy: 0.9122\n",
            "Epoch 515/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1931 - accuracy: 0.9101 - val_loss: 0.1879 - val_accuracy: 0.9122\n",
            "Epoch 516/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9126 - val_loss: 0.2027 - val_accuracy: 0.9102\n",
            "Epoch 517/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1898 - accuracy: 0.9114 - val_loss: 0.1881 - val_accuracy: 0.9143\n",
            "Epoch 518/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1857 - accuracy: 0.9150 - val_loss: 0.1880 - val_accuracy: 0.9149\n",
            "Epoch 519/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1862 - accuracy: 0.9151 - val_loss: 0.1878 - val_accuracy: 0.9134\n",
            "Epoch 520/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1876 - accuracy: 0.9139 - val_loss: 0.1870 - val_accuracy: 0.9129\n",
            "Epoch 521/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1884 - accuracy: 0.9146 - val_loss: 0.1888 - val_accuracy: 0.9135\n",
            "Epoch 522/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1894 - accuracy: 0.9126 - val_loss: 0.1895 - val_accuracy: 0.9132\n",
            "Epoch 523/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1886 - accuracy: 0.9153 - val_loss: 0.2024 - val_accuracy: 0.9127\n",
            "Epoch 524/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1894 - accuracy: 0.9123 - val_loss: 0.1879 - val_accuracy: 0.9143\n",
            "Epoch 525/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1916 - accuracy: 0.9116 - val_loss: 0.1870 - val_accuracy: 0.9156\n",
            "Epoch 526/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1869 - accuracy: 0.9141 - val_loss: 0.1896 - val_accuracy: 0.9137\n",
            "Epoch 527/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1933 - accuracy: 0.9120 - val_loss: 0.1906 - val_accuracy: 0.9140\n",
            "Epoch 528/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1904 - accuracy: 0.9147 - val_loss: 0.1884 - val_accuracy: 0.9127\n",
            "Epoch 529/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9137 - val_loss: 0.1889 - val_accuracy: 0.9127\n",
            "Epoch 530/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1905 - accuracy: 0.9117 - val_loss: 0.1917 - val_accuracy: 0.9104\n",
            "Epoch 531/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1887 - accuracy: 0.9127 - val_loss: 0.1881 - val_accuracy: 0.9133\n",
            "Epoch 532/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1917 - accuracy: 0.9113 - val_loss: 0.1904 - val_accuracy: 0.9135\n",
            "Epoch 533/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1873 - accuracy: 0.9141 - val_loss: 0.1884 - val_accuracy: 0.9152\n",
            "Epoch 534/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1881 - accuracy: 0.9155 - val_loss: 0.1901 - val_accuracy: 0.9142\n",
            "Epoch 535/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1868 - accuracy: 0.9130 - val_loss: 0.1865 - val_accuracy: 0.9152\n",
            "Epoch 536/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1887 - accuracy: 0.9118 - val_loss: 0.1875 - val_accuracy: 0.9136\n",
            "Epoch 537/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1882 - accuracy: 0.9139 - val_loss: 0.1899 - val_accuracy: 0.9128\n",
            "Epoch 538/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1862 - accuracy: 0.9149 - val_loss: 0.1880 - val_accuracy: 0.9143\n",
            "Epoch 539/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1909 - accuracy: 0.9130 - val_loss: 0.1890 - val_accuracy: 0.9146\n",
            "Epoch 540/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1846 - accuracy: 0.9154 - val_loss: 0.1894 - val_accuracy: 0.9139\n",
            "Epoch 541/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1898 - accuracy: 0.9120 - val_loss: 0.1898 - val_accuracy: 0.9119\n",
            "Epoch 542/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1882 - accuracy: 0.9145 - val_loss: 0.1864 - val_accuracy: 0.9148\n",
            "Epoch 543/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9142 - val_loss: 0.1899 - val_accuracy: 0.9133\n",
            "Epoch 544/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1910 - accuracy: 0.9125 - val_loss: 0.1878 - val_accuracy: 0.9128\n",
            "Epoch 545/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1846 - accuracy: 0.9129 - val_loss: 0.1944 - val_accuracy: 0.9102\n",
            "Epoch 546/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9134 - val_loss: 0.1883 - val_accuracy: 0.9148\n",
            "Epoch 547/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1886 - accuracy: 0.9130 - val_loss: 0.1864 - val_accuracy: 0.9149\n",
            "Epoch 548/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1896 - accuracy: 0.9122 - val_loss: 0.1878 - val_accuracy: 0.9147\n",
            "Epoch 549/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1861 - accuracy: 0.9154 - val_loss: 0.1867 - val_accuracy: 0.9155\n",
            "Epoch 550/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9148 - val_loss: 0.1918 - val_accuracy: 0.9138\n",
            "Epoch 551/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9122 - val_loss: 0.1876 - val_accuracy: 0.9131\n",
            "Epoch 552/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9113 - val_loss: 0.1881 - val_accuracy: 0.9144\n",
            "Epoch 553/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1894 - accuracy: 0.9132 - val_loss: 0.1898 - val_accuracy: 0.9139\n",
            "Epoch 554/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1893 - accuracy: 0.9135 - val_loss: 0.1903 - val_accuracy: 0.9132\n",
            "Epoch 555/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1966 - accuracy: 0.9090 - val_loss: 0.1904 - val_accuracy: 0.9128\n",
            "Epoch 556/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9151 - val_loss: 0.1906 - val_accuracy: 0.9141\n",
            "Epoch 557/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1868 - accuracy: 0.9129 - val_loss: 0.1887 - val_accuracy: 0.9142\n",
            "Epoch 558/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1880 - accuracy: 0.9144 - val_loss: 0.1913 - val_accuracy: 0.9128\n",
            "Epoch 559/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1863 - accuracy: 0.9122 - val_loss: 0.1927 - val_accuracy: 0.9131\n",
            "Epoch 560/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1917 - accuracy: 0.9098 - val_loss: 0.1878 - val_accuracy: 0.9148\n",
            "Epoch 561/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1880 - accuracy: 0.9143 - val_loss: 0.1880 - val_accuracy: 0.9133\n",
            "Epoch 562/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1944 - accuracy: 0.9104 - val_loss: 0.1895 - val_accuracy: 0.9143\n",
            "Epoch 563/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1879 - accuracy: 0.9153 - val_loss: 0.1884 - val_accuracy: 0.9151\n",
            "Epoch 564/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1872 - accuracy: 0.9117 - val_loss: 0.1878 - val_accuracy: 0.9152\n",
            "Epoch 565/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1930 - accuracy: 0.9098 - val_loss: 0.1872 - val_accuracy: 0.9149\n",
            "Epoch 566/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1871 - accuracy: 0.9141 - val_loss: 0.1904 - val_accuracy: 0.9129\n",
            "Epoch 567/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1883 - accuracy: 0.9119 - val_loss: 0.1927 - val_accuracy: 0.9137\n",
            "Epoch 568/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1884 - accuracy: 0.9130 - val_loss: 0.1874 - val_accuracy: 0.9150\n",
            "Epoch 569/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1840 - accuracy: 0.9181 - val_loss: 0.1883 - val_accuracy: 0.9133\n",
            "Epoch 570/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1871 - accuracy: 0.9156 - val_loss: 0.1864 - val_accuracy: 0.9143\n",
            "Epoch 571/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1840 - accuracy: 0.9149 - val_loss: 0.1885 - val_accuracy: 0.9133\n",
            "Epoch 572/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1893 - accuracy: 0.9122 - val_loss: 0.2021 - val_accuracy: 0.9112\n",
            "Epoch 573/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1888 - accuracy: 0.9130 - val_loss: 0.1864 - val_accuracy: 0.9142\n",
            "Epoch 574/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1893 - accuracy: 0.9129 - val_loss: 0.1862 - val_accuracy: 0.9154\n",
            "Epoch 575/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1935 - accuracy: 0.9112 - val_loss: 0.2017 - val_accuracy: 0.9047\n",
            "Epoch 576/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1869 - accuracy: 0.9148 - val_loss: 0.1881 - val_accuracy: 0.9143\n",
            "Epoch 577/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1861 - accuracy: 0.9125 - val_loss: 0.2033 - val_accuracy: 0.9038\n",
            "Epoch 578/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1929 - accuracy: 0.9122 - val_loss: 0.1866 - val_accuracy: 0.9148\n",
            "Epoch 579/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1898 - accuracy: 0.9117 - val_loss: 0.1877 - val_accuracy: 0.9142\n",
            "Epoch 580/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1890 - accuracy: 0.9153 - val_loss: 0.1864 - val_accuracy: 0.9156\n",
            "Epoch 581/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1913 - accuracy: 0.9117 - val_loss: 0.1979 - val_accuracy: 0.9080\n",
            "Epoch 582/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1918 - accuracy: 0.9115 - val_loss: 0.1871 - val_accuracy: 0.9139\n",
            "Epoch 583/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1908 - accuracy: 0.9121 - val_loss: 0.1888 - val_accuracy: 0.9135\n",
            "Epoch 584/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1880 - accuracy: 0.9148 - val_loss: 0.1884 - val_accuracy: 0.9146\n",
            "Epoch 585/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1902 - accuracy: 0.9133 - val_loss: 0.1869 - val_accuracy: 0.9124\n",
            "Epoch 586/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1897 - accuracy: 0.9147 - val_loss: 0.1879 - val_accuracy: 0.9135\n",
            "Epoch 587/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1866 - accuracy: 0.9157 - val_loss: 0.1911 - val_accuracy: 0.9140\n",
            "Epoch 588/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1899 - accuracy: 0.9131 - val_loss: 0.1860 - val_accuracy: 0.9144\n",
            "Epoch 589/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1891 - accuracy: 0.9138 - val_loss: 0.1878 - val_accuracy: 0.9139\n",
            "Epoch 590/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1847 - accuracy: 0.9165 - val_loss: 0.1878 - val_accuracy: 0.9146\n",
            "Epoch 591/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1885 - accuracy: 0.9124 - val_loss: 0.1877 - val_accuracy: 0.9148\n",
            "Epoch 592/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1887 - accuracy: 0.9139 - val_loss: 0.1888 - val_accuracy: 0.9139\n",
            "Epoch 593/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1882 - accuracy: 0.9128 - val_loss: 0.1925 - val_accuracy: 0.9138\n",
            "Epoch 594/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1890 - accuracy: 0.9132 - val_loss: 0.1887 - val_accuracy: 0.9124\n",
            "Epoch 595/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1919 - accuracy: 0.9094 - val_loss: 0.1893 - val_accuracy: 0.9142\n",
            "Epoch 596/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1851 - accuracy: 0.9159 - val_loss: 0.1980 - val_accuracy: 0.9125\n",
            "Epoch 597/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1920 - accuracy: 0.9115 - val_loss: 0.1972 - val_accuracy: 0.9109\n",
            "Epoch 598/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1870 - accuracy: 0.9136 - val_loss: 0.1864 - val_accuracy: 0.9131\n",
            "Epoch 599/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1939 - accuracy: 0.9112 - val_loss: 0.1860 - val_accuracy: 0.9157\n",
            "Epoch 600/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1909 - accuracy: 0.9121 - val_loss: 0.1867 - val_accuracy: 0.9145\n",
            "Epoch 601/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1865 - accuracy: 0.9141 - val_loss: 0.1915 - val_accuracy: 0.9139\n",
            "Epoch 602/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1897 - accuracy: 0.9132 - val_loss: 0.1870 - val_accuracy: 0.9145\n",
            "Epoch 603/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9133 - val_loss: 0.1937 - val_accuracy: 0.9133\n",
            "Epoch 604/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1936 - accuracy: 0.9087 - val_loss: 0.2094 - val_accuracy: 0.9110\n",
            "Epoch 605/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1914 - accuracy: 0.9121 - val_loss: 0.1869 - val_accuracy: 0.9144\n",
            "Epoch 606/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1904 - accuracy: 0.9135 - val_loss: 0.1935 - val_accuracy: 0.9123\n",
            "Epoch 607/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1880 - accuracy: 0.9133 - val_loss: 0.1908 - val_accuracy: 0.9132\n",
            "Epoch 608/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1887 - accuracy: 0.9112 - val_loss: 0.1912 - val_accuracy: 0.9126\n",
            "Epoch 609/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1887 - accuracy: 0.9126 - val_loss: 0.1908 - val_accuracy: 0.9120\n",
            "Epoch 610/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1901 - accuracy: 0.9133 - val_loss: 0.1887 - val_accuracy: 0.9126\n",
            "Epoch 611/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1940 - accuracy: 0.9096 - val_loss: 0.1911 - val_accuracy: 0.9116\n",
            "Epoch 612/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1924 - accuracy: 0.9099 - val_loss: 0.1893 - val_accuracy: 0.9143\n",
            "Epoch 613/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1860 - accuracy: 0.9150 - val_loss: 0.1862 - val_accuracy: 0.9144\n",
            "Epoch 614/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1840 - accuracy: 0.9159 - val_loss: 0.1930 - val_accuracy: 0.9144\n",
            "Epoch 615/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1857 - accuracy: 0.9139 - val_loss: 0.1866 - val_accuracy: 0.9139\n",
            "Epoch 616/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1870 - accuracy: 0.9117 - val_loss: 0.1871 - val_accuracy: 0.9152\n",
            "Epoch 617/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1921 - accuracy: 0.9110 - val_loss: 0.1884 - val_accuracy: 0.9150\n",
            "Epoch 618/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9135 - val_loss: 0.1895 - val_accuracy: 0.9142\n",
            "Epoch 619/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1936 - accuracy: 0.9119 - val_loss: 0.1957 - val_accuracy: 0.9114\n",
            "Epoch 620/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1965 - accuracy: 0.9093 - val_loss: 0.1886 - val_accuracy: 0.9142\n",
            "Epoch 621/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1867 - accuracy: 0.9137 - val_loss: 0.1876 - val_accuracy: 0.9147\n",
            "Epoch 622/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1896 - accuracy: 0.9117 - val_loss: 0.1872 - val_accuracy: 0.9142\n",
            "Epoch 623/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1892 - accuracy: 0.9126 - val_loss: 0.1879 - val_accuracy: 0.9124\n",
            "Epoch 624/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1888 - accuracy: 0.9137 - val_loss: 0.1879 - val_accuracy: 0.9152\n",
            "Epoch 625/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1867 - accuracy: 0.9142 - val_loss: 0.1862 - val_accuracy: 0.9142\n",
            "Epoch 626/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1841 - accuracy: 0.9150 - val_loss: 0.1889 - val_accuracy: 0.9139\n",
            "Epoch 627/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1876 - accuracy: 0.9118 - val_loss: 0.1911 - val_accuracy: 0.9116\n",
            "Epoch 628/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1888 - accuracy: 0.9139 - val_loss: 0.1940 - val_accuracy: 0.9126\n",
            "Epoch 629/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1937 - accuracy: 0.9109 - val_loss: 0.1857 - val_accuracy: 0.9144\n",
            "Epoch 630/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1891 - accuracy: 0.9124 - val_loss: 0.1872 - val_accuracy: 0.9145\n",
            "Epoch 631/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1902 - accuracy: 0.9116 - val_loss: 0.1864 - val_accuracy: 0.9150\n",
            "Epoch 632/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1866 - accuracy: 0.9149 - val_loss: 0.1908 - val_accuracy: 0.9139\n",
            "Epoch 633/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1845 - accuracy: 0.9145 - val_loss: 0.1884 - val_accuracy: 0.9148\n",
            "Epoch 634/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1896 - accuracy: 0.9116 - val_loss: 0.1872 - val_accuracy: 0.9143\n",
            "Epoch 635/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1838 - accuracy: 0.9155 - val_loss: 0.1873 - val_accuracy: 0.9149\n",
            "Epoch 636/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1915 - accuracy: 0.9126 - val_loss: 0.1854 - val_accuracy: 0.9147\n",
            "Epoch 637/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1900 - accuracy: 0.9122 - val_loss: 0.1904 - val_accuracy: 0.9126\n",
            "Epoch 638/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1934 - accuracy: 0.9112 - val_loss: 0.1910 - val_accuracy: 0.9153\n",
            "Epoch 639/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1922 - accuracy: 0.9133 - val_loss: 0.1919 - val_accuracy: 0.9127\n",
            "Epoch 640/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1869 - accuracy: 0.9133 - val_loss: 0.1935 - val_accuracy: 0.9108\n",
            "Epoch 641/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1886 - accuracy: 0.9132 - val_loss: 0.1870 - val_accuracy: 0.9137\n",
            "Epoch 642/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1917 - accuracy: 0.9100 - val_loss: 0.1879 - val_accuracy: 0.9139\n",
            "Epoch 643/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1846 - accuracy: 0.9173 - val_loss: 0.1891 - val_accuracy: 0.9144\n",
            "Epoch 644/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1882 - accuracy: 0.9147 - val_loss: 0.1853 - val_accuracy: 0.9153\n",
            "Epoch 645/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1889 - accuracy: 0.9115 - val_loss: 0.1927 - val_accuracy: 0.9135\n",
            "Epoch 646/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1888 - accuracy: 0.9140 - val_loss: 0.1892 - val_accuracy: 0.9135\n",
            "Epoch 647/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1908 - accuracy: 0.9098 - val_loss: 0.1934 - val_accuracy: 0.9135\n",
            "Epoch 648/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1869 - accuracy: 0.9121 - val_loss: 0.1857 - val_accuracy: 0.9149\n",
            "Epoch 649/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1877 - accuracy: 0.9131 - val_loss: 0.1889 - val_accuracy: 0.9148\n",
            "Epoch 650/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1864 - accuracy: 0.9113 - val_loss: 0.1876 - val_accuracy: 0.9148\n",
            "Epoch 651/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1849 - accuracy: 0.9140 - val_loss: 0.1876 - val_accuracy: 0.9135\n",
            "Epoch 652/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1817 - accuracy: 0.9176 - val_loss: 0.1884 - val_accuracy: 0.9144\n",
            "Epoch 653/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1916 - accuracy: 0.9121 - val_loss: 0.1887 - val_accuracy: 0.9123\n",
            "Epoch 654/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1862 - accuracy: 0.9150 - val_loss: 0.1854 - val_accuracy: 0.9133\n",
            "Epoch 655/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1878 - accuracy: 0.9123 - val_loss: 0.1894 - val_accuracy: 0.9149\n",
            "Epoch 656/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1885 - accuracy: 0.9128 - val_loss: 0.1929 - val_accuracy: 0.9103\n",
            "Epoch 657/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1951 - accuracy: 0.9103 - val_loss: 0.1875 - val_accuracy: 0.9151\n",
            "Epoch 658/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1928 - accuracy: 0.9109 - val_loss: 0.1867 - val_accuracy: 0.9142\n",
            "Epoch 659/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1854 - accuracy: 0.9150 - val_loss: 0.1868 - val_accuracy: 0.9140\n",
            "Epoch 660/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1850 - accuracy: 0.9159 - val_loss: 0.1901 - val_accuracy: 0.9140\n",
            "Epoch 661/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1876 - accuracy: 0.9116 - val_loss: 0.1904 - val_accuracy: 0.9151\n",
            "Epoch 662/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1851 - accuracy: 0.9131 - val_loss: 0.1885 - val_accuracy: 0.9133\n",
            "Epoch 663/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1887 - accuracy: 0.9145 - val_loss: 0.1880 - val_accuracy: 0.9131\n",
            "Epoch 664/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1941 - accuracy: 0.9120 - val_loss: 0.1871 - val_accuracy: 0.9126\n",
            "Epoch 665/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1856 - accuracy: 0.9139 - val_loss: 0.1877 - val_accuracy: 0.9128\n",
            "Epoch 666/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1918 - accuracy: 0.9129 - val_loss: 0.1884 - val_accuracy: 0.9143\n",
            "Epoch 667/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1871 - accuracy: 0.9137 - val_loss: 0.1921 - val_accuracy: 0.9108\n",
            "Epoch 668/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1909 - accuracy: 0.9121 - val_loss: 0.1943 - val_accuracy: 0.9125\n",
            "Epoch 669/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1877 - accuracy: 0.9143 - val_loss: 0.1861 - val_accuracy: 0.9150\n",
            "Epoch 670/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1855 - accuracy: 0.9139 - val_loss: 0.1888 - val_accuracy: 0.9127\n",
            "Epoch 671/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1908 - accuracy: 0.9122 - val_loss: 0.1916 - val_accuracy: 0.9139\n",
            "Epoch 672/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1929 - accuracy: 0.9102 - val_loss: 0.1913 - val_accuracy: 0.9124\n",
            "Epoch 673/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1870 - accuracy: 0.9130 - val_loss: 0.2003 - val_accuracy: 0.9088\n",
            "Epoch 674/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9121 - val_loss: 0.1931 - val_accuracy: 0.9119\n",
            "Epoch 675/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1849 - accuracy: 0.9156 - val_loss: 0.1910 - val_accuracy: 0.9136\n",
            "Epoch 676/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1857 - accuracy: 0.9134 - val_loss: 0.1871 - val_accuracy: 0.9155\n",
            "Epoch 677/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1886 - accuracy: 0.9131 - val_loss: 0.1887 - val_accuracy: 0.9152\n",
            "Epoch 678/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1909 - accuracy: 0.9123 - val_loss: 0.1870 - val_accuracy: 0.9133\n",
            "Epoch 679/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9120 - val_loss: 0.1866 - val_accuracy: 0.9129\n",
            "Epoch 680/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1927 - accuracy: 0.9115 - val_loss: 0.1859 - val_accuracy: 0.9150\n",
            "Epoch 681/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9127 - val_loss: 0.1897 - val_accuracy: 0.9143\n",
            "Epoch 682/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1860 - accuracy: 0.9141 - val_loss: 0.1882 - val_accuracy: 0.9144\n",
            "Epoch 683/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1915 - accuracy: 0.9133 - val_loss: 0.1887 - val_accuracy: 0.9149\n",
            "Epoch 684/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1854 - accuracy: 0.9153 - val_loss: 0.1901 - val_accuracy: 0.9135\n",
            "Epoch 685/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1882 - accuracy: 0.9113 - val_loss: 0.1871 - val_accuracy: 0.9142\n",
            "Epoch 686/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1885 - accuracy: 0.9129 - val_loss: 0.1849 - val_accuracy: 0.9147\n",
            "Epoch 687/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9128 - val_loss: 0.1901 - val_accuracy: 0.9126\n",
            "Epoch 688/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1820 - accuracy: 0.9159 - val_loss: 0.1869 - val_accuracy: 0.9126\n",
            "Epoch 689/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1894 - accuracy: 0.9130 - val_loss: 0.1867 - val_accuracy: 0.9138\n",
            "Epoch 690/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1881 - accuracy: 0.9151 - val_loss: 0.1868 - val_accuracy: 0.9157\n",
            "Epoch 691/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1881 - accuracy: 0.9126 - val_loss: 0.1876 - val_accuracy: 0.9130\n",
            "Epoch 692/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1846 - accuracy: 0.9158 - val_loss: 0.1859 - val_accuracy: 0.9136\n",
            "Epoch 693/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1862 - accuracy: 0.9162 - val_loss: 0.1912 - val_accuracy: 0.9104\n",
            "Epoch 694/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1926 - accuracy: 0.9122 - val_loss: 0.1876 - val_accuracy: 0.9140\n",
            "Epoch 695/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1952 - accuracy: 0.9106 - val_loss: 0.1866 - val_accuracy: 0.9131\n",
            "Epoch 696/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1856 - accuracy: 0.9135 - val_loss: 0.1880 - val_accuracy: 0.9122\n",
            "Epoch 697/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1864 - accuracy: 0.9159 - val_loss: 0.1861 - val_accuracy: 0.9137\n",
            "Epoch 698/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1902 - accuracy: 0.9135 - val_loss: 0.1884 - val_accuracy: 0.9139\n",
            "Epoch 699/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1882 - accuracy: 0.9120 - val_loss: 0.1871 - val_accuracy: 0.9138\n",
            "Epoch 700/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1828 - accuracy: 0.9159 - val_loss: 0.1898 - val_accuracy: 0.9140\n",
            "Epoch 701/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1898 - accuracy: 0.9117 - val_loss: 0.1867 - val_accuracy: 0.9129\n",
            "Epoch 702/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1871 - accuracy: 0.9141 - val_loss: 0.1890 - val_accuracy: 0.9147\n",
            "Epoch 703/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1931 - accuracy: 0.9090 - val_loss: 0.1881 - val_accuracy: 0.9127\n",
            "Epoch 704/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1893 - accuracy: 0.9139 - val_loss: 0.1850 - val_accuracy: 0.9143\n",
            "Epoch 705/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1922 - accuracy: 0.9122 - val_loss: 0.1910 - val_accuracy: 0.9130\n",
            "Epoch 706/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1855 - accuracy: 0.9167 - val_loss: 0.1854 - val_accuracy: 0.9133\n",
            "Epoch 707/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1883 - accuracy: 0.9142 - val_loss: 0.1908 - val_accuracy: 0.9131\n",
            "Epoch 708/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1896 - accuracy: 0.9117 - val_loss: 0.1872 - val_accuracy: 0.9148\n",
            "Epoch 709/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1893 - accuracy: 0.9109 - val_loss: 0.1880 - val_accuracy: 0.9138\n",
            "Epoch 710/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1889 - accuracy: 0.9138 - val_loss: 0.1909 - val_accuracy: 0.9101\n",
            "Epoch 711/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1881 - accuracy: 0.9125 - val_loss: 0.1910 - val_accuracy: 0.9114\n",
            "Epoch 712/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1868 - accuracy: 0.9129 - val_loss: 0.1895 - val_accuracy: 0.9141\n",
            "Epoch 713/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1892 - accuracy: 0.9132 - val_loss: 0.1913 - val_accuracy: 0.9130\n",
            "Epoch 714/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1906 - accuracy: 0.9142 - val_loss: 0.1890 - val_accuracy: 0.9135\n",
            "Epoch 715/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9124 - val_loss: 0.1899 - val_accuracy: 0.9134\n",
            "Epoch 716/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1881 - accuracy: 0.9133 - val_loss: 0.1876 - val_accuracy: 0.9135\n",
            "Epoch 717/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1905 - accuracy: 0.9134 - val_loss: 0.1885 - val_accuracy: 0.9151\n",
            "Epoch 718/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1940 - accuracy: 0.9088 - val_loss: 0.1874 - val_accuracy: 0.9148\n",
            "Epoch 719/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1844 - accuracy: 0.9149 - val_loss: 0.1876 - val_accuracy: 0.9146\n",
            "Epoch 720/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1882 - accuracy: 0.9115 - val_loss: 0.1871 - val_accuracy: 0.9152\n",
            "Epoch 721/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1850 - accuracy: 0.9154 - val_loss: 0.1875 - val_accuracy: 0.9135\n",
            "Epoch 722/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1835 - accuracy: 0.9158 - val_loss: 0.1886 - val_accuracy: 0.9145\n",
            "Epoch 723/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9131 - val_loss: 0.1864 - val_accuracy: 0.9131\n",
            "Epoch 724/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1863 - accuracy: 0.9150 - val_loss: 0.1922 - val_accuracy: 0.9114\n",
            "Epoch 725/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1856 - accuracy: 0.9160 - val_loss: 0.1860 - val_accuracy: 0.9147\n",
            "Epoch 726/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1836 - accuracy: 0.9150 - val_loss: 0.1866 - val_accuracy: 0.9143\n",
            "Epoch 727/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1842 - accuracy: 0.9132 - val_loss: 0.1879 - val_accuracy: 0.9139\n",
            "Epoch 728/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1863 - accuracy: 0.9141 - val_loss: 0.1921 - val_accuracy: 0.9117\n",
            "Epoch 729/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1916 - accuracy: 0.9106 - val_loss: 0.1857 - val_accuracy: 0.9140\n",
            "Epoch 730/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1887 - accuracy: 0.9127 - val_loss: 0.1896 - val_accuracy: 0.9131\n",
            "Epoch 731/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1884 - accuracy: 0.9123 - val_loss: 0.1898 - val_accuracy: 0.9112\n",
            "Epoch 732/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1808 - accuracy: 0.9178 - val_loss: 0.1863 - val_accuracy: 0.9144\n",
            "Epoch 733/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9148 - val_loss: 0.1951 - val_accuracy: 0.9118\n",
            "Epoch 734/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1908 - accuracy: 0.9122 - val_loss: 0.1932 - val_accuracy: 0.9097\n",
            "Epoch 735/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1844 - accuracy: 0.9149 - val_loss: 0.1867 - val_accuracy: 0.9144\n",
            "Epoch 736/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1897 - accuracy: 0.9111 - val_loss: 0.1904 - val_accuracy: 0.9146\n",
            "Epoch 737/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1895 - accuracy: 0.9130 - val_loss: 0.1852 - val_accuracy: 0.9141\n",
            "Epoch 738/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1906 - accuracy: 0.9120 - val_loss: 0.1882 - val_accuracy: 0.9156\n",
            "Epoch 739/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1883 - accuracy: 0.9125 - val_loss: 0.1925 - val_accuracy: 0.9131\n",
            "Epoch 740/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1806 - accuracy: 0.9174 - val_loss: 0.1895 - val_accuracy: 0.9131\n",
            "Epoch 741/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1898 - accuracy: 0.9118 - val_loss: 0.1866 - val_accuracy: 0.9148\n",
            "Epoch 742/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1851 - accuracy: 0.9148 - val_loss: 0.1876 - val_accuracy: 0.9135\n",
            "Epoch 743/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1861 - accuracy: 0.9148 - val_loss: 0.1885 - val_accuracy: 0.9125\n",
            "Epoch 744/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1897 - accuracy: 0.9136 - val_loss: 0.1869 - val_accuracy: 0.9145\n",
            "Epoch 745/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1850 - accuracy: 0.9146 - val_loss: 0.1928 - val_accuracy: 0.9122\n",
            "Epoch 746/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9114 - val_loss: 0.1891 - val_accuracy: 0.9129\n",
            "Epoch 747/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1897 - accuracy: 0.9139 - val_loss: 0.1886 - val_accuracy: 0.9124\n",
            "Epoch 748/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1888 - accuracy: 0.9119 - val_loss: 0.1854 - val_accuracy: 0.9143\n",
            "Epoch 749/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1876 - accuracy: 0.9139 - val_loss: 0.1879 - val_accuracy: 0.9143\n",
            "Epoch 750/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1882 - accuracy: 0.9100 - val_loss: 0.1938 - val_accuracy: 0.9132\n",
            "Epoch 751/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1894 - accuracy: 0.9106 - val_loss: 0.1856 - val_accuracy: 0.9143\n",
            "Epoch 752/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1830 - accuracy: 0.9176 - val_loss: 0.1905 - val_accuracy: 0.9149\n",
            "Epoch 753/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1895 - accuracy: 0.9128 - val_loss: 0.1915 - val_accuracy: 0.9140\n",
            "Epoch 754/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1865 - accuracy: 0.9142 - val_loss: 0.1853 - val_accuracy: 0.9142\n",
            "Epoch 755/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1870 - accuracy: 0.9138 - val_loss: 0.1872 - val_accuracy: 0.9140\n",
            "Epoch 756/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1856 - accuracy: 0.9168 - val_loss: 0.1942 - val_accuracy: 0.9126\n",
            "Epoch 757/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1819 - accuracy: 0.9150 - val_loss: 0.1875 - val_accuracy: 0.9139\n",
            "Epoch 758/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1796 - accuracy: 0.9175 - val_loss: 0.1916 - val_accuracy: 0.9135\n",
            "Epoch 759/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1924 - accuracy: 0.9102 - val_loss: 0.1902 - val_accuracy: 0.9140\n",
            "Epoch 760/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1929 - accuracy: 0.9110 - val_loss: 0.1860 - val_accuracy: 0.9155\n",
            "Epoch 761/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1870 - accuracy: 0.9124 - val_loss: 0.1867 - val_accuracy: 0.9135\n",
            "Epoch 762/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1886 - accuracy: 0.9120 - val_loss: 0.1876 - val_accuracy: 0.9122\n",
            "Epoch 763/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1817 - accuracy: 0.9164 - val_loss: 0.1873 - val_accuracy: 0.9141\n",
            "Epoch 764/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1875 - accuracy: 0.9134 - val_loss: 0.1864 - val_accuracy: 0.9150\n",
            "Epoch 765/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1884 - accuracy: 0.9135 - val_loss: 0.1888 - val_accuracy: 0.9145\n",
            "Epoch 766/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1871 - accuracy: 0.9118 - val_loss: 0.1897 - val_accuracy: 0.9145\n",
            "Epoch 767/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1835 - accuracy: 0.9151 - val_loss: 0.1865 - val_accuracy: 0.9151\n",
            "Epoch 768/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9127 - val_loss: 0.1873 - val_accuracy: 0.9137\n",
            "Epoch 769/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1866 - accuracy: 0.9139 - val_loss: 0.1874 - val_accuracy: 0.9147\n",
            "Epoch 770/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1877 - accuracy: 0.9140 - val_loss: 0.1927 - val_accuracy: 0.9121\n",
            "Epoch 771/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1856 - accuracy: 0.9166 - val_loss: 0.1879 - val_accuracy: 0.9154\n",
            "Epoch 772/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1863 - accuracy: 0.9152 - val_loss: 0.1893 - val_accuracy: 0.9142\n",
            "Epoch 773/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1842 - accuracy: 0.9141 - val_loss: 0.1971 - val_accuracy: 0.9135\n",
            "Epoch 774/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9135 - val_loss: 0.1871 - val_accuracy: 0.9152\n",
            "Epoch 775/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1888 - accuracy: 0.9134 - val_loss: 0.1861 - val_accuracy: 0.9152\n",
            "Epoch 776/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1842 - accuracy: 0.9177 - val_loss: 0.1926 - val_accuracy: 0.9123\n",
            "Epoch 777/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1890 - accuracy: 0.9117 - val_loss: 0.1880 - val_accuracy: 0.9121\n",
            "Epoch 778/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9145 - val_loss: 0.1904 - val_accuracy: 0.9119\n",
            "Epoch 779/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1894 - accuracy: 0.9113 - val_loss: 0.1891 - val_accuracy: 0.9129\n",
            "Epoch 780/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1889 - accuracy: 0.9117 - val_loss: 0.1896 - val_accuracy: 0.9159\n",
            "Epoch 781/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1917 - accuracy: 0.9115 - val_loss: 0.1887 - val_accuracy: 0.9124\n",
            "Epoch 782/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1845 - accuracy: 0.9149 - val_loss: 0.1879 - val_accuracy: 0.9126\n",
            "Epoch 783/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9151 - val_loss: 0.1860 - val_accuracy: 0.9150\n",
            "Epoch 784/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1868 - accuracy: 0.9130 - val_loss: 0.1871 - val_accuracy: 0.9130\n",
            "Epoch 785/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1871 - accuracy: 0.9129 - val_loss: 0.1859 - val_accuracy: 0.9132\n",
            "Epoch 786/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9132 - val_loss: 0.1940 - val_accuracy: 0.9109\n",
            "Epoch 787/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1838 - accuracy: 0.9134 - val_loss: 0.1863 - val_accuracy: 0.9128\n",
            "Epoch 788/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1862 - accuracy: 0.9141 - val_loss: 0.1910 - val_accuracy: 0.9126\n",
            "Epoch 789/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1842 - accuracy: 0.9152 - val_loss: 0.1863 - val_accuracy: 0.9138\n",
            "Epoch 790/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1857 - accuracy: 0.9145 - val_loss: 0.1894 - val_accuracy: 0.9115\n",
            "Epoch 791/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1895 - accuracy: 0.9147 - val_loss: 0.1851 - val_accuracy: 0.9134\n",
            "Epoch 792/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1856 - accuracy: 0.9138 - val_loss: 0.1983 - val_accuracy: 0.9057\n",
            "Epoch 793/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9125 - val_loss: 0.1879 - val_accuracy: 0.9143\n",
            "Epoch 794/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1856 - accuracy: 0.9138 - val_loss: 0.1852 - val_accuracy: 0.9152\n",
            "Epoch 795/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9136 - val_loss: 0.1877 - val_accuracy: 0.9142\n",
            "Epoch 796/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9128 - val_loss: 0.1877 - val_accuracy: 0.9142\n",
            "Epoch 797/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1815 - accuracy: 0.9177 - val_loss: 0.1891 - val_accuracy: 0.9133\n",
            "Epoch 798/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1840 - accuracy: 0.9165 - val_loss: 0.1864 - val_accuracy: 0.9130\n",
            "Epoch 799/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1854 - accuracy: 0.9132 - val_loss: 0.1895 - val_accuracy: 0.9122\n",
            "Epoch 800/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1876 - accuracy: 0.9123 - val_loss: 0.1852 - val_accuracy: 0.9157\n",
            "Epoch 801/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1860 - accuracy: 0.9140 - val_loss: 0.1953 - val_accuracy: 0.9095\n",
            "Epoch 802/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9119 - val_loss: 0.1854 - val_accuracy: 0.9146\n",
            "Epoch 803/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1857 - accuracy: 0.9142 - val_loss: 0.1927 - val_accuracy: 0.9116\n",
            "Epoch 804/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9136 - val_loss: 0.1902 - val_accuracy: 0.9127\n",
            "Epoch 805/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1847 - accuracy: 0.9149 - val_loss: 0.1929 - val_accuracy: 0.9124\n",
            "Epoch 806/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9150 - val_loss: 0.1867 - val_accuracy: 0.9133\n",
            "Epoch 807/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1834 - accuracy: 0.9134 - val_loss: 0.1848 - val_accuracy: 0.9143\n",
            "Epoch 808/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1910 - accuracy: 0.9126 - val_loss: 0.1933 - val_accuracy: 0.9102\n",
            "Epoch 809/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1924 - accuracy: 0.9112 - val_loss: 0.1857 - val_accuracy: 0.9142\n",
            "Epoch 810/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1862 - accuracy: 0.9126 - val_loss: 0.1880 - val_accuracy: 0.9151\n",
            "Epoch 811/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1884 - accuracy: 0.9137 - val_loss: 0.1859 - val_accuracy: 0.9134\n",
            "Epoch 812/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1834 - accuracy: 0.9153 - val_loss: 0.1895 - val_accuracy: 0.9153\n",
            "Epoch 813/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9112 - val_loss: 0.1850 - val_accuracy: 0.9148\n",
            "Epoch 814/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9140 - val_loss: 0.1866 - val_accuracy: 0.9118\n",
            "Epoch 815/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1812 - accuracy: 0.9166 - val_loss: 0.1857 - val_accuracy: 0.9140\n",
            "Epoch 816/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1832 - accuracy: 0.9159 - val_loss: 0.1880 - val_accuracy: 0.9139\n",
            "Epoch 817/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1823 - accuracy: 0.9166 - val_loss: 0.1888 - val_accuracy: 0.9137\n",
            "Epoch 818/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1841 - accuracy: 0.9126 - val_loss: 0.1873 - val_accuracy: 0.9137\n",
            "Epoch 819/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1875 - accuracy: 0.9129 - val_loss: 0.1884 - val_accuracy: 0.9131\n",
            "Epoch 820/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9134 - val_loss: 0.1860 - val_accuracy: 0.9137\n",
            "Epoch 821/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9134 - val_loss: 0.1876 - val_accuracy: 0.9132\n",
            "Epoch 822/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1858 - accuracy: 0.9128 - val_loss: 0.1876 - val_accuracy: 0.9154\n",
            "Epoch 823/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1830 - accuracy: 0.9147 - val_loss: 0.1868 - val_accuracy: 0.9140\n",
            "Epoch 824/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1835 - accuracy: 0.9154 - val_loss: 0.1907 - val_accuracy: 0.9153\n",
            "Epoch 825/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1915 - accuracy: 0.9114 - val_loss: 0.1898 - val_accuracy: 0.9139\n",
            "Epoch 826/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1869 - accuracy: 0.9152 - val_loss: 0.1897 - val_accuracy: 0.9095\n",
            "Epoch 827/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9128 - val_loss: 0.1897 - val_accuracy: 0.9139\n",
            "Epoch 828/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1849 - accuracy: 0.9142 - val_loss: 0.1895 - val_accuracy: 0.9141\n",
            "Epoch 829/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1817 - accuracy: 0.9136 - val_loss: 0.1876 - val_accuracy: 0.9130\n",
            "Epoch 830/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1860 - accuracy: 0.9137 - val_loss: 0.1899 - val_accuracy: 0.9146\n",
            "Epoch 831/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1876 - accuracy: 0.9155 - val_loss: 0.1859 - val_accuracy: 0.9158\n",
            "Epoch 832/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1821 - accuracy: 0.9157 - val_loss: 0.1850 - val_accuracy: 0.9152\n",
            "Epoch 833/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9127 - val_loss: 0.1868 - val_accuracy: 0.9143\n",
            "Epoch 834/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1802 - accuracy: 0.9199 - val_loss: 0.1906 - val_accuracy: 0.9146\n",
            "Epoch 835/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1889 - accuracy: 0.9126 - val_loss: 0.1866 - val_accuracy: 0.9118\n",
            "Epoch 836/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1864 - accuracy: 0.9125 - val_loss: 0.1911 - val_accuracy: 0.9127\n",
            "Epoch 837/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1888 - accuracy: 0.9146 - val_loss: 0.1889 - val_accuracy: 0.9133\n",
            "Epoch 838/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1878 - accuracy: 0.9142 - val_loss: 0.1871 - val_accuracy: 0.9137\n",
            "Epoch 839/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1843 - accuracy: 0.9127 - val_loss: 0.1913 - val_accuracy: 0.9128\n",
            "Epoch 840/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9135 - val_loss: 0.1957 - val_accuracy: 0.9102\n",
            "Epoch 841/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1852 - accuracy: 0.9140 - val_loss: 0.1897 - val_accuracy: 0.9151\n",
            "Epoch 842/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1893 - accuracy: 0.9105 - val_loss: 0.1876 - val_accuracy: 0.9132\n",
            "Epoch 843/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1889 - accuracy: 0.9127 - val_loss: 0.1874 - val_accuracy: 0.9147\n",
            "Epoch 844/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9141 - val_loss: 0.1895 - val_accuracy: 0.9132\n",
            "Epoch 845/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1858 - accuracy: 0.9126 - val_loss: 0.1864 - val_accuracy: 0.9131\n",
            "Epoch 846/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9142 - val_loss: 0.1859 - val_accuracy: 0.9154\n",
            "Epoch 847/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9128 - val_loss: 0.1878 - val_accuracy: 0.9143\n",
            "Epoch 848/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9124 - val_loss: 0.1857 - val_accuracy: 0.9139\n",
            "Epoch 849/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1887 - accuracy: 0.9130 - val_loss: 0.1863 - val_accuracy: 0.9147\n",
            "Epoch 850/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1873 - accuracy: 0.9124 - val_loss: 0.1868 - val_accuracy: 0.9135\n",
            "Epoch 851/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1889 - accuracy: 0.9129 - val_loss: 0.1901 - val_accuracy: 0.9137\n",
            "Epoch 852/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1857 - accuracy: 0.9152 - val_loss: 0.1871 - val_accuracy: 0.9149\n",
            "Epoch 853/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1843 - accuracy: 0.9161 - val_loss: 0.1903 - val_accuracy: 0.9132\n",
            "Epoch 854/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1920 - accuracy: 0.9114 - val_loss: 0.1880 - val_accuracy: 0.9134\n",
            "Epoch 855/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9110 - val_loss: 0.1942 - val_accuracy: 0.9128\n",
            "Epoch 856/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1884 - accuracy: 0.9118 - val_loss: 0.1869 - val_accuracy: 0.9137\n",
            "Epoch 857/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1914 - accuracy: 0.9111 - val_loss: 0.1859 - val_accuracy: 0.9122\n",
            "Epoch 858/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1809 - accuracy: 0.9147 - val_loss: 0.1864 - val_accuracy: 0.9141\n",
            "Epoch 859/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9121 - val_loss: 0.1905 - val_accuracy: 0.9149\n",
            "Epoch 860/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9118 - val_loss: 0.1874 - val_accuracy: 0.9144\n",
            "Epoch 861/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1863 - accuracy: 0.9152 - val_loss: 0.1898 - val_accuracy: 0.9145\n",
            "Epoch 862/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1865 - accuracy: 0.9147 - val_loss: 0.1913 - val_accuracy: 0.9114\n",
            "Epoch 863/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1901 - accuracy: 0.9127 - val_loss: 0.1866 - val_accuracy: 0.9115\n",
            "Epoch 864/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1867 - accuracy: 0.9134 - val_loss: 0.1861 - val_accuracy: 0.9145\n",
            "Epoch 865/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1873 - accuracy: 0.9143 - val_loss: 0.1857 - val_accuracy: 0.9133\n",
            "Epoch 866/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1861 - accuracy: 0.9146 - val_loss: 0.1868 - val_accuracy: 0.9146\n",
            "Epoch 867/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1828 - accuracy: 0.9139 - val_loss: 0.1861 - val_accuracy: 0.9137\n",
            "Epoch 868/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1863 - accuracy: 0.9144 - val_loss: 0.1867 - val_accuracy: 0.9132\n",
            "Epoch 869/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1840 - accuracy: 0.9149 - val_loss: 0.1881 - val_accuracy: 0.9126\n",
            "Epoch 870/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1824 - accuracy: 0.9179 - val_loss: 0.1879 - val_accuracy: 0.9144\n",
            "Epoch 871/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1829 - accuracy: 0.9135 - val_loss: 0.1875 - val_accuracy: 0.9125\n",
            "Epoch 872/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1894 - accuracy: 0.9127 - val_loss: 0.1871 - val_accuracy: 0.9139\n",
            "Epoch 873/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1875 - accuracy: 0.9130 - val_loss: 0.1880 - val_accuracy: 0.9142\n",
            "Epoch 874/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1884 - accuracy: 0.9133 - val_loss: 0.1964 - val_accuracy: 0.9101\n",
            "Epoch 875/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1886 - accuracy: 0.9111 - val_loss: 0.1896 - val_accuracy: 0.9141\n",
            "Epoch 876/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1838 - accuracy: 0.9169 - val_loss: 0.1887 - val_accuracy: 0.9144\n",
            "Epoch 877/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1862 - accuracy: 0.9121 - val_loss: 0.1866 - val_accuracy: 0.9139\n",
            "Epoch 878/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1891 - accuracy: 0.9129 - val_loss: 0.1872 - val_accuracy: 0.9143\n",
            "Epoch 879/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1891 - accuracy: 0.9126 - val_loss: 0.1855 - val_accuracy: 0.9149\n",
            "Epoch 880/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1879 - accuracy: 0.9141 - val_loss: 0.1853 - val_accuracy: 0.9160\n",
            "Epoch 881/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1860 - accuracy: 0.9145 - val_loss: 0.1867 - val_accuracy: 0.9138\n",
            "Epoch 882/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1873 - accuracy: 0.9123 - val_loss: 0.1887 - val_accuracy: 0.9133\n",
            "Epoch 883/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1872 - accuracy: 0.9124 - val_loss: 0.1874 - val_accuracy: 0.9148\n",
            "Epoch 884/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1842 - accuracy: 0.9148 - val_loss: 0.1888 - val_accuracy: 0.9128\n",
            "Epoch 885/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1878 - accuracy: 0.9114 - val_loss: 0.1902 - val_accuracy: 0.9126\n",
            "Epoch 886/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1872 - accuracy: 0.9124 - val_loss: 0.1868 - val_accuracy: 0.9138\n",
            "Epoch 887/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1842 - accuracy: 0.9138 - val_loss: 0.1867 - val_accuracy: 0.9152\n",
            "Epoch 888/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1862 - accuracy: 0.9141 - val_loss: 0.1890 - val_accuracy: 0.9141\n",
            "Epoch 889/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1860 - accuracy: 0.9150 - val_loss: 0.1851 - val_accuracy: 0.9141\n",
            "Epoch 890/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1895 - accuracy: 0.9112 - val_loss: 0.1857 - val_accuracy: 0.9155\n",
            "Epoch 891/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1821 - accuracy: 0.9153 - val_loss: 0.1880 - val_accuracy: 0.9149\n",
            "Epoch 892/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1868 - accuracy: 0.9128 - val_loss: 0.1924 - val_accuracy: 0.9119\n",
            "Epoch 893/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1916 - accuracy: 0.9117 - val_loss: 0.1918 - val_accuracy: 0.9142\n",
            "Epoch 894/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1851 - accuracy: 0.9148 - val_loss: 0.1891 - val_accuracy: 0.9124\n",
            "Epoch 895/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1875 - accuracy: 0.9139 - val_loss: 0.1927 - val_accuracy: 0.9128\n",
            "Epoch 896/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1855 - accuracy: 0.9146 - val_loss: 0.1880 - val_accuracy: 0.9140\n",
            "Epoch 897/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1838 - accuracy: 0.9152 - val_loss: 0.1888 - val_accuracy: 0.9128\n",
            "Epoch 898/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1799 - accuracy: 0.9147 - val_loss: 0.1853 - val_accuracy: 0.9155\n",
            "Epoch 899/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1876 - accuracy: 0.9131 - val_loss: 0.1884 - val_accuracy: 0.9125\n",
            "Epoch 900/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1796 - accuracy: 0.9180 - val_loss: 0.1872 - val_accuracy: 0.9143\n",
            "Epoch 901/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1848 - accuracy: 0.9142 - val_loss: 0.1879 - val_accuracy: 0.9128\n",
            "Epoch 902/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1831 - accuracy: 0.9141 - val_loss: 0.1884 - val_accuracy: 0.9143\n",
            "Epoch 903/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1882 - accuracy: 0.9117 - val_loss: 0.1861 - val_accuracy: 0.9127\n",
            "Epoch 904/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1895 - accuracy: 0.9125 - val_loss: 0.1895 - val_accuracy: 0.9135\n",
            "Epoch 905/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1902 - accuracy: 0.9086 - val_loss: 0.1888 - val_accuracy: 0.9142\n",
            "Epoch 906/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1870 - accuracy: 0.9129 - val_loss: 0.1862 - val_accuracy: 0.9139\n",
            "Epoch 907/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1849 - accuracy: 0.9149 - val_loss: 0.1871 - val_accuracy: 0.9139\n",
            "Epoch 908/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1822 - accuracy: 0.9160 - val_loss: 0.1941 - val_accuracy: 0.9109\n",
            "Epoch 909/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1848 - accuracy: 0.9157 - val_loss: 0.1859 - val_accuracy: 0.9155\n",
            "Epoch 910/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1816 - accuracy: 0.9178 - val_loss: 0.1854 - val_accuracy: 0.9157\n",
            "Epoch 911/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1868 - accuracy: 0.9150 - val_loss: 0.1938 - val_accuracy: 0.9141\n",
            "Epoch 912/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1821 - accuracy: 0.9165 - val_loss: 0.1881 - val_accuracy: 0.9121\n",
            "Epoch 913/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1863 - accuracy: 0.9117 - val_loss: 0.1875 - val_accuracy: 0.9159\n",
            "Epoch 914/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1885 - accuracy: 0.9129 - val_loss: 0.1955 - val_accuracy: 0.9143\n",
            "Epoch 915/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1877 - accuracy: 0.9149 - val_loss: 0.1857 - val_accuracy: 0.9129\n",
            "Epoch 916/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1888 - accuracy: 0.9121 - val_loss: 0.1915 - val_accuracy: 0.9130\n",
            "Epoch 917/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1908 - accuracy: 0.9097 - val_loss: 0.1919 - val_accuracy: 0.9133\n",
            "Epoch 918/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1859 - accuracy: 0.9117 - val_loss: 0.1861 - val_accuracy: 0.9139\n",
            "Epoch 919/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1893 - accuracy: 0.9104 - val_loss: 0.1856 - val_accuracy: 0.9143\n",
            "Epoch 920/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1825 - accuracy: 0.9164 - val_loss: 0.1902 - val_accuracy: 0.9148\n",
            "Epoch 921/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1856 - accuracy: 0.9134 - val_loss: 0.1863 - val_accuracy: 0.9135\n",
            "Epoch 922/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1858 - accuracy: 0.9126 - val_loss: 0.1867 - val_accuracy: 0.9145\n",
            "Epoch 923/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1888 - accuracy: 0.9113 - val_loss: 0.1870 - val_accuracy: 0.9120\n",
            "Epoch 924/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1887 - accuracy: 0.9129 - val_loss: 0.1881 - val_accuracy: 0.9136\n",
            "Epoch 925/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.1868 - accuracy: 0.9130 - val_loss: 0.1847 - val_accuracy: 0.9139\n",
            "Epoch 926/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1875 - accuracy: 0.9124 - val_loss: 0.1902 - val_accuracy: 0.9132\n",
            "Epoch 927/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9127 - val_loss: 0.1889 - val_accuracy: 0.9136\n",
            "Epoch 928/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1869 - accuracy: 0.9137 - val_loss: 0.1869 - val_accuracy: 0.9134\n",
            "Epoch 929/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1883 - accuracy: 0.9119 - val_loss: 0.1963 - val_accuracy: 0.9139\n",
            "Epoch 930/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1862 - accuracy: 0.9124 - val_loss: 0.1917 - val_accuracy: 0.9135\n",
            "Epoch 931/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9126 - val_loss: 0.1858 - val_accuracy: 0.9152\n",
            "Epoch 932/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1844 - accuracy: 0.9144 - val_loss: 0.1896 - val_accuracy: 0.9120\n",
            "Epoch 933/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1864 - accuracy: 0.9131 - val_loss: 0.1873 - val_accuracy: 0.9134\n",
            "Epoch 934/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1842 - accuracy: 0.9145 - val_loss: 0.1859 - val_accuracy: 0.9147\n",
            "Epoch 935/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1861 - accuracy: 0.9142 - val_loss: 0.1859 - val_accuracy: 0.9129\n",
            "Epoch 936/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1810 - accuracy: 0.9168 - val_loss: 0.1881 - val_accuracy: 0.9135\n",
            "Epoch 937/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1818 - accuracy: 0.9178 - val_loss: 0.1884 - val_accuracy: 0.9133\n",
            "Epoch 938/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1880 - accuracy: 0.9128 - val_loss: 0.1902 - val_accuracy: 0.9121\n",
            "Epoch 939/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1864 - accuracy: 0.9126 - val_loss: 0.1904 - val_accuracy: 0.9141\n",
            "Epoch 940/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1862 - accuracy: 0.9153 - val_loss: 0.1880 - val_accuracy: 0.9141\n",
            "Epoch 941/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1853 - accuracy: 0.9158 - val_loss: 0.1859 - val_accuracy: 0.9137\n",
            "Epoch 942/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1860 - accuracy: 0.9120 - val_loss: 0.1881 - val_accuracy: 0.9135\n",
            "Epoch 943/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1843 - accuracy: 0.9137 - val_loss: 0.1868 - val_accuracy: 0.9122\n",
            "Epoch 944/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1809 - accuracy: 0.9170 - val_loss: 0.1921 - val_accuracy: 0.9088\n",
            "Epoch 945/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9123 - val_loss: 0.1876 - val_accuracy: 0.9145\n",
            "Epoch 946/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1848 - accuracy: 0.9148 - val_loss: 0.1869 - val_accuracy: 0.9139\n",
            "Epoch 947/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1822 - accuracy: 0.9160 - val_loss: 0.1854 - val_accuracy: 0.9142\n",
            "Epoch 948/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1881 - accuracy: 0.9101 - val_loss: 0.1877 - val_accuracy: 0.9130\n",
            "Epoch 949/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9134 - val_loss: 0.1889 - val_accuracy: 0.9146\n",
            "Epoch 950/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9151 - val_loss: 0.1891 - val_accuracy: 0.9146\n",
            "Epoch 951/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1830 - accuracy: 0.9154 - val_loss: 0.1846 - val_accuracy: 0.9124\n",
            "Epoch 952/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1856 - accuracy: 0.9140 - val_loss: 0.1864 - val_accuracy: 0.9118\n",
            "Epoch 953/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1836 - accuracy: 0.9157 - val_loss: 0.1932 - val_accuracy: 0.9121\n",
            "Epoch 954/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9140 - val_loss: 0.1895 - val_accuracy: 0.9144\n",
            "Epoch 955/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1849 - accuracy: 0.9150 - val_loss: 0.1901 - val_accuracy: 0.9143\n",
            "Epoch 956/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1843 - accuracy: 0.9141 - val_loss: 0.1870 - val_accuracy: 0.9158\n",
            "Epoch 957/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1872 - accuracy: 0.9123 - val_loss: 0.1867 - val_accuracy: 0.9130\n",
            "Epoch 958/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1859 - accuracy: 0.9147 - val_loss: 0.1901 - val_accuracy: 0.9131\n",
            "Epoch 959/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1837 - accuracy: 0.9160 - val_loss: 0.1861 - val_accuracy: 0.9130\n",
            "Epoch 960/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1883 - accuracy: 0.9121 - val_loss: 0.1971 - val_accuracy: 0.9105\n",
            "Epoch 961/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1885 - accuracy: 0.9131 - val_loss: 0.1912 - val_accuracy: 0.9120\n",
            "Epoch 962/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1795 - accuracy: 0.9162 - val_loss: 0.1957 - val_accuracy: 0.9131\n",
            "Epoch 963/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9135 - val_loss: 0.1853 - val_accuracy: 0.9142\n",
            "Epoch 964/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1859 - accuracy: 0.9130 - val_loss: 0.1859 - val_accuracy: 0.9143\n",
            "Epoch 965/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1843 - accuracy: 0.9143 - val_loss: 0.1906 - val_accuracy: 0.9139\n",
            "Epoch 966/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1853 - accuracy: 0.9140 - val_loss: 0.1884 - val_accuracy: 0.9140\n",
            "Epoch 967/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1837 - accuracy: 0.9144 - val_loss: 0.1878 - val_accuracy: 0.9141\n",
            "Epoch 968/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1852 - accuracy: 0.9139 - val_loss: 0.1865 - val_accuracy: 0.9135\n",
            "Epoch 969/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1838 - accuracy: 0.9135 - val_loss: 0.1902 - val_accuracy: 0.9133\n",
            "Epoch 970/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1839 - accuracy: 0.9149 - val_loss: 0.1859 - val_accuracy: 0.9139\n",
            "Epoch 971/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1906 - accuracy: 0.9104 - val_loss: 0.2112 - val_accuracy: 0.9119\n",
            "Epoch 972/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1893 - accuracy: 0.9145 - val_loss: 0.1905 - val_accuracy: 0.9123\n",
            "Epoch 973/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9134 - val_loss: 0.1951 - val_accuracy: 0.9089\n",
            "Epoch 974/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1856 - accuracy: 0.9149 - val_loss: 0.1860 - val_accuracy: 0.9129\n",
            "Epoch 975/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1891 - accuracy: 0.9110 - val_loss: 0.1849 - val_accuracy: 0.9145\n",
            "Epoch 976/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1820 - accuracy: 0.9169 - val_loss: 0.1859 - val_accuracy: 0.9137\n",
            "Epoch 977/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1845 - accuracy: 0.9142 - val_loss: 0.1868 - val_accuracy: 0.9126\n",
            "Epoch 978/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1883 - accuracy: 0.9148 - val_loss: 0.1870 - val_accuracy: 0.9125\n",
            "Epoch 979/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1821 - accuracy: 0.9156 - val_loss: 0.1943 - val_accuracy: 0.9126\n",
            "Epoch 980/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1853 - accuracy: 0.9134 - val_loss: 0.1850 - val_accuracy: 0.9152\n",
            "Epoch 981/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9132 - val_loss: 0.1871 - val_accuracy: 0.9145\n",
            "Epoch 982/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9113 - val_loss: 0.1904 - val_accuracy: 0.9126\n",
            "Epoch 983/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1801 - accuracy: 0.9159 - val_loss: 0.1979 - val_accuracy: 0.9090\n",
            "Epoch 984/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1882 - accuracy: 0.9125 - val_loss: 0.1857 - val_accuracy: 0.9136\n",
            "Epoch 985/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1828 - accuracy: 0.9148 - val_loss: 0.1938 - val_accuracy: 0.9127\n",
            "Epoch 986/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1823 - accuracy: 0.9139 - val_loss: 0.1860 - val_accuracy: 0.9134\n",
            "Epoch 987/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9124 - val_loss: 0.1867 - val_accuracy: 0.9143\n",
            "Epoch 988/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1839 - accuracy: 0.9131 - val_loss: 0.1859 - val_accuracy: 0.9139\n",
            "Epoch 989/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1859 - accuracy: 0.9138 - val_loss: 0.1856 - val_accuracy: 0.9131\n",
            "Epoch 990/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1798 - accuracy: 0.9165 - val_loss: 0.1859 - val_accuracy: 0.9141\n",
            "Epoch 991/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1828 - accuracy: 0.9133 - val_loss: 0.1879 - val_accuracy: 0.9150\n",
            "Epoch 992/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1774 - accuracy: 0.9181 - val_loss: 0.1864 - val_accuracy: 0.9142\n",
            "Epoch 993/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1873 - accuracy: 0.9150 - val_loss: 0.1898 - val_accuracy: 0.9119\n",
            "Epoch 994/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1862 - accuracy: 0.9135 - val_loss: 0.1856 - val_accuracy: 0.9148\n",
            "Epoch 995/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1832 - accuracy: 0.9138 - val_loss: 0.1872 - val_accuracy: 0.9124\n",
            "Epoch 996/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1868 - accuracy: 0.9121 - val_loss: 0.1908 - val_accuracy: 0.9131\n",
            "Epoch 997/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9140 - val_loss: 0.1875 - val_accuracy: 0.9141\n",
            "Epoch 998/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1892 - accuracy: 0.9132 - val_loss: 0.1909 - val_accuracy: 0.9142\n",
            "Epoch 999/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1881 - accuracy: 0.9164 - val_loss: 0.1855 - val_accuracy: 0.9156\n",
            "Epoch 1000/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1870 - accuracy: 0.9147 - val_loss: 0.1876 - val_accuracy: 0.9126\n",
            "Epoch 1001/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1898 - accuracy: 0.9119 - val_loss: 0.1854 - val_accuracy: 0.9126\n",
            "Epoch 1002/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1874 - accuracy: 0.9109 - val_loss: 0.1879 - val_accuracy: 0.9131\n",
            "Epoch 1003/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1900 - accuracy: 0.9129 - val_loss: 0.1879 - val_accuracy: 0.9143\n",
            "Epoch 1004/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1859 - accuracy: 0.9113 - val_loss: 0.1911 - val_accuracy: 0.9138\n",
            "Epoch 1005/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1866 - accuracy: 0.9115 - val_loss: 0.1866 - val_accuracy: 0.9152\n",
            "Epoch 1006/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1827 - accuracy: 0.9131 - val_loss: 0.1865 - val_accuracy: 0.9147\n",
            "Epoch 1007/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1824 - accuracy: 0.9169 - val_loss: 0.1906 - val_accuracy: 0.9125\n",
            "Epoch 1008/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1904 - accuracy: 0.9126 - val_loss: 0.1882 - val_accuracy: 0.9139\n",
            "Epoch 1009/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9134 - val_loss: 0.1869 - val_accuracy: 0.9133\n",
            "Epoch 1010/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9134 - val_loss: 0.1857 - val_accuracy: 0.9153\n",
            "Epoch 1011/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1832 - accuracy: 0.9154 - val_loss: 0.1904 - val_accuracy: 0.9105\n",
            "Epoch 1012/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1847 - accuracy: 0.9134 - val_loss: 0.1918 - val_accuracy: 0.9117\n",
            "Epoch 1013/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1842 - accuracy: 0.9138 - val_loss: 0.1888 - val_accuracy: 0.9137\n",
            "Epoch 1014/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1844 - accuracy: 0.9149 - val_loss: 0.1879 - val_accuracy: 0.9139\n",
            "Epoch 1015/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1824 - accuracy: 0.9142 - val_loss: 0.2147 - val_accuracy: 0.9082\n",
            "Epoch 1016/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1901 - accuracy: 0.9144 - val_loss: 0.1918 - val_accuracy: 0.9127\n",
            "Epoch 1017/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1829 - accuracy: 0.9146 - val_loss: 0.1872 - val_accuracy: 0.9153\n",
            "Epoch 1018/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1877 - accuracy: 0.9138 - val_loss: 0.1921 - val_accuracy: 0.9142\n",
            "Epoch 1019/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1900 - accuracy: 0.9127 - val_loss: 0.1854 - val_accuracy: 0.9141\n",
            "Epoch 1020/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1846 - accuracy: 0.9150 - val_loss: 0.1912 - val_accuracy: 0.9114\n",
            "Epoch 1021/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1842 - accuracy: 0.9134 - val_loss: 0.1885 - val_accuracy: 0.9143\n",
            "Epoch 1022/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1823 - accuracy: 0.9157 - val_loss: 0.1872 - val_accuracy: 0.9141\n",
            "Epoch 1023/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1859 - accuracy: 0.9151 - val_loss: 0.1888 - val_accuracy: 0.9150\n",
            "Epoch 1024/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1812 - accuracy: 0.9165 - val_loss: 0.1865 - val_accuracy: 0.9129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f538dcb2150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5wURfbAv28TS05LkLioKGAgCigGFOXAgIog5nAqJ2bPhN6piPqTMycM6GFOiGJEMRwoCiggOQioSBSWzAIbZqd+f3TPTE9PT9rdYRf2fT+f/Wx3dVV19UxPvar3Xr0SYwyKoiiKkihpFd0ARVEUZd9CBYeiKIqSFCo4FEVRlKRQwaEoiqIkhQoORVEUJSkyKroBe4OcnByTm5tb0c1QFEXZp5g9e/YmY0wjd3qVEBy5ubnMmjWropuhKIqyTyEif3qlq6pKURRFSQoVHIqiKEpSqOBQFEVRkkIFh6IoipIUKjgURVGUpFDBoSiKoiSFCg5FURQlKVRwKFWbTSvg9+8quhWKsk9RJRYAKkpUnu1q/R+xvWLboew77N4CNRpUdCsqFJ1xKPsGyybB9NGlK+srgk+uhx3ry7dNSsWydSUU7ty791w3Fx5uAwvGx87nL4GNS/ZOmyoAFRzKvsHb58Kku0pX9qfn4ZfXYeKt5dumfYWNS2HL73vnXpt/g7xf9869nuoIr/SPTF/1kzUriMXk/4PJDyV/z3W/WP//+D56Hr8fxvSG53omJzzmvAXjLk2+TRWACg5l/+WlPvDpjfD1PdZ58Z6KbU88XjkVPvyH97VEtnjOz4P54yLTn+sBT3cuW9uiYYzVUQZ4pguM7p58PfPHwYi6yX9Hfy2ITBvbF147I3a57/4D341K6Bab8gv5fL41WzW+IgB8kmld9PpefnwC/ppvHW9bndA9APj4Glj8UeL5AV4bAO9fnlyZciClgkNE+onIryKyQkSGe1xvLSLfish8EZkiIi0c174UkW0i8pmrzKsi8oeIzLX/OqXyGZRypGAHFO0q/3pLimHXZut44Yfw+xTreO0smP1qKJ+vMH5dT3W0OrCZ/y3vVobIz7NUGW7+/BHmv+td5pX+8EAT72vFBfDNCKuz/PAqq/6ysHODJQzyN8bP+3/N4Z0hZbsfwDf3Wf93ebR9RF1490LreNVPMPftsMt/bS8InQSE2IaF9nlJ6PPYutL6nLyEU8F2qw0lxRGXhr05m2vf/oXN+YX8un4rAL+s3QUbFsF99WDFt3BfA/jv36wCK751tCeyvnLlj+9g0YeAJdR+XrQcv89X9ncgDikTHCKSDowG+gMdgPNFpIMr26PA68aYI4GRgHPu+AhwcZTqbzPGdLL/5pZz05VUMaolPHtUYnm3rbY6MDdeP/qPr4NHDrQ6jfGXw+tnetdZEkdw7NpsdS4An/8zsXbGwlcEfy0MTyvMh0cPhi8jxlGOcoXw6xewa1MobdV08Dk6SGOsTtTvh5kvwQ9PQJ6tFpEEftZz3oSfX4Jtq2DC1SGhunY2PHYIPN0RHm3r/R04Kd4Fy7+Kfz8n29dE1uv32W1P9y6z1B4/ju0LHw1j+ejBwUtXvW5Hvp75X/jltfBy3z1sfd47N8A751uf06jWjvvagmbKf+CHx2FepODesMP6bHYU+CgqtN6/3cV+eL6XlWHBeDAlsHqGde4UPiVF4ZVtXFq6wdOGRfDpTd4DDrvevP8Opvv73Vj01m3WMwcGUykglTOO7sAKY8zvxpgi4F3A/YvuAPzPPp7svG6M+RbYy5YvJeXsWGupkBaMj613f/JwqwNz81BLa4TtZMH71n/3j9SNryD29ScOCz8v8YWfv3cRfDE8UhhEY+Kt8EIv2L7WUafdxtmveZcB+PwWeOc8eOSg6Hlm/dfqRD+/OfJzFInfto+vtdr3xR0w7x3L+WDxx7BhsXV92yrr/x5rhM38cfBgM0vltWcbrPwxfEayfn78ewLMeN76nN3fbUBw+H2RZQKMOTF42DYvJKwKd9r2jM//CZ/d5KjTH1JHbf0DivKtY+cAonC7pc6cYTteBPIA42atZsqvG6lZzXI+3bq7iDRbKDQpXAnYaqrCHeHtTM8MHYcJkWJLbRjPjuGl/nr/cpj9imVDAljyGf5ix3M814PG66cA0HS91aUW/DGNTb9Op8SfgJozSVIpOJoDTgXfGjvNyTxgoH18NlBbRBomUPeDtnrrCRGp5pVBRIaKyCwRmZWXl9ppW5Xl7fPg5VOsDtap5zYGfnrRGln+/JKtB3eMlNbOgg+uCOndv73f6pTG9oPxfw+/h/tH5C8OjdZLiq3Ozth1xxUcMWYcJcXgc81mCraF33/Jp5ah/YVejuccY6ngvFg13frv9PwJfA4lhdZn5i+Bx9rDl3eG8qyZGZ7/t8nh9e7eApP+ZR3PfhVmjQ2/bux6Zzwf3+so0FFOfQzGXQKfXBd+/bkelmrow6us2cWW363nevVUawAQ4MXjYPwV0e8T6ECjzbQC3+EC20azbg6MqMuSBY7PImCYdvGQ72GPVGHLVseIe882SPNYfVCwI1ydOWWUpRqb9x7nfnY4w16ZSs0saxa0Jb+IdL/1jrXP/ylUxiU4Nu5yvOslxdbME0IDl5U/hOU3xrB9d0jAlJT42FUYLkDX7JLQvf6YCu9diDzk7k5t7OfMHn8ROe/0Y/nG8h9/V7Rx/FbgBBGZA5wArAWizMWC3Am0A44CGgB3eGUyxowxxnQzxnRr1ChiAyulPFj2Baz5Ge5vCO87RlErvoUvbrdGlhNvhR+fDFe7OFkzG6Y+anVKq6bDwg/Cr3vNEtbNsf7fn2N1dgE2rwgdexkl3YLDaUi+Pycy/yMHWaoNiJx9gKVf/uI2a9TuRUBIFGyzOnpfYfiI+uWTYGQD2LkOZjwXSneOWB9sCm+cFV7vx9fGnj1tW2WpXL4cDtOeiZ4PQt5B62NofD8aFn4eaN/2VeHpC0MuqpvzC63n/+pu+OFJ6/Pd6toT6Mu7QqrHwMzmfw9Y/207xqfvvhS7/UAbs8oj1XDhsw4V2p6t3oJjp8tFu2Cb9X+KpTVvJRs5AOvd3VXkI7M4yiDBwY4ix2Bj6WfwQCPLiB94/3x7+GrGXHKHf85f2wt4+os5DLz/lWCRhz5bwGH3TuLnP7YEBcqf+bYKr3AHM5dYsw6JYj/xu9R9jWtnx21zsqRyAeBaoKXjvIWdFsQYsw57xiEitYBzjDHbYlVqjAl804Ui8gqW8FFSTdEu+OgaOPwcOPAEyK4bfn3JJ6HjYpcO95sR0H6Ad70vnxT7vn9OgzcHhqft2eJpxOSlkCqDJw+PvO6ekXx4Vex7g+XGe9w/I9URENJV74ni+mkLCfPDE8iyL6Fxe2hzQuh6QAC6cf7w3W3evia+0dr5OWQ4Oo3vHoaew0Ij4NKSgA3liW+W8Y9mv9Ny2tOhxI2LoWajkAF8xmio0wzq54YX/nakw5YVX82SBp7qnaaFf0CWfVJS6K0CGvu3KJVaXeOkasNhA9RIv4p1m1pz9KqvI/OmZ4WdFhvH9/frROv/hGFwQch+kvX5DcAdfPLsP7mx+C1udOhN3p3xG1CDc1+cTtM62Vxd/DoXpi0FYNf2Lbz0w0qOCr9lGBvzfTRxaCvrVc+MnrmUpHLGMRNoKyJtRCQLOA/4xJlBRHJEgm/hnYBrzh2JiBxg/xfgLCBBhXMFsvrnkG6yIpj8kKVSChhrvVwYnWxfC4+0hbxlobRFEyxXwXEXw6hWcF/9yHK/vA4vHu9dZ2kXQ719bmRawQ6rA00Wt3E5UfJ+hd9d6qJvR8K7F1jHUQy6Pp8l3JZusdV421bH1uEHiKVyc9th4pHmaNvkBy314vPHJFeHi3VzPTpPF/WqZzHio3nhiSVFUMvlGTbtaXjvwvC0qY/BnDcAyJZEhJzxVEO+kvWI497F8Z0jnFSrHXb6n8yXGPZDLxqzNSLrDkcTdzzSkWI83ocNC8LamIX1bgwtfisia0vJ4/OsOzlI1vLXjgIuMx+RKdbsddQHUxmUHmMNCVBkwrv1tLQEbF5JkjLBYYzxAdcBk4AlwDhjzCIRGSkigeFnb+BXEVkGNAEeDJQXkanA+0AfEVkjIoGhwVsisgBYAOQAD6TqGcqN/55i+beXhqLdyeUvzA8/Lym2DIRrfram6y/0gheOjXG/XbD0c9i10dLnByhwheQwfiL45HpYPy9cfRTA3TnEwmkz8OpoC7bBt/clXl+APVvh+0csD5toI003Ita6BLftZepjoeO0NEs99EBTePRQK61oNxn56wBou8l2z/xuFJumPE9cMjzNdiHWzkqs7RBawxLgzx8g/6/Ey3vQbMFzcfM0yCohC9d39/2jke62+bG9tm7IiL+uQTBQHPt3smLVWs+Z1vclR3jmNwnMdALUWfVN6HjXSo7cOdUz385dod9mx7TfuDXjPc98V2V8zmFpf/J11u1kuD7D2zLeo2/67JjtKTLlP8Nwk9JYVcaYicBEV9o9juPxgOfafWPMcVHS4+g29jEK8+Gbe6HdaXCQ69GWfQVvD4YrvoGWCbixTnsWvvoX3LIMatsjuzCvDseIa81saN4l3ANnyWdWB9/zWuu8YLvV2X47EjJrlO75SoPTUByNRRNKV/f/Hgjp0RMhkRXXkmbNPnx7IH8PTLw9TJ+e4TDb5cx/MX59O8vWsVcG/v7d0fzdrU75K0HPqyQRY7wHMg4OXvgExdXq4+5SC/DW+fy1bjUHlFP7Arw15hGutl+LmlLIdRkfe+bLtIVFmhg+yBoRdq2OxF8geXT64vAEYxLztEuCijaOKzOeg5kvwxtnR8a/WWGPZNbMhN/+Fy4EinZZKjAnAX1yQB+/4pvwkbJzOv/ySZYhOrBuAULeHoER7cIPYGx/y2tnvvfoKCVsXr737lUe/LUwPA7Wzy+GdNulwW2wVWJSh3z+fKx33HyZhZFqppbibS8KGMTLk6szPk0oX21CwqFjWvjApcSUQgAkoh5NEhUcFY1zMdC6OeFurYEv/IcnLMFyf46lCtq0HJ7uYqnAtv4JT3WyXFkD0/5nu1m+6W+eY3k+BXDrgT+4wlopXZgP/8m1Fn9BeMcVWFRG+etJo7L6J+/0mo33XhuSYctvsNvV0ZTGBlPOfFQS3ZbxVUnXiLRnfGd55Nw3aO1PIrSHg/ZppSuXSupJPjtMdc9r6VKKNRmJRExIEhUclYm1v8DI+tbiKoDdth/6LseoaOrjlmAI6KlX/2wtbgqsGQjg9E0PsGNtZBrAQ80tldQaewbj1entSiD8RDI08fB6iseJd8Hdm6BztIACcWjsDlxQRg7sDdWjhNdOdaiJBCg00V1vVpqmEWlFpnLvsvB48aCKbsJeoVnWHtaaclxCEG99UylQwVEe7N5iGYbjBWib+bKV96NrrJWgy78O1z2ummb9/9/9lt+/V8CzNJfHxoYknMq8PJS8iKMvLheihU6IRXqWtYYg4GJaw2PtRTQGPAPZ9ULnZRUinS6CSz6O7opbCfDHmCXu8dDtF7osAA8Xn8sRBS8ndK/5/jb87o8URl6cWvh/CeVzs4vyX4/g5vKi2+K3w8RxXigjjc0mqtWsGz9joqjgqKRMftByRZ3rcq3bsCgU+AysUBLfPWzlW/QhvDXIez3Cqukw/Vnve7kXMe2tcNnlTUAN1+DAxMsEFp4FPgO3eihm2WqQZRv4s2pBj6sTL+uBX9Ixybj0Jlt/aXTZLkwMwVHg0fkVkRl23y/8PdhJuFPEOuM9w8qR7WRJYrr0xSY3oXzuGYbPy821nJnpPzRunj6FjyZV51/Gcl1fYxIc6JQUUZheg9uKhyZ1n1j1lTcqOMqDQAfy+S1WuILRPa3zibeHAp8F87pG89u8Vr3GwG2kTiSCaWUkEGLi4FMSLxNYh1GUWAiFbVmOEXB6JmTaeuPM6pBVM2q5R4sHR71WaLs6zl+3k4v+G8UW48FzvigLIKOQ7zG67lLwQlJ1xJpxFET4F1kzDqco9FqPsC5K57fT1Ah6A8Uiz0QfSX9eEh6O/XN/j7DzkjJ2V+9VP5c/2ocGDDtMSCjuyGrCE9WvJZ/43oPdDo8vXJz8aSwPx89b3Z5wmSKpzjqTSPSlBCjrgk8PVHCUB25Xt7wlsOUP7xDRP7vcMZd8EpknGdyCaV8hEKiwpkuX2zXG3gKBuEuBWdpZrjURnS9i8qnfMePiFTBiO/9pF/JS21IkoZlKRvWYayW2E12obKIOAHPX5vPjisSjjz7hS04/P6Ekcq3NFuqwxN8KgPuKL6ZP4SNcU3RD1DpidbRFHoKjyGTid5TxmUjB4TXHeqx4EJcX3U514ndQlxZFCc8C3FMc/t2L625ndWkVt/5YDLnjJVo2rAVA0XHD2UnIAL29Xge2t7fWGuUfeGrMep4+P8EIz8C5hXdTYi/Iy6oWRdXmjKrQzFrvVZSeTXF52JyOHALZdcpejwsVHOWCx8huz9aQcXtfJFp46/Ii4E1WyyU4jog+2g8ubgx4ibhCPXDmaC7/cC3nvfQzb/+0ip0FITXgzeOXBFUdO/2ZlvCIgj/Gz8JvdwIlSahNSurlUuxaMvWO78Sw84GFI7iw6E42GMsOs8y05MTCx3AzWB4lt+BtXinpz2+mOV/7uyX0HG7Vl5dQKSSTyf7Q9jY+j2VeDxefF5H2kb8X68jhO/+REdf6FY7imIJQ2JGiGEvH3DaWP01TfvK3Y7nfCubXrbVHtIIkyUizO/H0NJpUC70fzepW41+ntefrm4+n1tlPhJX5oCR8SVlaevT344qiW8LOfzbtSRdLy5DbqLZXkdBMGIKdfHFatqdwB+CiDyLTMqIIpb4PQK3y90ZUwVEeeC2uKSmO7z9du1lq2lNWrp0Jhw+Mn6+03LQgpG5yx7yKFQepub36PjDjSA//YRWXhNSAd01YwKotodXERWQwf50leFZuL2FPjNFcLBWPz/7J+JL46aSnpTH6gi6cU3hvMO2dkpN47+jQbHMX2fzoP4L5fsvmU0y6p5fToK4tyLI7rtf+3p17z+zoec+trf7Gj/6Q55pxzbC81Er5VOf64uuD5zl1ItU2M0278FhMhLy3/lV8BX9dFu7dt9S0Yh0h9VYsO4VbmBWTwbqzPwjZHRJx2khkLxKrMjKKQyu50/GTmZ5G2ya1IwYk9Y48lfyO4ZED7mv3Cd0LRocSmhwOJwznf/7InRbTsNrdu10U5wHnd5Nlz4jSqocL2caOUDO1HPUc0t/6vQ6b5u2u7hXYsRxQwVEueAmOKEHVnLg7zcpCI499MAAu/xLuTGB9QrUYz3X6E1CvVagTcI38C30l7DnGQxfc+DBoa9tDAsa+9PDO8O+vzgw7n78mFCal0GSyIs/yettDNWbnRRcOsVQ8gVF8rFlJJMKpRzRltgnpxktI56h2oQ2FAh1qwKDdu30zz3Zs213Elzcdx+CuLeh1UEMuOjo38nYHdKT+pW8z6q47MLbnWXpm+Ig026FWKqlvCaudpjoLHghtmfPoedZajwuLQiv5l97fLxg3KUAhmawcdRrzR51L09btg+lv+E627pXpUH/FmHH4SeNVX18A/tvqP9xw0sH4/bDBNi57/l76/Sf8vNMFUeuPwCmInMeuOFV9OjSjVu8bw9LuOOdYnv2HQ6X1j+/hxDsxHt9Zui04JFon7hRU9syhfcsm4TOw0x8PHTvrGfKm9XtteFB4Hq+85YgKjvLAa8axa5O1SUwsqkWZupaGf3jHxyk1XkKvRsPE2uz+OBocCN3+bo2Outkjt1MfhXqtI2wcD362kEsne9gfqjtcaY+92fqBtQwZU9/wnczU5dG9rIrICM4SCkwWF32azxz/wZ55DcIphQ8zqPCeiGslwRlHcqo8cb0jPtKoXi8U8M9PGlkZabQ7wFJVdG3TmBl3RToO7CzwcWCjWjwyuCMZ0VQmWbUhPYOcWtWQGraB1aXKqIbDm89u205qkJkeaufhLS2h86M/FM8pOzPyucM6OMdz3u37O1NvP5Ef7wiF0nHPVpyUkMZ/fOdxc9EwmnQ9g3/2PRS/MTxXcibvNBsOhw20BhAn/TtUqF7L8Era9I5afziul9TpHp6eCSO2wwGdQueuDjg7M53ubRweZrab/DPnh2YcJblWwM+ltl0qbM1Po/ZQx95Pwym0bCHSJKcB397Uy1G/4/5Ol/x0R7rXbCs9NXGrVHCUCx6CI9bWoAEyy9EvvUYcD4zed0HfB2PnCcMlOC75JPpMxI37BZZ0a6Zhh5UuLvFT3PUKuGk+tOgKl4a2lV++YUewc17tb8QDxXZwRGMoLvEz4pNFvJfXkrdPmcmP60I/uLt9riCELooIGX4D8Yke8XmvaykxaSw3LZhl2kVeIy34/4Y+bWPeM4jHD9pHOjWyMhznadx9Wnta2cbbJvVqQq3G7GoWWv1988mHMGJAKaPjulRVDauFOkqxbUa7yQ4XcGmZfDDsaB4eFGm7cOK23wR4+vzOtGxQg4a1QveOJXA/vv549pDNBP9xQaHYtkltiskgvctFVjDJa6bB8bex7aw3KDryokg7VzIxmVo4jNyx1GBpmdFtfme9AEeFwvOf0TGkfpZLLFXkfb5LGHf4GMg52FLTXv8LXDvDGgC57x04zqwBDR3vl/MdkjQY/Jq1lsiJVxvTUiM4KvdS0crKsknWiPydIdCyBzSPDN8QL+onEN1Ae80MeK5ncm2q4fKvT68WHtQw52Bo3s0Kgghw3K3WVpSJGvAPPCF+niDeP97pv23mxe9/Y+ryTWSlp7Hk/n5M/20z0IGjGxwYXJMSUNdsoi4LTZtg+Sm/5vHqtJVhda5MUPYWkxHstPZgdWTT/Icz038IR6UtC8ubiBqq50GN6dT7IPgxgZvbndkXNx4HtlPd40M6U9exT4I/GApbQv/T0sm+/CN40NJd33hyooLK8fkHOhxbcPziP5guaSsY3ONgCJgjqteDHWvY7V7bkZZO19YN6Nq6AXxGVKKp9gZ0jLTheYYctzmseX2GdGvJe7NWUzvb+mw6tazHjDv70KROeNvqdRoAnQZYgUCdeAmOAc96G4gv+dgK7DlhaBTBYQ+e0tKjq3w6nW/9eRAIZ15IFucOOttuuMMzLCDUjR/OHmOdL/3cSsuqCRkOoeiecRx2lvUXdkMvwZEaJxedcSTLzg3WCux3hljnq3+i1HGcnN4Up4y0/qdlWBv+lKUugH6u1bnGhL9E9XOh36jIegIjlFj2mUYx2nflt3CCbaO4yt5O3v4xPzJpKVN+zaPEb9hTXML42Ws4/6UZnP/SDGhtjax3mexgR5SGH2N7A/20cgtbd5XeH72Y9KAnVIEjFMefHqE3uuZGCSPioOfBjT3VNt5Yz9D+gJBbZMeW4ffwkU6X1vUdI0vr80/PiLFjT0K3tuuzO55xJb15vcal1Drpn6Es57/Dv4r/zhZcbpsJj94Tf/+jzU4C3H1GB+4/63COOSg0g25aNztC1RfC9Z56qWu6XAyHeITRz6oJdVtYx16RDOrbg5Y6zVLTAQdmCMZAxyFwxKBQqBr379l5/2izH69nL+eouMHmpKTW/RmvneBK++UEXo5mXUI7wwW+/IP6eJeJxc2LQsdeMZScL5Z7ih8gGGMphuC4NEaUzxbdrF3mRmwPeogE+HNz+J4Jt74f2uin3fQ+DCu6kfnmoDDB4eT2DyLDcvcpfIQBhfdHpDetEz4V8Zu0YL3OcBtC5Ejzwh6teeq8ThHpYSRjdPR6P1w/8h/vPJnDmtUN5Q0I7rL+8F0dXglpzMu9wnr3rvgGrpuF1GvFWyUnl+0+CRIQHAW974HWkWtValXL4OKerWMIChfuAU7CXlWu/F4zjr4PwPnvWgO5UgqOB88+nHeHRtEeuL9rCAkw9+8zmo3DifuzaJykWjMJVHAki9dez9HCg8QjoHfOrBE6DowmhrwRpvuP4Ppfous4s2p5L3BzGrajLYA71PYUiTXjcK+9iMOuQh/LNuxkZ0F09+QCqvGFvVI4z17L8L3HugA3jdocwXxzUER651b1mHp7aK2Ej/Sg4HDuweC1yCpNhDM7NWflqNPgulmhkSeOsXVZvVVcHVxGRkBtFbhDEuFMul4W4z72O2G39++92vDAWbabbsujICdB9VcMLuiR+MK8oFdVr5vg8s8tdVFZiOjw4wkcl0AOqos8Zhz1WsKh/e38pRMcF/ZoTc8Do9gfvYRWwNXc/X5JAjMOnytW3jXTEm9okqjgSJaiXfHzuMmK4okUGFVkZoeOAy9TVs3QugUv6rcJ6UuPvz1UBqw9sl2uqhhjCY6Awa1a+GwAgFuXwzn/DRSIfu8YHPXgNzz7v/D9NNZvL6DvE99TVJKAHz6wgQb0LHiGx6IYr508fm4nLjsmNyI9KyONlg1C6xBKSOOyXpbbaUBw/HRXHzr9/WnG+k8LL+wc7ea0DfP8Cq5mTkpwxJ9xBDswr1FoPE5wrcZ2tr+jvWDvnJeh57W073sl1bPKV+3yf2d776IXi8yAR9iBvct2c7dKJ9EZR3BGF2PG4STZGUfLBGyUnoIj4GruMmo77x+tLcUeg9oUoYIjWQoTi5MEwNAp8O+NcPK9kddO/FfInpBZPVJwQOxRTprHV5ddB/6dB8f+M+LlWrLedg2ubvvEZ3kIjlqNQ4EAXR3X7qLEAtjl7Szk0a+Wxc8Yh79oGNdIPbx/O5rVq849p0dGus1yuapOvPlEqqdbP9CzjzqYu05tR5M62bQ7sDUX3/tmnNaEPosW9e2OKl4HNWw6HGOHA0lAVRX6rkujmopR5qgrrVD0jQ617F6J2Ex6XhM9XDzAcbdEX6mcIOnltQ/2gb2h/8PQxBZecQWH676B7yZetOZkBgp3b4bLv4ifz2XPstph/87c3lBhNo4ozxg240jt/jkpFRwi0k9EfhWRFSIS4Z8qIq1F5FsRmS8iU0SkhePalyKyTUQ+c5VpIyI/2XW+JyJltB4miZeNIxrV61sqIa+X7oTbQ6MKZ+wk50vhVe6YG0IzDK9ZQUaW9WNwCY4HfgiE67BeLpNRjRm/h3tUFRSXkF/oo/cjk9m6O9wQ/X8TrQ2dVm/ZzYhPFuE7qK+3cd2m0FfCJ/PWRb0ejRcu8vBQi0LtbPRhttUAACAASURBVOvzSfPohLIywl/tJnVrBoO9tW7SgKHHh9RbmTFCSLipGXChjdeRNOkA7c+wT7wEhystUF+iI+Cwutztd3pVSfK+/P0egjv+iH69zz3w7wS8BvcGItDjH6EZdLI2oaCBOs7nnYyqKj3De2AXWWnkvZvZ60BquzauTcTGEVgXshdImeAQkXRgNNAf6ACcLyLuoeGjwOvGmCOBkcBDjmuPAF479vwHeMIYczCwFbiivNsek2RmHNVsL5VoX3RQVVU99GKG+dF7lOt7P5z0r/j3tuvztejJ6YUPMEfaYYwhb6s185i+ag+Xzmgatktcu7u/ZO6qbazcvJtlG8Kf892fV7O7yMct78/j1WkrOXjRZXxbd2BUI/vjXy3jqW+T3wK296Hh9pOHB0VXg9RwqFxu7NOW3IY1+Ocp1lqTDLcwScuIrgZwE7PzSUJVFejQY804gjYI+3/A7TknwTUzXvWnyJMmKWoltjdH+WE/c9wZRxRjelxVVQq6Sq97nzzCWoWe41qcmoiN4+A+MGisnWffnXF0B1YYY343xhQB7wJnuvJ0AGyfTSY7rxtjvgXCei+xXC1OAgJhT18D9t5+l39MhdUz4+cLEDBGR/uiA51uelbIPtHzmtD1Unz5V7w6k35Pfs9DX1rqIr/fz0JzIAJs2VVEYYHl2bQ231BIFjcVXxde/jXr+V7dER48z+c3XDZ2Jj//scWRdxZDqz/OB42vx82L3/8ejLUTCBnR6+DYixTT04TszHSu6W3NBkZf0IXWTa3VywGDuZPqmaHO++ZTDmHKbScGZw8RXjlp6SGPsWgeZcmQkOBwrMmIuGb/9HrdEF5fpwvhlmXQzOXV1eb4WDeK35aycs5/Y3vTORm+Cm6Yk9r2RMXxWRwaI8pt4LsJuOMmuqtk9bIHWoxoQ9jK8Uw4wCP+WCIzDrDWalmVl7l5sUjlAsDmgHND3zVAD1eeecBA4CngbKC2iDQ0xkRbldYQ2GaMCSjc19j32Tu8dnpy+d0bD9VvY23zGrweUnss2VTEdbU/5sPuvfCM9HTB+9baCwd/7SikKbD0rx0cagwiwrdLrf05asg2qAbG1t2KCH9s2sXbxYN4POsFpm/w/uoLfdZL/IW/B7kFb7MyOxT75+eVkbvdfbWpAV9xNOdkPxNxbbVpwj+Lrg5GXH318u78tb2AujUyOXLEVxH5O7awnvz2fu24vZ+9ats0xQx4hjvHWbaFJ4d04o4P5lPo89OxZeQn5QyZEUZahiM4YjzBkcCPLhFjaazRb+DaySOsv2C6QO0m4XlHxAldk6wLqgf/OeeIMGeCCI5IIiy8V0ypv0+yvoNnU6TiEo8Zx/nvxC9Xo0H8zzfAuW94d+qlJTjjSMARwjnjiaU2izXLLUcq2jh+K3CCiMwBTgDWAqXYUzQSERkqIrNEZFZense+GMmwe0vyGy45SXOpIwIEF2el8+Q3y/gtbxfTfosSb+mQvmEhP9Zu28PbP1ltmrR4Az/9Ed6pb8QaGe1pao1ASvyGQS9M50P/8eQWvM2HC5LYPa8MfOg/nq3UYc7dp5CZbnk61ckOVxXd2td6rtyGHvtgiCBdLmEnNWhWN5uzOjfn1wf6s3LUaRxQN3LlfVq0H4ykhVRVZXGlDfzIE/KykbB/ntfKg4hnTr7uIUe14piDktiKN1la9bTW+KSaVHaYHQZA/dbx8yWK14wjGonOOILX9t0Zx1rAGYGshZ0WxBizDmvGgYjUAs4xxmyLUedmoJ6IZNizjog6HXWPAcYAdOvWrWx7fD7Txdpfo7QERwHhcvqvLdtpCmEj4MC7NG/1Nt75eRVO8/Nf2wuYs2orObWrMfiF6dzgeH+2uYzZa0wjJhz9Ae0P7wwzZrCnOLo8HlZ0I/Ul3/PavcWXJr7lZRTO7tycejXChcXC+/5Ghq2a+u8P1iwsYOz24vMbjo1Y1OdF1C9aJLQTWrwZRyKdT0LCJ9CaBIzjVZkLx0Per+VTV7zPNSD4U7jtb8J4eVVFzZuAV1W8a+VIKgXHTKCtiLTB6tzPA8JiHotIDrDFGOMH7gTGxqrQGGNEZDIwCMtmcilQxhVECVAWoQHBTqbYL8E4ok98vYz0n37jhgwgIzv4Hl/95i9hRUfZfeWKjTs5+fHvPasXDHNXb6dvh3CD5M2TCzl722rPMk6+8Ls1iCFM96F8//MqvF7uc7u14Iflm1i3Pbb/+KODO0bYHGpVC716gQ2XamdHN1of1iy5EPSe/UdgxhFj97/4JGEcj7X6uzx/4JXROB6FrIw0inyuEXbbU0Ih88uKpFlrKDZHccw4pB9MfRQO3jsr5WMTcAVOcsYR6/v1crJJASkTT/aM4DpgErAEGGeMWSQiI0UksFdib+BXEVkGNAGC4VtFZCrwPtBHRNaISCDYzB3AP0VkBZbNI7BirXJxxTehY3v6uNsX6nyf+nZ5MLT1u3M2snFnIbHo+0Sk0HB25S989xsH3jUxIs+EOZ4TMgByamVx3YneocUD1M7OwO+SGc3qhkb+l3gsvnMTz2f/gu6t6Na6PpccXY5qAC/8UVblRuBqr9foNJaeuYnbEyzVgiOGO24lY+ZdJzPzXynstCUNrpgEt//ufb3lUZZNo0Xibt8pIxnX60TVq3tpxpHSuxhjJhpjDjHGHGSMedBOu8cY84l9PN4Y09bOc6UxptBR9jhjTCNjTHVjTAtjzCQ7/XdjTHdjzMHGmMHOMnuVg06Kfb2lI2Rz8EsP/0EHBMfSTUXMXR1LQ0dE5x1ZW/Jsyi/i1r8dSpdW4R5LDw0MdXz1a2Th7Pf/eOjUYJTW9DThwBzLLjHvkBvJ63ITT9v7ERzePPF9jhvXyWb8sGNonIAqqkwEVvPWixMioyyqqqsmw6WBnf320oyjEgsKN3VrZNKodllmfPHYdz6LkD0iSeN4LGJ58pUjGla9NDRsa62e/e1/8fOCY3Qa/oJk2YIj6t7CCWLK+JI8OaQzA5//kU35lion3dHRXXpMLhlpwohPF9PvsKaIHcdpzqpt3Nr3UBrWqsb7Vx/Nka1PRUTosNGyleQX+LzVEinGxNJdn3A7HHmutVta6W9g/Y9moHSGiTF7ycZRiVVTe4998DMIrL6PtTlaxwtg3tvW8ZXfwh/fxa4zRWHU3ajgKA3J/lCdcfcdfOo/hguYzDR/ZMiMZOie2wBWhKdlpgvFJbFHMoH1Eq0a1mDmv07m8a+XMXHBek7p0IRR5giWrN9BZnoal/Vqw/k9WlEtw3qO7Mx0Rp0TCkB4lCMMeeuGNeie24CbTzmEdk1rB917KwVp6QkKjXJyx91rNg53+JK91Ik27lC+u1hWNQ7sbW2n0OWS6HnOfBZOe8w6btEtvmfaXnLHVcFRauJ/MfmFPj78ZQ1nNcyiDrB1+07qOn7j0/2HkVvwdqlbcHaXZrAAGrg8lnJqVeOT63pxzKjwGVHTOtn8taOA87u34t4zOoTtJyEi3NL3UG7pa+2LfV73cHVOQGjEIzM9jXFXH12axykXArGk2uR4uPaWK4n8MGPNOPYDVdU10+PnUaIjAr1ujJ0nLT0UPy6hOvcDG8f+S2I/1Ac/X8w9Hy9i4CsLAagp3t5HbuOx0y21e8FoTih8nD7tGtPKtUCrTY412mtk7442pJvl/dy8fnWa1QutcejY0rJhjDzzMDLShFv7HpLEJkT7Fn87rClvX9mDi3qUwdCeSMiRRH6gFeVVtS+qbZRyQm0clZsEpoIzV1puvPnG6sRr4C043DXVyc4M7l1x66ATuH38fD4YdCQ5taqxp6iE9vd8GZa/QY0slozshwjMXb2NO/uH75X95hXdWbttD+2a1mHF/8UIw7AfICIcc3AKF7IFSEiXvJdmHBWlqqqUVIL1GRVJYI3SUakN4aeCozQk+MNcETAUYwmOmhLdAexfp7bnwYlLaNmgejAC6ykdmnBut5ac2y20jrJ6Vjrj/nE0Kzfvgl2LwtIBJt0cimk0+oIu1KmeQe3sTNo1Tc2m9ZWeI4fAn8mqVGJ8v7EM3m4CQS4bHJjcPZLGVVe9FLs1V0aqtLB0kJ5hba2QbETkJFHBUSqEeD/8wKI2gF3EdjMVgauOP5DszDROat+E2+wtVa84to1n/u5tGtC9TQOI42Bx2pEHxM5QFRg4phwq8fKFTmDG0KSDtfWoV4DCVHlVdTzf2vK0qlEZVoJXFsq6T30CqI0jRbw3M7Ri25DGS75TGVJ4NwC/+oPbjpDbsAaPDLICp118dC7N61XnkcEdOb97K7q0ihOJs/tV0H5AeERdpWwcNjCxVcWJ+tUf2j8U+dhJuY6QXRFhM1O8Hkap8uiMozSIxP3hP/D5krDzB33W/uBHFoyhkKxg7KWGtSIXQzWvVz1sEV5Uqte39iZXyk5WLSjKh8GvxMmYhHF8b1GZ2lJRqKpqr6KCI4XkNqzB9j3FbN0dUlvtwNqprMMBdSL3jFAqjmE/wvr58fO596quDOh7FEJVVnuFSvT270sk9kP1+Y3neoJx/zhahUZlo36uFTY7UQKCo+mRsfPtDfRdUvYyKjhKg9+Hl/D4uqRL2LkxMOaSbjw2OHzzl66ty3EXMaViCAiOyyeGdrvLUNuCUjVQwVEa/MUMfH5aRPItxVeHnR/atDY5taoxsEtok8KVo06LGy1Wqcy4VFXValvutneth9v/iF5MUfYj1MZRGkp8nsmFhNzghvdvF1x/ISJceWwb+rRv4llO2Yfocw98ODQywm4yYSFSiur4ldSjgqM0+L0FhzNK7dUnhAfT+/fpZQtkqFQgToNrh7Pg8HMqri2KUglQVVVp8Bd7hjIva3hzZV+gsn/Hlb19qUZnXHsDFRyloaTYM1lf2SpApfdgqqJvYaX/XvYvVHCUBn+J5+zCrx/n/s0Jw7WDUhRUcMRni8fexX6dcVRJ2vat6BYkQBUVbP0fsbZzbtmjoltSJUip4BCRfiLyq4isEJHhHtdbi8i3IjJfRKaISAvHtUtFZLn9d6kjfYpd51z7r3Eqn4GnO0emRVVVVdEfrVKJqKLDl8bt4OIJkFk9fl6lzKTMq0pE0oHRwCnAGmCmiHxijFnsyPYo8Lox5jUROQl4CLhYRBoA9wLdsH4Js+2yW+1yFxpjZqWq7XExJVF+nio49k+qaGesKFFI5YyjO7DCGPO7MaYIeBc405WnAxDY33Sy4/rfgK+NMVtsYfE10C+FbS0XmtbRlcOKouz/pFJwNAdWO87X2GlO5gED7eOzgdoi0jCBsq/Yaqq7JUrQJxEZKiKzRGRWXl5eWZ7DEy+11Iy7+pT7fRRFUSobFW0cvxU4QUTmACcAa4GSOGUuNMYcARxn/13slckYM8YY080Y061Ro0bl2WZFUZQqTSoFx1qgpeO8hZ0WxBizzhgz0BjTGfiXnbYtVlljTOD/TuBtLJXYXkcN4Uqlot3pFd0CpQqRSsExE2grIm1EJAs4D/jEmUFEckSCGxvcCYy1jycBfUWkvojUB/oCk0QkQ0Ry7LKZwOnAwhQ+g6KEqMxjBV1fouxFUiY4jDE+4DosIbAEGGeMWSQiI0UksPFBb+BXEVkGNAEetMtuAe7HEj4zgZF2WjUsATIfmIs1C3kpVc8QC/WzqUJkWZtvIekV2w5FqSSkNMihMWYiMNGVdo/jeDwwPkrZsYRmIIG0XUDX8m9pOXLmc9BEAxruV5zzMsx5Ew7oGD+volQBNDpuKYlq4+h84d5tiJJ6ajeF428tn7qumQG7yt/Ljx7DYMmn0LJn+detKC5UcCjK3qRxe6B9+deb2wtGbC//ehXFg4p2x1UURVH2MVRwlBr1YlEUpWqigkNRFEVJChUcsfD7o15Sd1xFUaoqKjhiYaJHP3nwrCP2YkMURVEqDyo4YuGPLjg6t66/FxuiKIpSeVDBEYsYMw5FUZSqigqOWPh9MS6qV5WiKFUTFRyxiKGqUhRFqarEFRwicoYjgm3VIpbgcEYj7Xkt1GiY+vYoiqJUAhIRCEOA5SLysIi0S3WDKhWJ2jj6/R/c/ntq26IoilJJiCs4jDEXAZ2B34BXRWS6vS1r7ZS3rqJRVZWiKEoECamgjDE7sMKfvwscgLU/+C8icn0K21bxxJxxqHFcUZSqSSI2jgEiMgGYAmQC3Y0x/YGOwC2pbV4FY3R9uKIoiptEwqqfAzxhjPnemWiM2S0iV6SmWZUEEz3kiG7VqShKVSURwTECWB84EZHqQBNjzEpjzLepaljlQGcciqIobhKxcbwPOIfeJXZaXESkn4j8KiIrRGS4x/XWIvKtiMwXkSki0sJx7VIRWW7/XepI7yoiC+w6nxZJ4dA/pqpKZxyKolRNEhEcGcaYosCJfZwVr5CIpAOjgf5AB+B8EXFvxv0o8Lox5khgJPCQXbYBcC/QA+gO3CsigeBQzwNXAW3tv34JPEPpiKWqUhRFqaIkIjjyRGRA4EREzgQ2JVCuO7DCGPO7LWzeBc505ekA/M8+nuy4/jfga2PMFmPMVuBroJ+IHADUMcbMMMYY4HXgrATaUjpizTjUxqEoShUlEcFxNXCXiKwSkdXAHcA/EijXHFjtOF9jpzmZBwy0j88GaotIwxhlm9vHseoEwF5rMktEZuXl5SXQXA90xqEoihJBIgsAfzPG9MSaHbQ3xhxjjFlRTve/FThBROYAJwBrsWwoZcYYM8YY080Y061Ro0alraU8mqIoirJfkYhXFSJyGnAYkB2wRRtjRsYpthZo6ThvYacFMcasw55xiEgt4BxjzDYRWQv0dpWdYpdv4UoPq7NciTnjUFWVoihVk0QWAL6AFa/qeqzecjDQOoG6ZwJtRaSNiGQB5wGfuOrOcQRQvBMYax9PAvqKSH3bKN4XmGSMWQ/sEJGetjfVJcDHCbSldKiqSlEUJYJEbBzHGGMuAbYaY+4DjgYOiVfIGOMDrsMSAkuAccaYRSIy0mFs7w38KiLLgCbAg3bZLcD9WMJnJjDSTgO4BngZWIEVP+uLRB60VKhxXFEUJYJEVFUF9v/dItIM2IwVryouxpiJwERX2j2O4/FYMbC8yo4lNANxps8CDk/k/mVGZxyKoigRJDLj+FRE6gGPAL8AK4G3U9moyoM149jhGQhYZxyKolRNYs44bPvDt8aYbcAHIvIZkG2M2b5XWlfR2DMOvwoJRVGUIDFnHMYYP9bq78B5YZURGhC0cRjjITjUxqEoShUlEVXVtyJyTkpjQlVWAoKjgpuhKIpSmUhEcPwDK6hhoYjsEJGdIrIjxe2qHNiqKqOqKkVRlCBxvaqMMfv/FrFRseYa6lulKIoSIq7gEJHjvdLdGzvtlwSM4wZ1olIURbFJZB3HbY7jbKyot7OBk1LSospELFVVFTT5KIqiQGKqqjOc5yLSEngyZS2qTASN4yokFEVRAiRiHHezBmhf3g2plARnHF6oMFEUpWqSiI3jGUJ9ZxrQCWsFeRUgYBwvjXxVFEXZP0nExjHLcewD3jHG/Jii9lQuYsWqUhuHoihVlEQEx3igwBhTAtZe4iJSwxizO7VNqwQEVFVGVDOlKIpik9DKcaC647w68E1qmlPJMGH/XKgkURSlapKI4Mg2xuQHTuzjGqlrUiVCV44riqJEkIjg2CUiXQInItIV2JO6JlUmYrjjqo1DUZQqSiI2jpuA90VkHZZ+pinWVrL7PzHdcRVFUaomiSwAnCki7YBD7aRfjTHFqW1WJUFVVYqiKBHEVVWJyLVATWPMQmPMQqCWiFyTSOUi0k9EfhWRFSIy3ON6KxGZLCJzRGS+iJxqp2eJyCsiskBE5olIb0eZKXadc+2/xgk/bbLEXDmuwkRRlKpJIjaOq+wdAAEwxmwFropXSETSsTaB6g90AM4XkQ6ubP8GxhljOgPnAc8F7mnf6wjgFOAxezfCABcaYzrZfxsTeIbSoTMORVGUCBIRHOnOTZxsgZCVQLnuwApjzO/GmCLgXeBMVx4D1LGP6wLr7OMOwP8AbMGwDeiWwD3LGWvGMcJ3aeQlNY4rilJFSURwfAm8JyJ9RKQP8A7wRQLlmgOrHedr7DQnI4CLRGQNMBG43k6fBwwQkQwRaQN0BVo6yr1iq6nujrYzoYgMFZFZIjIrLy8vgeZ6YM84Npp6pSuvKIqyH5KI4LgDa/R/tf23gPAFgWXhfOBVY0wL4FTgDVslNRZL0MzCisQ7DSixy1xoq7COs/8u9qrYGDPGGNPNGNOtUaNGpWudCcSqUhuHoihKgLiCwxjjB34CVmKpn04CliRQ91rCZwkt7DQnVwDj7PtMx9rvI8cY4zPG3GzbMM4E6gHL7Hxr7f87gbftNqUGtXEoiqJEEFVwiMghInKviCwFngFWARhjTjTGPJtA3TOBtiLSRkSysIzfn7jyrAL62PdrjyU48kSkhojUtNNPAXzGmMW26irHTs8ETgcWJvG8yRHLq0ptHIqiVFFireNYCkwFTjfGrAAQkZsTrdgY4xOR64BJQDow1hizSERGArOMMZ8AtwAv2fUa4DJjjLFdbCeJiB9rlhJQR1Wz0zPtOr8BXkrieZMklqpKURSlahJLcAzEmiVMFpEvsbyikupBjTETsYzezrR7HMeLgV4e5VYSWnDoTN+FZSjfO8RUVakwURSlahJVVWWM+cgYcx7QDpiMFXqksYg8LyJ991YDK5SYxnFFUZSqSSLG8V3GmLftvcdbAHOwPK32f+wZR+9Dm1ZwQxRFUSoPSe2JaozZaru59klVgyoVgR0APTVVOgtRFKVqoptpx8RSVVmL5RVFURRQwREbe8YRZW36Xm2KoihKZUEFRyyMzjgURVHcqOCIRcDGkebxMamNQ1GUKooKjljkLQXAn5ZIMGBFUZSqgQqOWGTV5OO0kynMqONxUWcciqJUTVRwxOLkETyYdrWnpkpRFKWqol1iHPwIUbb8UBRFqZKo4IiDMYY0XQCoKIoSRAVHHPzGkKZCQlEUJYgKjjj4DVEEhwoTRVGqJio44uA3RrVSiqIoDlRwxMFEm3GoNFEUpYqigiMO/mjGcUVRlCqKCo44qHFcURQlnJQKDhHpJyK/isgKERnucb2ViEwWkTkiMl9ETrXTs0TkFRFZICLzRKS3o0xXO32FiDwtKV5k4TfoOg5FURQHKRMcYoWUHQ30BzoA54tIB1e2fwPjjDGdsfY3f85OvwrAGHMEcArwmIgE2vq8fb2t/dcvVc9gt0HXcSiKojhI5YyjO7DCGPO7MaYIeBc405XHAIFAUHWBdfZxB+B/AMaYjcA2oJuIHADUMcbMMMYY4HXgrBQ+Qwx3XEVRlKpJKgVHc2C143yNneZkBHCRiKwBJgLX2+nzgAEikiEibYCuQEu7/Jo4dZYrahxXFEUJp6KN4+cDrxpjWgCnAm/YKqmxWEJhFvAkMA0oSaZiERkqIrNEZFZeXl6pGmeMwUS1cag0URSlapKRwrrXYs0SArSw05xcgW2jMMZMF5FsIMdWT90cyCQi04BlwFa7nlh1Ytc3BhgD0K1bN1OaB7A3AFRVlaIoioNUzjhmAm1FpI2IZGEZvz9x5VkF9AEQkfZANpAnIjVEpKadfgrgM8YsNsasB3aISE/bm+oS4ONUPYDflhxqHFcURQmRshmHMcYnItcBk4B0YKwxZpGIjARmGWM+AW4BXhKRm7EM5ZcZY4yINAYmiYgfa0ZxsaPqa4BXgerAF/ZfSvAHZhxq5FAURQmSSlUVxpiJWEZvZ9o9juPFQC+PciuBQ6PUOQs4vFwbGoXAjMN7cqHCRFGUqklFG8crNWrjUBRFiUQFRwzUxqEoihKJCo4YhASHCglFUZQAKjhiEDCO6zoORVGUECo4YmBiqaoURVGqKCo4YlDiV1WVoiiKGxUcMQiu41DjuKIoShAVHDEwwXUcKiQURVECqOCIQWDGke495dirbVEURaksqOCIQXDleAW3Q1EUpTKhgiMGvhJLcGSke3xMqr5SFKWKooIjBkUl1hYgWRn6MSmKogTQHjEGRT5rxpGVrjYORVGUACo4YlBU4gd0xqEoiuJEe8QYFPlswZGeXsEtURRFqTyo4IhBUHB4zTjUOK4oShVFBUcM1DiuKIoSifaIMQgYxzPdxvF6rVDjuKIoVZWUbh27rxMwjldzzjj+8T3UaVFBLVIURal4UjrjEJF+IvKriKwQkeEe11uJyGQRmSMi80XkVDs9U0ReE5EFIrJERO50lFlpp88VkVmpbL+ncfyAjlCzodo4FEWpsqRsxiEi6cBo4BRgDTBTRD4xxix2ZPs3MM4Y87yIdAAmArnAYKCaMeYIEakBLBaRd4wxK+1yJxpjNqWq7QFiGscVRVGqKKlUVXUHVhhjfgcQkXeBMwGn4DBAHfu4LrDOkV5TRDKA6kARsCOFbfXE57cER0a6QJdLoXiP46rOOBTFTXFxMWvWrKGgoKCim6IkQXZ2Ni1atCAzMzOh/KkUHM2B1Y7zNUAPV54RwFcicj1QEzjZTh+PJWTWAzWAm40xW+xrxi5jgBeNMWO8bi4iQ4GhAK1atSrVAwQ2ckoXgQFPl6oORalKrFmzhtq1a5Obm6vbEewjGGPYvHkza9asoU2bNgmVqWgdzPnAq8aYFsCpwBsikoY1WykBmgFtgFtE5EC7zLHGmC5Af+BaETneq2JjzBhjTDdjTLdGjRqVqnHBjZy8wqrrj0JRIigoKKBhw4YqNPYhRISGDRsmNUtMpeBYC7R0nLew05xcAYwDMMZMB7KBHOAC4EtjTLExZiPwI9DNzrfW/r8RmIAlZFKC3697jitKsqjQ2PdI9jtLpeCYCbQVkTYikgWcB3ziyrMK6AMgIu2xBEeenX6SnV4T6AksFZGaIlLbkd4XWJiqByix9+Pw3shJURSlapIywWGM8QHXAZOAJVjeU4tEZKSIDLCz3QJcJSLzgHeAy4y1X+tooJaILMISQK8YY+YDTYAf7Pw/A58bY75M1TOUBGccGh1XUfYFtm3b3UG1gwAAENVJREFUxnPPPVeqsqeeeirbtm2Lmeeee+7hm2++KVX9sXj11Ve57rrrYuaZMmUK06ZNK/d7l4aULgA0xkzEcrF1pt3jOF4M9PIol4/lkutO/x3oWP4t9cbojENR9ikCguOaa66JuObz+cjIiN7lTZw4Meq1ACNHjixT+8rClClTqFWrFsccc0yFtSGArhyPgb1w3HvGoXpcRYnJfZ8uYvG68vWi79CsDveecVjU68OHD+e3336jU6dOnHLKKZx22mncfffd1K9fn6VLl7Js2TLOOussVq9eTUFBATfeeCNDhw4FIDc3l1mzZpGfn0///v059thjmTZtGs2bN+fjjz+mevXqXHbZZZx++ukMGjSI3NxcLr30Uj799FOKi4t5//33adeuHXl5eVxwwQWsW7eOo48+mq+//prZs2eTk5MT1tZXXnmFhx56iHr16tGxY0eqVasGwKeffsoDDzxAUVERDRs25K233mLPnj288MILpKen8+abb/LMM8+wbdu2iHxNmjQp1887GhXtVVWpCdg4dMKhKPsGo0aN4qCDDmLu3Lk88sgjAPzyyy889dRTLFu2DICxY8cye/ZsZs2axdNPP83mzZsj6lm+fDnXXnstixYtol69enzwwQee98vJyeGXX35h2LBhPProowDcd999nHTSSSxatIhBgwaxatWqiHLr16/n3nvv5ccff+SHH35g8eLQ8rZjjz2WGTNmMGfOHM477zwefvhhcnNzufrqq7n55puZO3cuxx13nGe+vYXOOGLg9xvSJIrHgc44FCUmsWYGe5Pu3buHrU94+umnmTBhAgCrV69m+fLlNGzYMKxMmzZt6NSpEwBdu3Zl5cqVnnUPHDgwmOfDDz8E4IcffgjW369fP+rXrx9R7qeffqJ3794ElgoMGTIkKNjWrFnDkCFDWL9+PUVFRVHXViSaLxXojCMGfmOiGMYVRdlXqFmzZvB4ypQpfPPNN0yfPp158+bRuXNnz/ULAbURQHp6Oj6fz7PuQL5YeZLl+uuv57rrrmPBggW8+OKLUddXJJovFajgiEGJMd6L/xRFqZTUrl2bnTt3Rr2+fft26tevT40aNVi6dCkzZswo9zb06tWLcePGAfDVV1+xdevWiDw9evTgu+++Y/PmzUH7iLONzZs3B+C1114LprufLVq+vYEKjhj4/cYKN6Ioyj5Bw4YN6dWrF4cffji33XZbxPV+/frh8/lo3749w4cPp2fPnuXehnvvvZevvvqKww8/nPfff5+mTZtSu3btsDwHHHAAI0aM4Oijj6ZXr160b98+eG3EiBEMHjyYrl27hhnUzzjjDCZMmECnTp2YOnVq1Hx7Awm4nO7PdOvWzcyalXwE9vs/W8x7M1ez8L6/eWcYUdf+v70MrVOU/YclS5aEdYJVkcLCQtLT08nIyGD69OkMGzaMuXPnVnSz4uL13YnIbGNMN3deNY7HoMRv1AauKEpSrFq1inPPPRe/309WVhYvvfRSRTep3FHBEQO/Mbr4T1GUpGjbti1z5syp6GakFLVxxKBEbRyKoigRqOCIgd9ECamuKIpShVHBEYPAAkBFURQlhAqOGJQYVVUpiqK4UcERA79fFwAqyv5OrVq1AFi3bh2DBg3yzNO7d2/iufQ/+eST7N69O3ieSJj20hBobzTKElo+UVRwxEC9qhSl6tCsWTPGjx9f6vJuwTFx4kTq1atXHk1Lir0hONQdNwYlJtomToqixOWL4fDXgvKts+kR0H9U1MvDhw+nZcuWXHvttYC1CrtWrVpcffXVnHnmmWzdupXi4mIeeOABzjzzzLCyK1eu5PTTT2fhwoXs2bOHyy+/nHnz5tGuXTv27NkTzDds2DBmzpzJnj17GDRoEPfddx9PP/0069at48QTTyQnJ4fJkycHw7Tn5OTw+OOPM3bsWACuvPJKbrrpJlauXBk1fLuTP/74gwsuuID8/PywNgfO3c/kDi1/7733xn32ZFHBEQM1jivKvsWQIUO46aabgoJj3LhxTJo0iezsbCZMmECdOnXYtGkTPXv2ZMCAAVH32n7++eepUaMGS5YsYf78+XTp0iV47cEHH6RBgwaUlJTQp08f5s+fzw033MDjjz/O5MmTI8J/zJ49m1deeYWffvoJYww9evTghBNOoH79+ixfvpx33nmHl156iXPPPZcPPviAiy66KKz8jTfeyLBhw7jkkksYPXp0MD3aM40aNYqFCxcGV6v7fL6knj0RVHDEoMQfR1V17D/hkH57r0GKsi8RY2aQKjp37szGjRtZt24deXl51K9fn5YtW1JcXMxdd93F999/T1paGmvXrmXDhg00bdrUs57vv/+eG264AYAjjzySI488Mnht3LhxjBkzBp/Px/r161m8eHHYdTc//PADZ599djBK78CBA5k6dSoDBgxIKHz7jz/+GNwP5OKLL+aOO+4ArB1KvZ7JTbR80Z49EVIqOESkH/AUkA68bIwZ5breCngNqGfnGW6MmSgimcDLQBe7ja8bYx5KpM7yJG5Y9ZPvTdWtFUUpJYMHD2b8+PH89ddfDBkyBIC33nqLvLw8Zs+eTWZmJrm5uaUKQ/7HH3/w6KOPMnPmTOrXr89ll11WpnDm7vDtTpWYE6/ZQaLPVF7P7iRlxnERSQdGA/2BDsD5ItLBle3fwDhjTGfgPCBg0RkMVDPGHAF0Bf4hIrkJ1llu6H4cirLvMWTIEN59913Gjx/P4MGDASsEeePGjcnMzGTy5Mn8+eefMes4/vjjefvttwFYuHAh8+fPB2DHjh3UrFmTunXrsmHDBr744otgmWgh3Y877jg++ugjdu/eza5du5gwYQLHHXdcws/Tq1cv3n33XcASAgGiPZNX+PVknj0RUulV1R1YYYz53RhTBLwLuC0yBqhjH9cF1jnSa4pIBlAdKAJ2JFhnuRFXVaUoSqXjsMMOY+fOnTRv3pwDDjgAgAsvvJBZs2ZxxBFH8Prrr9OuXbuYdQwbNoz8/Hzat2/PPffcQ9euXQHo2LEjnTt3pl27dlxwwQX06tUrWGbo0KH069ePE088MayuLl26cNlll9G9e3d69OjBlVdeSefOnRN+nqeeeorRo0dzxBFHsHbt2mB6tGdyh5ZP9tkTIWVh1UVkENDPGHOlfX4x0MMYc50jzwHAV0B9oCZwsjFmtq2qegPoA9QAbjbGjEmkTkfdQ4GhAK1atepaGik7evIK8gt93NGv7B+0olQFNKz6vsu+FFb9fOBVY8xjInI08IaIHI41sygBmmEJlaki8k0yFRtjxgBjwNqPozSNu/bEg0tTTFEUZb8mlYJjLdDScd7CTnNyBdAPwBgzXUSygRzgAuBLY0wxsFFEfgS6AasTqFNRFEVJIam0ccwE2opIGxHJwjJ+f+LKswpLHYWItAeygTw7/SQ7vSbQE1iaYJ2KolQgVWFX0f2NZL+zlAkOY4wPuA6YBCzB8p5aJCIjRWSAne0W4CoRmQe8A1xmrCcYDdQSkUVYwuIVY8z8aHWm6hkURUmO7OxsNm/erMJjH8IYw+bNm8nOzk64jO45rihKuVFcXMyaNWvKvE5A2btkZ2fTokULMjMzw9Irq3FcUZT9iMzMTNq0aVPRzVBSjEbHVRRFUZJCBYeiKIqSFCo4FEVRlKSoEsZxEckDShugJQfYVI7Nqez8f3v3FytVdcVx/PsTKogY/jRqEI1IJW1po4CmSm0TE+s/0qgPmIZaS2wTX0yEpkkLUUPaNxNTpIlRmv5RK1GjYkt48c/VkPAg/1pqKUq5qImXQGkaSkuTNhaWD3sNHS5SOHfm3sM98/skJ3fOPjuTvWbNzZpz5szevRRvL8UKvRVvL8UKwxfvpRFx/uDGnigcnZC09ZPuKmiqXoq3l2KF3oq3l2KFkY/Xl6rMzKwSFw4zM6vEhePUflb3AEZYL8XbS7FCb8XbS7HCCMfr7zjMzKwSn3GYmVklLhxmZlaJC8f/IekWSbsk9UtaVvd4OiXpEklvStop6U+SlmT7VEmvSdqdf6dkuyT9NON/W9K8eiOoTtIYSb+XtD73L5O0KWN6PqfnR9K43O/P4zPqHPdQSJos6UVJ70p6R9L8puZW0vfyPbxD0rOSxjcpt5J+KemApB1tbZVzKWlx9t8taXG3xufCcRKSxlCmd78VmA0skjS73lF17L/A9yNiNmWNk/sypmVAX0TMAvpyH0rss3K7F3h85IfcsSWUKfhbHgZWRsTlwEHKYmLk34PZvjL7jTarKAugfQ64khJ343IraTpwP3B1RHwRGENZm6dJuX2SXOSuTaVcSpoKrACuoayquqJVbDoWEd4+YQPmA6+07S8Hltc9ri7H+FvgRmAXMC3bpgG78vFqYFFb/2P9RsNGWSGyj7Io2HpAlF/Xjh2cY8oaL/Pz8djsp7pjqBDrJOD9wWNuYm6B6ZTVQKdmrtYDNzctt8AMYMdQc0lZmnt1W/tx/TrZfMZxcq03Z8tAtjVCnq7PBTYBF0bEvjy0H7gwH4/21+BR4AfA0dz/NPD3KAuCwfHxHIs1jx/K/qPFZZTVM3+Vl+Z+nqtnNi63EbEXeISyUug+Sq620dzctlTN5bDl2IWjB0maCLwELI2If7Qfi/LRZNTfoy3p68CBiNhW91hGyFhgHvB4RMwF/sX/LmUAjcrtFOB2SrG8CDiXEy/rNFrduXThOLm9wCVt+xdn26gm6VOUorEmItZm818kTcvj04AD2T6aX4PrgNskfQA8R7lctQqYLKm1gFl7PMdizeOTgL+N5IA7NAAMRMSm3H+RUkiamNuvAe9HxF8j4iNgLSXfTc1tS9VcDluOXThObgswK+/UOJvy5du6msfUEUkCfgG8ExE/aTu0DmjdcbGY8t1Hq/3bedfGtcChtlPlM1pELI+IiyNiBiV3b0TEXcCbwMLsNjjW1muwMPuPmk/nEbEf+FDSZ7PpBmAnDcwt5RLVtZIm5Hu6FWsjc9umai5fAW6SNCXP0m7Kts7V/QXQmbwBC4A/A3uAB+oeTxfi+Qrl9PZtYHtuCyjXe/uA3cDrwNTsL8qdZXuAP1LuYqk9jiHEfT2wPh/PBDYD/cALwLhsH5/7/Xl8Zt3jHkKcc4Ctmd/fAFOamlvgR8C7wA7g18C4JuUWeJby/c1HlLPJ7w4ll8B3Mu5+4J5ujc9TjpiZWSW+VGVmZpW4cJiZWSUuHGZmVokLh5mZVeLCYWZmlbhwmJ3hJF3fmt3X7EzgwmFmZpW4cJh1iaRvSdosabuk1bkWyGFJK3PtiD5J52ffOZLeyvUTXm5bW+FySa9L+oOk30n6TD79xLa1NtbkL6bNauHCYdYFkj4PfAO4LiLmAEeAuygT8G2NiC8AGyjrIwA8DfwwIq6g/Nq31b4GeCwirgS+TPn1MJSZjJdS1oaZSZmbyawWY0/dxcxOww3AVcCWPBk4hzIJ3VHg+ezzDLBW0iRgckRsyPangBcknQdMj4iXASLi3wD5fJsjYiD3t1PWatg4/GGZnciFw6w7BDwVEcuPa5QeGtRvqHP8/Kft8RH8v2s18qUqs+7oAxZKugCOrQ99KeV/rDVj6zeBjRFxCDgo6avZfjewISL+CQxIuiOfY5ykCSMahdlp8KcWsy6IiJ2SHgRelXQWZVbT+ygLKn0pjx2gfA8CZVrsJ7IwvAfck+13A6sl/Tif484RDMPstHh2XLNhJOlwREysexxm3eRLVWZmVonPOMzMrBKfcZiZWSUuHGZmVokLh5mZVeLCYWZmlbhwmJlZJR8D3lAUtch33vwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Crvu5cKh3CwA",
        "outputId": "3c03a0ce-ed5d-4ed8-84d3-caa776bb2760"
      },
      "source": [
        "model_o = Sequential()\n",
        "model_o.add(Dense(64, input_dim = 20,activation='relu'))\n",
        "model_o.add(Dense(32,activation='relu'))\n",
        "model_o.add(Dense(16,activation='relu'))\n",
        "model_o.add(Dense(8,activation='relu'))\n",
        "model_o.add(Dense(4,activation='relu'))\n",
        "model_o.add(Dense(1,activation='sigmoid'))\n",
        "model_o.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_o.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=1024)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1024\n",
            "901/901 [==============================] - 3s 2ms/step - loss: 0.3220 - accuracy: 0.8835 - val_loss: 0.2106 - val_accuracy: 0.9114\n",
            "Epoch 2/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2138 - accuracy: 0.9092 - val_loss: 0.2022 - val_accuracy: 0.9138\n",
            "Epoch 3/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2084 - accuracy: 0.9095 - val_loss: 0.1912 - val_accuracy: 0.9148\n",
            "Epoch 4/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9110 - val_loss: 0.1910 - val_accuracy: 0.9133\n",
            "Epoch 5/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9070 - val_loss: 0.1881 - val_accuracy: 0.9147\n",
            "Epoch 6/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9118 - val_loss: 0.1844 - val_accuracy: 0.9144\n",
            "Epoch 7/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9086 - val_loss: 0.1853 - val_accuracy: 0.9149\n",
            "Epoch 8/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9124 - val_loss: 0.1842 - val_accuracy: 0.9146\n",
            "Epoch 9/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9102 - val_loss: 0.1889 - val_accuracy: 0.9120\n",
            "Epoch 10/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9128 - val_loss: 0.1942 - val_accuracy: 0.9113\n",
            "Epoch 11/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9135 - val_loss: 0.1818 - val_accuracy: 0.9147\n",
            "Epoch 12/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9129 - val_loss: 0.1831 - val_accuracy: 0.9140\n",
            "Epoch 13/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9129 - val_loss: 0.1816 - val_accuracy: 0.9148\n",
            "Epoch 14/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9144 - val_loss: 0.1807 - val_accuracy: 0.9145\n",
            "Epoch 15/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9122 - val_loss: 0.1805 - val_accuracy: 0.9144\n",
            "Epoch 16/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9127 - val_loss: 0.1816 - val_accuracy: 0.9148\n",
            "Epoch 17/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9142 - val_loss: 0.1904 - val_accuracy: 0.9121\n",
            "Epoch 18/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9163 - val_loss: 0.1774 - val_accuracy: 0.9148\n",
            "Epoch 19/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9161 - val_loss: 0.1842 - val_accuracy: 0.9147\n",
            "Epoch 20/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9139 - val_loss: 0.1787 - val_accuracy: 0.9146\n",
            "Epoch 21/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9145 - val_loss: 0.1786 - val_accuracy: 0.9153\n",
            "Epoch 22/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9162 - val_loss: 0.1781 - val_accuracy: 0.9169\n",
            "Epoch 23/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9172 - val_loss: 0.1787 - val_accuracy: 0.9139\n",
            "Epoch 24/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9174 - val_loss: 0.1814 - val_accuracy: 0.9162\n",
            "Epoch 25/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1767 - accuracy: 0.9182 - val_loss: 0.1837 - val_accuracy: 0.9149\n",
            "Epoch 26/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9178 - val_loss: 0.1825 - val_accuracy: 0.9135\n",
            "Epoch 27/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9172 - val_loss: 0.1836 - val_accuracy: 0.9138\n",
            "Epoch 28/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9148 - val_loss: 0.1779 - val_accuracy: 0.9155\n",
            "Epoch 29/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9160 - val_loss: 0.1769 - val_accuracy: 0.9148\n",
            "Epoch 30/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1767 - accuracy: 0.9166 - val_loss: 0.1781 - val_accuracy: 0.9148\n",
            "Epoch 31/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1771 - accuracy: 0.9197 - val_loss: 0.1779 - val_accuracy: 0.9148\n",
            "Epoch 32/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1764 - accuracy: 0.9183 - val_loss: 0.1795 - val_accuracy: 0.9159\n",
            "Epoch 33/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1751 - accuracy: 0.9200 - val_loss: 0.1820 - val_accuracy: 0.9148\n",
            "Epoch 34/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1751 - accuracy: 0.9206 - val_loss: 0.1801 - val_accuracy: 0.9151\n",
            "Epoch 35/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1734 - accuracy: 0.9204 - val_loss: 0.1802 - val_accuracy: 0.9152\n",
            "Epoch 36/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1732 - accuracy: 0.9201 - val_loss: 0.1864 - val_accuracy: 0.9105\n",
            "Epoch 37/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1712 - accuracy: 0.9209 - val_loss: 0.1805 - val_accuracy: 0.9140\n",
            "Epoch 38/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1695 - accuracy: 0.9210 - val_loss: 0.1772 - val_accuracy: 0.9165\n",
            "Epoch 39/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1700 - accuracy: 0.9203 - val_loss: 0.1819 - val_accuracy: 0.9159\n",
            "Epoch 40/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1680 - accuracy: 0.9208 - val_loss: 0.1863 - val_accuracy: 0.9109\n",
            "Epoch 41/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1697 - accuracy: 0.9214 - val_loss: 0.1817 - val_accuracy: 0.9146\n",
            "Epoch 42/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1718 - accuracy: 0.9191 - val_loss: 0.1800 - val_accuracy: 0.9159\n",
            "Epoch 43/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1646 - accuracy: 0.9234 - val_loss: 0.1811 - val_accuracy: 0.9154\n",
            "Epoch 44/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1663 - accuracy: 0.9227 - val_loss: 0.1802 - val_accuracy: 0.9157\n",
            "Epoch 45/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1690 - accuracy: 0.9227 - val_loss: 0.1814 - val_accuracy: 0.9142\n",
            "Epoch 46/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1684 - accuracy: 0.9236 - val_loss: 0.1805 - val_accuracy: 0.9143\n",
            "Epoch 47/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1672 - accuracy: 0.9218 - val_loss: 0.1879 - val_accuracy: 0.9166\n",
            "Epoch 48/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1655 - accuracy: 0.9236 - val_loss: 0.1851 - val_accuracy: 0.9108\n",
            "Epoch 49/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1643 - accuracy: 0.9243 - val_loss: 0.1811 - val_accuracy: 0.9151\n",
            "Epoch 50/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1685 - accuracy: 0.9239 - val_loss: 0.1864 - val_accuracy: 0.9151\n",
            "Epoch 51/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1659 - accuracy: 0.9225 - val_loss: 0.1864 - val_accuracy: 0.9126\n",
            "Epoch 52/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1632 - accuracy: 0.9251 - val_loss: 0.1809 - val_accuracy: 0.9158\n",
            "Epoch 53/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1675 - accuracy: 0.9226 - val_loss: 0.1814 - val_accuracy: 0.9148\n",
            "Epoch 54/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1668 - accuracy: 0.9224 - val_loss: 0.2009 - val_accuracy: 0.9035\n",
            "Epoch 55/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1663 - accuracy: 0.9239 - val_loss: 0.1860 - val_accuracy: 0.9118\n",
            "Epoch 56/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1604 - accuracy: 0.9262 - val_loss: 0.1828 - val_accuracy: 0.9166\n",
            "Epoch 57/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1568 - accuracy: 0.9279 - val_loss: 0.1893 - val_accuracy: 0.9105\n",
            "Epoch 58/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1591 - accuracy: 0.9272 - val_loss: 0.1845 - val_accuracy: 0.9134\n",
            "Epoch 59/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1606 - accuracy: 0.9263 - val_loss: 0.1915 - val_accuracy: 0.9117\n",
            "Epoch 60/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1623 - accuracy: 0.9241 - val_loss: 0.1882 - val_accuracy: 0.9115\n",
            "Epoch 61/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1588 - accuracy: 0.9290 - val_loss: 0.1840 - val_accuracy: 0.9141\n",
            "Epoch 62/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1603 - accuracy: 0.9251 - val_loss: 0.1920 - val_accuracy: 0.9108\n",
            "Epoch 63/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1588 - accuracy: 0.9274 - val_loss: 0.1837 - val_accuracy: 0.9155\n",
            "Epoch 64/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1579 - accuracy: 0.9269 - val_loss: 0.1891 - val_accuracy: 0.9141\n",
            "Epoch 65/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1586 - accuracy: 0.9254 - val_loss: 0.1839 - val_accuracy: 0.9168\n",
            "Epoch 66/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1618 - accuracy: 0.9247 - val_loss: 0.1863 - val_accuracy: 0.9144\n",
            "Epoch 67/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1590 - accuracy: 0.9250 - val_loss: 0.1866 - val_accuracy: 0.9115\n",
            "Epoch 68/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1551 - accuracy: 0.9294 - val_loss: 0.1867 - val_accuracy: 0.9118\n",
            "Epoch 69/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1602 - accuracy: 0.9257 - val_loss: 0.1900 - val_accuracy: 0.9103\n",
            "Epoch 70/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1524 - accuracy: 0.9300 - val_loss: 0.1886 - val_accuracy: 0.9092\n",
            "Epoch 71/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1578 - accuracy: 0.9284 - val_loss: 0.1945 - val_accuracy: 0.9071\n",
            "Epoch 72/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1530 - accuracy: 0.9290 - val_loss: 0.1908 - val_accuracy: 0.9137\n",
            "Epoch 73/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1581 - accuracy: 0.9260 - val_loss: 0.1892 - val_accuracy: 0.9122\n",
            "Epoch 74/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1532 - accuracy: 0.9300 - val_loss: 0.1909 - val_accuracy: 0.9125\n",
            "Epoch 75/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1590 - accuracy: 0.9264 - val_loss: 0.1919 - val_accuracy: 0.9127\n",
            "Epoch 76/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1547 - accuracy: 0.9307 - val_loss: 0.1950 - val_accuracy: 0.9134\n",
            "Epoch 77/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1570 - accuracy: 0.9274 - val_loss: 0.1962 - val_accuracy: 0.9080\n",
            "Epoch 78/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1529 - accuracy: 0.9300 - val_loss: 0.1919 - val_accuracy: 0.9130\n",
            "Epoch 79/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1517 - accuracy: 0.9297 - val_loss: 0.1949 - val_accuracy: 0.9080\n",
            "Epoch 80/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1517 - accuracy: 0.9301 - val_loss: 0.1914 - val_accuracy: 0.9126\n",
            "Epoch 81/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1494 - accuracy: 0.9327 - val_loss: 0.1941 - val_accuracy: 0.9095\n",
            "Epoch 82/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1533 - accuracy: 0.9297 - val_loss: 0.1988 - val_accuracy: 0.9069\n",
            "Epoch 83/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1563 - accuracy: 0.9296 - val_loss: 0.1972 - val_accuracy: 0.9073\n",
            "Epoch 84/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1493 - accuracy: 0.9326 - val_loss: 0.1965 - val_accuracy: 0.9081\n",
            "Epoch 85/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1524 - accuracy: 0.9295 - val_loss: 0.2000 - val_accuracy: 0.9072\n",
            "Epoch 86/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1469 - accuracy: 0.9309 - val_loss: 0.1954 - val_accuracy: 0.9103\n",
            "Epoch 87/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1538 - accuracy: 0.9288 - val_loss: 0.1999 - val_accuracy: 0.9050\n",
            "Epoch 88/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1504 - accuracy: 0.9320 - val_loss: 0.1946 - val_accuracy: 0.9131\n",
            "Epoch 89/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1470 - accuracy: 0.9318 - val_loss: 0.1972 - val_accuracy: 0.9134\n",
            "Epoch 90/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1475 - accuracy: 0.9334 - val_loss: 0.1996 - val_accuracy: 0.9116\n",
            "Epoch 91/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1463 - accuracy: 0.9328 - val_loss: 0.1982 - val_accuracy: 0.9096\n",
            "Epoch 92/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1478 - accuracy: 0.9341 - val_loss: 0.1981 - val_accuracy: 0.9082\n",
            "Epoch 93/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1433 - accuracy: 0.9361 - val_loss: 0.2033 - val_accuracy: 0.9093\n",
            "Epoch 94/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1448 - accuracy: 0.9345 - val_loss: 0.2012 - val_accuracy: 0.9111\n",
            "Epoch 95/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1428 - accuracy: 0.9338 - val_loss: 0.2024 - val_accuracy: 0.9070\n",
            "Epoch 96/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1472 - accuracy: 0.9328 - val_loss: 0.1976 - val_accuracy: 0.9084\n",
            "Epoch 97/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1449 - accuracy: 0.9366 - val_loss: 0.2021 - val_accuracy: 0.9079\n",
            "Epoch 98/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1427 - accuracy: 0.9336 - val_loss: 0.2027 - val_accuracy: 0.9113\n",
            "Epoch 99/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1465 - accuracy: 0.9339 - val_loss: 0.2041 - val_accuracy: 0.9033\n",
            "Epoch 100/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1425 - accuracy: 0.9355 - val_loss: 0.2071 - val_accuracy: 0.9051\n",
            "Epoch 101/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1453 - accuracy: 0.9351 - val_loss: 0.2029 - val_accuracy: 0.9085\n",
            "Epoch 102/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1410 - accuracy: 0.9360 - val_loss: 0.2049 - val_accuracy: 0.9063\n",
            "Epoch 103/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1439 - accuracy: 0.9355 - val_loss: 0.2048 - val_accuracy: 0.9056\n",
            "Epoch 104/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1403 - accuracy: 0.9374 - val_loss: 0.2061 - val_accuracy: 0.9078\n",
            "Epoch 105/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1430 - accuracy: 0.9361 - val_loss: 0.2068 - val_accuracy: 0.9086\n",
            "Epoch 106/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1424 - accuracy: 0.9340 - val_loss: 0.2086 - val_accuracy: 0.9110\n",
            "Epoch 107/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1364 - accuracy: 0.9382 - val_loss: 0.2039 - val_accuracy: 0.9067\n",
            "Epoch 108/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1412 - accuracy: 0.9355 - val_loss: 0.2097 - val_accuracy: 0.9034\n",
            "Epoch 109/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1390 - accuracy: 0.9393 - val_loss: 0.2051 - val_accuracy: 0.9114\n",
            "Epoch 110/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1417 - accuracy: 0.9368 - val_loss: 0.2106 - val_accuracy: 0.9057\n",
            "Epoch 111/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1417 - accuracy: 0.9383 - val_loss: 0.2091 - val_accuracy: 0.9088\n",
            "Epoch 112/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1388 - accuracy: 0.9379 - val_loss: 0.2138 - val_accuracy: 0.9031\n",
            "Epoch 113/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1391 - accuracy: 0.9365 - val_loss: 0.2168 - val_accuracy: 0.9080\n",
            "Epoch 114/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1409 - accuracy: 0.9365 - val_loss: 0.2116 - val_accuracy: 0.9050\n",
            "Epoch 115/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1375 - accuracy: 0.9357 - val_loss: 0.2128 - val_accuracy: 0.9029\n",
            "Epoch 116/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1405 - accuracy: 0.9367 - val_loss: 0.2126 - val_accuracy: 0.9090\n",
            "Epoch 117/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1335 - accuracy: 0.9396 - val_loss: 0.2140 - val_accuracy: 0.9011\n",
            "Epoch 118/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1370 - accuracy: 0.9386 - val_loss: 0.2137 - val_accuracy: 0.9044\n",
            "Epoch 119/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1350 - accuracy: 0.9384 - val_loss: 0.2113 - val_accuracy: 0.9093\n",
            "Epoch 120/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1353 - accuracy: 0.9403 - val_loss: 0.2196 - val_accuracy: 0.9045\n",
            "Epoch 121/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1339 - accuracy: 0.9399 - val_loss: 0.2189 - val_accuracy: 0.9023\n",
            "Epoch 122/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1396 - accuracy: 0.9380 - val_loss: 0.2156 - val_accuracy: 0.9045\n",
            "Epoch 123/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1352 - accuracy: 0.9385 - val_loss: 0.2175 - val_accuracy: 0.9011\n",
            "Epoch 124/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1347 - accuracy: 0.9387 - val_loss: 0.2245 - val_accuracy: 0.9009\n",
            "Epoch 125/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1336 - accuracy: 0.9415 - val_loss: 0.2196 - val_accuracy: 0.9074\n",
            "Epoch 126/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1367 - accuracy: 0.9408 - val_loss: 0.2173 - val_accuracy: 0.9028\n",
            "Epoch 127/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1333 - accuracy: 0.9399 - val_loss: 0.2149 - val_accuracy: 0.9083\n",
            "Epoch 128/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1313 - accuracy: 0.9422 - val_loss: 0.2222 - val_accuracy: 0.9048\n",
            "Epoch 129/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1326 - accuracy: 0.9408 - val_loss: 0.2242 - val_accuracy: 0.8992\n",
            "Epoch 130/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1329 - accuracy: 0.9405 - val_loss: 0.2276 - val_accuracy: 0.9041\n",
            "Epoch 131/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1336 - accuracy: 0.9420 - val_loss: 0.2227 - val_accuracy: 0.9071\n",
            "Epoch 132/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1323 - accuracy: 0.9397 - val_loss: 0.2151 - val_accuracy: 0.9061\n",
            "Epoch 133/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1306 - accuracy: 0.9406 - val_loss: 0.2271 - val_accuracy: 0.9096\n",
            "Epoch 134/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1332 - accuracy: 0.9407 - val_loss: 0.2230 - val_accuracy: 0.9052\n",
            "Epoch 135/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1297 - accuracy: 0.9429 - val_loss: 0.2251 - val_accuracy: 0.9032\n",
            "Epoch 136/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1346 - accuracy: 0.9410 - val_loss: 0.2297 - val_accuracy: 0.9068\n",
            "Epoch 137/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1329 - accuracy: 0.9412 - val_loss: 0.2224 - val_accuracy: 0.9067\n",
            "Epoch 138/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1334 - accuracy: 0.9402 - val_loss: 0.2470 - val_accuracy: 0.9101\n",
            "Epoch 139/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1378 - accuracy: 0.9387 - val_loss: 0.2316 - val_accuracy: 0.9055\n",
            "Epoch 140/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1289 - accuracy: 0.9425 - val_loss: 0.2191 - val_accuracy: 0.9076\n",
            "Epoch 141/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1316 - accuracy: 0.9397 - val_loss: 0.2340 - val_accuracy: 0.9058\n",
            "Epoch 142/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1310 - accuracy: 0.9429 - val_loss: 0.2155 - val_accuracy: 0.9073\n",
            "Epoch 143/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1283 - accuracy: 0.9437 - val_loss: 0.2228 - val_accuracy: 0.9066\n",
            "Epoch 144/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1305 - accuracy: 0.9416 - val_loss: 0.2294 - val_accuracy: 0.9060\n",
            "Epoch 145/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1345 - accuracy: 0.9384 - val_loss: 0.2209 - val_accuracy: 0.9064\n",
            "Epoch 146/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1304 - accuracy: 0.9425 - val_loss: 0.2261 - val_accuracy: 0.9071\n",
            "Epoch 147/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1312 - accuracy: 0.9424 - val_loss: 0.2275 - val_accuracy: 0.9045\n",
            "Epoch 148/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1251 - accuracy: 0.9445 - val_loss: 0.2249 - val_accuracy: 0.9065\n",
            "Epoch 149/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1278 - accuracy: 0.9432 - val_loss: 0.2260 - val_accuracy: 0.9083\n",
            "Epoch 150/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1280 - accuracy: 0.9432 - val_loss: 0.2291 - val_accuracy: 0.9049\n",
            "Epoch 151/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1301 - accuracy: 0.9417 - val_loss: 0.2328 - val_accuracy: 0.9023\n",
            "Epoch 152/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1271 - accuracy: 0.9435 - val_loss: 0.2320 - val_accuracy: 0.9019\n",
            "Epoch 153/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1247 - accuracy: 0.9436 - val_loss: 0.2343 - val_accuracy: 0.9043\n",
            "Epoch 154/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1250 - accuracy: 0.9446 - val_loss: 0.2319 - val_accuracy: 0.9075\n",
            "Epoch 155/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1274 - accuracy: 0.9449 - val_loss: 0.2305 - val_accuracy: 0.9069\n",
            "Epoch 156/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1311 - accuracy: 0.9422 - val_loss: 0.2333 - val_accuracy: 0.9031\n",
            "Epoch 157/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1257 - accuracy: 0.9442 - val_loss: 0.2374 - val_accuracy: 0.9014\n",
            "Epoch 158/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1224 - accuracy: 0.9449 - val_loss: 0.2359 - val_accuracy: 0.9031\n",
            "Epoch 159/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1266 - accuracy: 0.9425 - val_loss: 0.2348 - val_accuracy: 0.9043\n",
            "Epoch 160/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1269 - accuracy: 0.9423 - val_loss: 0.2368 - val_accuracy: 0.9031\n",
            "Epoch 161/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1273 - accuracy: 0.9430 - val_loss: 0.2348 - val_accuracy: 0.9061\n",
            "Epoch 162/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1217 - accuracy: 0.9462 - val_loss: 0.2306 - val_accuracy: 0.9061\n",
            "Epoch 163/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1231 - accuracy: 0.9466 - val_loss: 0.2392 - val_accuracy: 0.9053\n",
            "Epoch 164/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1256 - accuracy: 0.9452 - val_loss: 0.2290 - val_accuracy: 0.9077\n",
            "Epoch 165/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1217 - accuracy: 0.9470 - val_loss: 0.2327 - val_accuracy: 0.9058\n",
            "Epoch 166/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1285 - accuracy: 0.9424 - val_loss: 0.2424 - val_accuracy: 0.9011\n",
            "Epoch 167/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1200 - accuracy: 0.9472 - val_loss: 0.2452 - val_accuracy: 0.9045\n",
            "Epoch 168/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1215 - accuracy: 0.9450 - val_loss: 0.2415 - val_accuracy: 0.9032\n",
            "Epoch 169/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1212 - accuracy: 0.9459 - val_loss: 0.2467 - val_accuracy: 0.9031\n",
            "Epoch 170/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1217 - accuracy: 0.9462 - val_loss: 0.2357 - val_accuracy: 0.9027\n",
            "Epoch 171/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1245 - accuracy: 0.9451 - val_loss: 0.2436 - val_accuracy: 0.9016\n",
            "Epoch 172/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1239 - accuracy: 0.9429 - val_loss: 0.2519 - val_accuracy: 0.8987\n",
            "Epoch 173/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1265 - accuracy: 0.9427 - val_loss: 0.2445 - val_accuracy: 0.9004\n",
            "Epoch 174/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1184 - accuracy: 0.9467 - val_loss: 0.2470 - val_accuracy: 0.9055\n",
            "Epoch 175/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1175 - accuracy: 0.9497 - val_loss: 0.2495 - val_accuracy: 0.9045\n",
            "Epoch 176/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1218 - accuracy: 0.9466 - val_loss: 0.2483 - val_accuracy: 0.9050\n",
            "Epoch 177/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1209 - accuracy: 0.9467 - val_loss: 0.2486 - val_accuracy: 0.9004\n",
            "Epoch 178/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1224 - accuracy: 0.9458 - val_loss: 0.2440 - val_accuracy: 0.9028\n",
            "Epoch 179/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1199 - accuracy: 0.9470 - val_loss: 0.2455 - val_accuracy: 0.9037\n",
            "Epoch 180/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1193 - accuracy: 0.9482 - val_loss: 0.2489 - val_accuracy: 0.9069\n",
            "Epoch 181/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1245 - accuracy: 0.9448 - val_loss: 0.2463 - val_accuracy: 0.9065\n",
            "Epoch 182/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1188 - accuracy: 0.9492 - val_loss: 0.2539 - val_accuracy: 0.9061\n",
            "Epoch 183/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1168 - accuracy: 0.9491 - val_loss: 0.2471 - val_accuracy: 0.9049\n",
            "Epoch 184/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1185 - accuracy: 0.9472 - val_loss: 0.2519 - val_accuracy: 0.9068\n",
            "Epoch 185/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1139 - accuracy: 0.9511 - val_loss: 0.2466 - val_accuracy: 0.9067\n",
            "Epoch 186/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1217 - accuracy: 0.9465 - val_loss: 0.2513 - val_accuracy: 0.9016\n",
            "Epoch 187/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1140 - accuracy: 0.9522 - val_loss: 0.2535 - val_accuracy: 0.9034\n",
            "Epoch 188/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1180 - accuracy: 0.9482 - val_loss: 0.2522 - val_accuracy: 0.9045\n",
            "Epoch 189/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1152 - accuracy: 0.9489 - val_loss: 0.2536 - val_accuracy: 0.9021\n",
            "Epoch 190/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1154 - accuracy: 0.9499 - val_loss: 0.2484 - val_accuracy: 0.9052\n",
            "Epoch 191/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1171 - accuracy: 0.9502 - val_loss: 0.2548 - val_accuracy: 0.9027\n",
            "Epoch 192/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1215 - accuracy: 0.9452 - val_loss: 0.2573 - val_accuracy: 0.9016\n",
            "Epoch 193/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1169 - accuracy: 0.9485 - val_loss: 0.2588 - val_accuracy: 0.9045\n",
            "Epoch 194/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1247 - accuracy: 0.9460 - val_loss: 0.2506 - val_accuracy: 0.9010\n",
            "Epoch 195/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1208 - accuracy: 0.9489 - val_loss: 0.2523 - val_accuracy: 0.9036\n",
            "Epoch 196/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1174 - accuracy: 0.9485 - val_loss: 0.2491 - val_accuracy: 0.9058\n",
            "Epoch 197/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1198 - accuracy: 0.9467 - val_loss: 0.2507 - val_accuracy: 0.9037\n",
            "Epoch 198/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1192 - accuracy: 0.9470 - val_loss: 0.2601 - val_accuracy: 0.8994\n",
            "Epoch 199/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1155 - accuracy: 0.9494 - val_loss: 0.2595 - val_accuracy: 0.9046\n",
            "Epoch 200/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1164 - accuracy: 0.9495 - val_loss: 0.2576 - val_accuracy: 0.9036\n",
            "Epoch 201/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1112 - accuracy: 0.9515 - val_loss: 0.2512 - val_accuracy: 0.9071\n",
            "Epoch 202/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1119 - accuracy: 0.9520 - val_loss: 0.2593 - val_accuracy: 0.8978\n",
            "Epoch 203/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1192 - accuracy: 0.9473 - val_loss: 0.2560 - val_accuracy: 0.9046\n",
            "Epoch 204/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1205 - accuracy: 0.9476 - val_loss: 0.2586 - val_accuracy: 0.9064\n",
            "Epoch 205/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1138 - accuracy: 0.9497 - val_loss: 0.2579 - val_accuracy: 0.9025\n",
            "Epoch 206/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1151 - accuracy: 0.9495 - val_loss: 0.2656 - val_accuracy: 0.9035\n",
            "Epoch 207/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1143 - accuracy: 0.9499 - val_loss: 0.2611 - val_accuracy: 0.9018\n",
            "Epoch 208/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1157 - accuracy: 0.9487 - val_loss: 0.2599 - val_accuracy: 0.9000\n",
            "Epoch 209/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1182 - accuracy: 0.9482 - val_loss: 0.2678 - val_accuracy: 0.9048\n",
            "Epoch 210/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1102 - accuracy: 0.9523 - val_loss: 0.2670 - val_accuracy: 0.9038\n",
            "Epoch 211/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1173 - accuracy: 0.9501 - val_loss: 0.2536 - val_accuracy: 0.9046\n",
            "Epoch 212/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1162 - accuracy: 0.9483 - val_loss: 0.2608 - val_accuracy: 0.9020\n",
            "Epoch 213/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.9519 - val_loss: 0.2654 - val_accuracy: 0.8994\n",
            "Epoch 214/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1159 - accuracy: 0.9492 - val_loss: 0.2667 - val_accuracy: 0.9024\n",
            "Epoch 215/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1110 - accuracy: 0.9520 - val_loss: 0.2684 - val_accuracy: 0.9043\n",
            "Epoch 216/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1168 - accuracy: 0.9498 - val_loss: 0.2786 - val_accuracy: 0.8952\n",
            "Epoch 217/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1142 - accuracy: 0.9485 - val_loss: 0.2730 - val_accuracy: 0.9030\n",
            "Epoch 218/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1115 - accuracy: 0.9513 - val_loss: 0.2584 - val_accuracy: 0.9047\n",
            "Epoch 219/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1109 - accuracy: 0.9510 - val_loss: 0.2743 - val_accuracy: 0.9013\n",
            "Epoch 220/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1133 - accuracy: 0.9508 - val_loss: 0.2693 - val_accuracy: 0.8971\n",
            "Epoch 221/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1135 - accuracy: 0.9497 - val_loss: 0.2634 - val_accuracy: 0.8990\n",
            "Epoch 222/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1102 - accuracy: 0.9514 - val_loss: 0.2711 - val_accuracy: 0.9047\n",
            "Epoch 223/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1140 - accuracy: 0.9492 - val_loss: 0.2675 - val_accuracy: 0.9058\n",
            "Epoch 224/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1125 - accuracy: 0.9514 - val_loss: 0.2677 - val_accuracy: 0.8994\n",
            "Epoch 225/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1182 - accuracy: 0.9492 - val_loss: 0.2772 - val_accuracy: 0.9035\n",
            "Epoch 226/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1095 - accuracy: 0.9529 - val_loss: 0.2761 - val_accuracy: 0.9032\n",
            "Epoch 227/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1116 - accuracy: 0.9522 - val_loss: 0.2890 - val_accuracy: 0.8992\n",
            "Epoch 228/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1179 - accuracy: 0.9488 - val_loss: 0.2791 - val_accuracy: 0.8981\n",
            "Epoch 229/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1164 - accuracy: 0.9504 - val_loss: 0.2727 - val_accuracy: 0.9041\n",
            "Epoch 230/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1054 - accuracy: 0.9551 - val_loss: 0.2640 - val_accuracy: 0.9004\n",
            "Epoch 231/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1134 - accuracy: 0.9508 - val_loss: 0.2693 - val_accuracy: 0.9037\n",
            "Epoch 232/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.9527 - val_loss: 0.2757 - val_accuracy: 0.9010\n",
            "Epoch 233/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1091 - accuracy: 0.9524 - val_loss: 0.2712 - val_accuracy: 0.9026\n",
            "Epoch 234/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1150 - accuracy: 0.9480 - val_loss: 0.2757 - val_accuracy: 0.9038\n",
            "Epoch 235/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1140 - accuracy: 0.9513 - val_loss: 0.2832 - val_accuracy: 0.9006\n",
            "Epoch 236/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1121 - accuracy: 0.9512 - val_loss: 0.2834 - val_accuracy: 0.8988\n",
            "Epoch 237/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1117 - accuracy: 0.9507 - val_loss: 0.2867 - val_accuracy: 0.9065\n",
            "Epoch 238/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1142 - accuracy: 0.9492 - val_loss: 0.2767 - val_accuracy: 0.9037\n",
            "Epoch 239/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1119 - accuracy: 0.9510 - val_loss: 0.2696 - val_accuracy: 0.8999\n",
            "Epoch 240/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1105 - accuracy: 0.9540 - val_loss: 0.2922 - val_accuracy: 0.9043\n",
            "Epoch 241/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.9531 - val_loss: 0.2750 - val_accuracy: 0.9031\n",
            "Epoch 242/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.9527 - val_loss: 0.2777 - val_accuracy: 0.8997\n",
            "Epoch 243/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1088 - accuracy: 0.9526 - val_loss: 0.2872 - val_accuracy: 0.8992\n",
            "Epoch 244/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1133 - accuracy: 0.9497 - val_loss: 0.2684 - val_accuracy: 0.9024\n",
            "Epoch 245/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1078 - accuracy: 0.9541 - val_loss: 0.2660 - val_accuracy: 0.9056\n",
            "Epoch 246/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1132 - accuracy: 0.9505 - val_loss: 0.2919 - val_accuracy: 0.8943\n",
            "Epoch 247/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1081 - accuracy: 0.9542 - val_loss: 0.2693 - val_accuracy: 0.9020\n",
            "Epoch 248/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1146 - accuracy: 0.9489 - val_loss: 0.2929 - val_accuracy: 0.8963\n",
            "Epoch 249/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.9527 - val_loss: 0.2869 - val_accuracy: 0.9019\n",
            "Epoch 250/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1066 - accuracy: 0.9533 - val_loss: 0.2801 - val_accuracy: 0.9003\n",
            "Epoch 251/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1130 - accuracy: 0.9509 - val_loss: 0.2745 - val_accuracy: 0.9013\n",
            "Epoch 252/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.9521 - val_loss: 0.2780 - val_accuracy: 0.9050\n",
            "Epoch 253/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.9519 - val_loss: 0.2727 - val_accuracy: 0.8982\n",
            "Epoch 254/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1051 - accuracy: 0.9538 - val_loss: 0.2747 - val_accuracy: 0.8990\n",
            "Epoch 255/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1077 - accuracy: 0.9519 - val_loss: 0.2754 - val_accuracy: 0.8989\n",
            "Epoch 256/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1092 - accuracy: 0.9525 - val_loss: 0.2878 - val_accuracy: 0.9053\n",
            "Epoch 257/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1131 - accuracy: 0.9519 - val_loss: 0.2770 - val_accuracy: 0.9024\n",
            "Epoch 258/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1086 - accuracy: 0.9540 - val_loss: 0.2822 - val_accuracy: 0.8970\n",
            "Epoch 259/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1102 - accuracy: 0.9509 - val_loss: 0.2867 - val_accuracy: 0.8965\n",
            "Epoch 260/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.9519 - val_loss: 0.2803 - val_accuracy: 0.9016\n",
            "Epoch 261/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1089 - accuracy: 0.9521 - val_loss: 0.2839 - val_accuracy: 0.8999\n",
            "Epoch 262/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1099 - accuracy: 0.9537 - val_loss: 0.2901 - val_accuracy: 0.8989\n",
            "Epoch 263/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1133 - accuracy: 0.9504 - val_loss: 0.3004 - val_accuracy: 0.9014\n",
            "Epoch 264/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1062 - accuracy: 0.9540 - val_loss: 0.2879 - val_accuracy: 0.9007\n",
            "Epoch 265/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1060 - accuracy: 0.9531 - val_loss: 0.2909 - val_accuracy: 0.8958\n",
            "Epoch 266/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1071 - accuracy: 0.9542 - val_loss: 0.2848 - val_accuracy: 0.9010\n",
            "Epoch 267/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1109 - accuracy: 0.9516 - val_loss: 0.2830 - val_accuracy: 0.9039\n",
            "Epoch 268/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1085 - accuracy: 0.9541 - val_loss: 0.2863 - val_accuracy: 0.8975\n",
            "Epoch 269/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1052 - accuracy: 0.9546 - val_loss: 0.2880 - val_accuracy: 0.9033\n",
            "Epoch 270/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1071 - accuracy: 0.9527 - val_loss: 0.2987 - val_accuracy: 0.8991\n",
            "Epoch 271/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1110 - accuracy: 0.9536 - val_loss: 0.2963 - val_accuracy: 0.9022\n",
            "Epoch 272/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1048 - accuracy: 0.9555 - val_loss: 0.2879 - val_accuracy: 0.9009\n",
            "Epoch 273/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1082 - accuracy: 0.9520 - val_loss: 0.2923 - val_accuracy: 0.8989\n",
            "Epoch 274/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1065 - accuracy: 0.9526 - val_loss: 0.2919 - val_accuracy: 0.8997\n",
            "Epoch 275/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1067 - accuracy: 0.9536 - val_loss: 0.2937 - val_accuracy: 0.8995\n",
            "Epoch 276/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1070 - accuracy: 0.9545 - val_loss: 0.2910 - val_accuracy: 0.9014\n",
            "Epoch 277/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1009 - accuracy: 0.9550 - val_loss: 0.2859 - val_accuracy: 0.9009\n",
            "Epoch 278/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1077 - accuracy: 0.9526 - val_loss: 0.2835 - val_accuracy: 0.8948\n",
            "Epoch 279/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1078 - accuracy: 0.9529 - val_loss: 0.2933 - val_accuracy: 0.9081\n",
            "Epoch 280/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1100 - accuracy: 0.9516 - val_loss: 0.2993 - val_accuracy: 0.9041\n",
            "Epoch 281/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1057 - accuracy: 0.9538 - val_loss: 0.2927 - val_accuracy: 0.9023\n",
            "Epoch 282/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1077 - accuracy: 0.9529 - val_loss: 0.2982 - val_accuracy: 0.8973\n",
            "Epoch 283/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1053 - accuracy: 0.9549 - val_loss: 0.3026 - val_accuracy: 0.8991\n",
            "Epoch 284/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1040 - accuracy: 0.9550 - val_loss: 0.2978 - val_accuracy: 0.8974\n",
            "Epoch 285/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1048 - accuracy: 0.9552 - val_loss: 0.3006 - val_accuracy: 0.8954\n",
            "Epoch 286/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1042 - accuracy: 0.9551 - val_loss: 0.2929 - val_accuracy: 0.9042\n",
            "Epoch 287/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1078 - accuracy: 0.9531 - val_loss: 0.2984 - val_accuracy: 0.8969\n",
            "Epoch 288/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1068 - accuracy: 0.9519 - val_loss: 0.2952 - val_accuracy: 0.9045\n",
            "Epoch 289/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1035 - accuracy: 0.9565 - val_loss: 0.2963 - val_accuracy: 0.8977\n",
            "Epoch 290/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1058 - accuracy: 0.9542 - val_loss: 0.3006 - val_accuracy: 0.9006\n",
            "Epoch 291/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1059 - accuracy: 0.9553 - val_loss: 0.3018 - val_accuracy: 0.8977\n",
            "Epoch 292/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1073 - accuracy: 0.9534 - val_loss: 0.2985 - val_accuracy: 0.9004\n",
            "Epoch 293/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1059 - accuracy: 0.9533 - val_loss: 0.3097 - val_accuracy: 0.9007\n",
            "Epoch 294/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1056 - accuracy: 0.9550 - val_loss: 0.2920 - val_accuracy: 0.9029\n",
            "Epoch 295/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1037 - accuracy: 0.9539 - val_loss: 0.2901 - val_accuracy: 0.9028\n",
            "Epoch 296/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1062 - accuracy: 0.9541 - val_loss: 0.3192 - val_accuracy: 0.9006\n",
            "Epoch 297/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1020 - accuracy: 0.9548 - val_loss: 0.3039 - val_accuracy: 0.8950\n",
            "Epoch 298/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1035 - accuracy: 0.9544 - val_loss: 0.2998 - val_accuracy: 0.9003\n",
            "Epoch 299/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1054 - accuracy: 0.9529 - val_loss: 0.2980 - val_accuracy: 0.9033\n",
            "Epoch 300/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1075 - accuracy: 0.9530 - val_loss: 0.3171 - val_accuracy: 0.8973\n",
            "Epoch 301/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1064 - accuracy: 0.9540 - val_loss: 0.3022 - val_accuracy: 0.9043\n",
            "Epoch 302/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1011 - accuracy: 0.9556 - val_loss: 0.3134 - val_accuracy: 0.9030\n",
            "Epoch 303/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1050 - accuracy: 0.9550 - val_loss: 0.2982 - val_accuracy: 0.9019\n",
            "Epoch 304/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0995 - accuracy: 0.9577 - val_loss: 0.3083 - val_accuracy: 0.8930\n",
            "Epoch 305/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1074 - accuracy: 0.9532 - val_loss: 0.3123 - val_accuracy: 0.9007\n",
            "Epoch 306/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1119 - accuracy: 0.9508 - val_loss: 0.3047 - val_accuracy: 0.9011\n",
            "Epoch 307/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1041 - accuracy: 0.9547 - val_loss: 0.3084 - val_accuracy: 0.9032\n",
            "Epoch 308/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1076 - accuracy: 0.9538 - val_loss: 0.3063 - val_accuracy: 0.9006\n",
            "Epoch 309/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1051 - accuracy: 0.9537 - val_loss: 0.2917 - val_accuracy: 0.8994\n",
            "Epoch 310/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1031 - accuracy: 0.9567 - val_loss: 0.3048 - val_accuracy: 0.8993\n",
            "Epoch 311/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1044 - accuracy: 0.9543 - val_loss: 0.3016 - val_accuracy: 0.9003\n",
            "Epoch 312/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1026 - accuracy: 0.9559 - val_loss: 0.3084 - val_accuracy: 0.8985\n",
            "Epoch 313/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1049 - accuracy: 0.9541 - val_loss: 0.3014 - val_accuracy: 0.9027\n",
            "Epoch 314/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1045 - accuracy: 0.9548 - val_loss: 0.2938 - val_accuracy: 0.9037\n",
            "Epoch 315/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9561 - val_loss: 0.3091 - val_accuracy: 0.8999\n",
            "Epoch 316/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1038 - accuracy: 0.9550 - val_loss: 0.3033 - val_accuracy: 0.9047\n",
            "Epoch 317/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1039 - accuracy: 0.9537 - val_loss: 0.3146 - val_accuracy: 0.9035\n",
            "Epoch 318/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1032 - accuracy: 0.9550 - val_loss: 0.3043 - val_accuracy: 0.8977\n",
            "Epoch 319/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0997 - accuracy: 0.9582 - val_loss: 0.3065 - val_accuracy: 0.9035\n",
            "Epoch 320/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1033 - accuracy: 0.9545 - val_loss: 0.3049 - val_accuracy: 0.8957\n",
            "Epoch 321/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1030 - accuracy: 0.9555 - val_loss: 0.3146 - val_accuracy: 0.8969\n",
            "Epoch 322/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1042 - accuracy: 0.9530 - val_loss: 0.3160 - val_accuracy: 0.9036\n",
            "Epoch 323/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1065 - accuracy: 0.9552 - val_loss: 0.3078 - val_accuracy: 0.8992\n",
            "Epoch 324/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1038 - accuracy: 0.9544 - val_loss: 0.3096 - val_accuracy: 0.8993\n",
            "Epoch 325/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1011 - accuracy: 0.9561 - val_loss: 0.3260 - val_accuracy: 0.9013\n",
            "Epoch 326/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1034 - accuracy: 0.9568 - val_loss: 0.3270 - val_accuracy: 0.8918\n",
            "Epoch 327/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1018 - accuracy: 0.9563 - val_loss: 0.3080 - val_accuracy: 0.9037\n",
            "Epoch 328/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1032 - accuracy: 0.9571 - val_loss: 0.3077 - val_accuracy: 0.8988\n",
            "Epoch 329/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1034 - accuracy: 0.9547 - val_loss: 0.3126 - val_accuracy: 0.8988\n",
            "Epoch 330/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1015 - accuracy: 0.9557 - val_loss: 0.3063 - val_accuracy: 0.9042\n",
            "Epoch 331/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1009 - accuracy: 0.9561 - val_loss: 0.3051 - val_accuracy: 0.9011\n",
            "Epoch 332/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1028 - accuracy: 0.9555 - val_loss: 0.3100 - val_accuracy: 0.9026\n",
            "Epoch 333/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1024 - accuracy: 0.9542 - val_loss: 0.3154 - val_accuracy: 0.9017\n",
            "Epoch 334/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1022 - accuracy: 0.9552 - val_loss: 0.3180 - val_accuracy: 0.9003\n",
            "Epoch 335/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1013 - accuracy: 0.9566 - val_loss: 0.3173 - val_accuracy: 0.8960\n",
            "Epoch 336/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1038 - accuracy: 0.9535 - val_loss: 0.3166 - val_accuracy: 0.9036\n",
            "Epoch 337/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1014 - accuracy: 0.9560 - val_loss: 0.3229 - val_accuracy: 0.8977\n",
            "Epoch 338/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1012 - accuracy: 0.9583 - val_loss: 0.3163 - val_accuracy: 0.8982\n",
            "Epoch 339/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1026 - accuracy: 0.9554 - val_loss: 0.3354 - val_accuracy: 0.8992\n",
            "Epoch 340/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1000 - accuracy: 0.9568 - val_loss: 0.3066 - val_accuracy: 0.9028\n",
            "Epoch 341/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1034 - accuracy: 0.9555 - val_loss: 0.3094 - val_accuracy: 0.9028\n",
            "Epoch 342/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0953 - accuracy: 0.9595 - val_loss: 0.3199 - val_accuracy: 0.9016\n",
            "Epoch 343/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0970 - accuracy: 0.9581 - val_loss: 0.3241 - val_accuracy: 0.8970\n",
            "Epoch 344/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1046 - accuracy: 0.9556 - val_loss: 0.3202 - val_accuracy: 0.8973\n",
            "Epoch 345/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0999 - accuracy: 0.9574 - val_loss: 0.3313 - val_accuracy: 0.8982\n",
            "Epoch 346/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1004 - accuracy: 0.9574 - val_loss: 0.3279 - val_accuracy: 0.8972\n",
            "Epoch 347/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9555 - val_loss: 0.3181 - val_accuracy: 0.8992\n",
            "Epoch 348/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0963 - accuracy: 0.9580 - val_loss: 0.3179 - val_accuracy: 0.8990\n",
            "Epoch 349/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0988 - accuracy: 0.9577 - val_loss: 0.3222 - val_accuracy: 0.9010\n",
            "Epoch 350/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0965 - accuracy: 0.9576 - val_loss: 0.3194 - val_accuracy: 0.8918\n",
            "Epoch 351/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1017 - accuracy: 0.9576 - val_loss: 0.3144 - val_accuracy: 0.9008\n",
            "Epoch 352/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1010 - accuracy: 0.9560 - val_loss: 0.3307 - val_accuracy: 0.8952\n",
            "Epoch 353/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0979 - accuracy: 0.9585 - val_loss: 0.3314 - val_accuracy: 0.9020\n",
            "Epoch 354/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0991 - accuracy: 0.9567 - val_loss: 0.3130 - val_accuracy: 0.8990\n",
            "Epoch 355/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0982 - accuracy: 0.9577 - val_loss: 0.3128 - val_accuracy: 0.9001\n",
            "Epoch 356/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0997 - accuracy: 0.9561 - val_loss: 0.3138 - val_accuracy: 0.9004\n",
            "Epoch 357/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1020 - accuracy: 0.9551 - val_loss: 0.3239 - val_accuracy: 0.8974\n",
            "Epoch 358/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1040 - accuracy: 0.9548 - val_loss: 0.3231 - val_accuracy: 0.9014\n",
            "Epoch 359/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0980 - accuracy: 0.9580 - val_loss: 0.3325 - val_accuracy: 0.9013\n",
            "Epoch 360/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0967 - accuracy: 0.9580 - val_loss: 0.2953 - val_accuracy: 0.9041\n",
            "Epoch 361/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0985 - accuracy: 0.9565 - val_loss: 0.3214 - val_accuracy: 0.9020\n",
            "Epoch 362/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0976 - accuracy: 0.9576 - val_loss: 0.3164 - val_accuracy: 0.8999\n",
            "Epoch 363/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0997 - accuracy: 0.9572 - val_loss: 0.3291 - val_accuracy: 0.8971\n",
            "Epoch 364/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1029 - accuracy: 0.9569 - val_loss: 0.3305 - val_accuracy: 0.9007\n",
            "Epoch 365/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0950 - accuracy: 0.9612 - val_loss: 0.3367 - val_accuracy: 0.8964\n",
            "Epoch 366/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0996 - accuracy: 0.9570 - val_loss: 0.3417 - val_accuracy: 0.9054\n",
            "Epoch 367/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1045 - accuracy: 0.9561 - val_loss: 0.3226 - val_accuracy: 0.8972\n",
            "Epoch 368/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9591 - val_loss: 0.3296 - val_accuracy: 0.8978\n",
            "Epoch 369/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0990 - accuracy: 0.9589 - val_loss: 0.3100 - val_accuracy: 0.9014\n",
            "Epoch 370/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0969 - accuracy: 0.9580 - val_loss: 0.3247 - val_accuracy: 0.8975\n",
            "Epoch 371/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0958 - accuracy: 0.9591 - val_loss: 0.3277 - val_accuracy: 0.9011\n",
            "Epoch 372/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0965 - accuracy: 0.9591 - val_loss: 0.3208 - val_accuracy: 0.8973\n",
            "Epoch 373/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0954 - accuracy: 0.9584 - val_loss: 0.3262 - val_accuracy: 0.9039\n",
            "Epoch 374/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1047 - accuracy: 0.9552 - val_loss: 0.3361 - val_accuracy: 0.8984\n",
            "Epoch 375/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1035 - accuracy: 0.9564 - val_loss: 0.3283 - val_accuracy: 0.9004\n",
            "Epoch 376/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0999 - accuracy: 0.9561 - val_loss: 0.3325 - val_accuracy: 0.8985\n",
            "Epoch 377/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0984 - accuracy: 0.9565 - val_loss: 0.3292 - val_accuracy: 0.8965\n",
            "Epoch 378/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0990 - accuracy: 0.9568 - val_loss: 0.3242 - val_accuracy: 0.8979\n",
            "Epoch 379/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9544 - val_loss: 0.3444 - val_accuracy: 0.8996\n",
            "Epoch 380/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0953 - accuracy: 0.9585 - val_loss: 0.3337 - val_accuracy: 0.8982\n",
            "Epoch 381/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0959 - accuracy: 0.9587 - val_loss: 0.3475 - val_accuracy: 0.9010\n",
            "Epoch 382/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0992 - accuracy: 0.9542 - val_loss: 0.3262 - val_accuracy: 0.9012\n",
            "Epoch 383/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1015 - accuracy: 0.9566 - val_loss: 0.3315 - val_accuracy: 0.9009\n",
            "Epoch 384/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0914 - accuracy: 0.9627 - val_loss: 0.3295 - val_accuracy: 0.8980\n",
            "Epoch 385/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0965 - accuracy: 0.9586 - val_loss: 0.3425 - val_accuracy: 0.9007\n",
            "Epoch 386/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1017 - accuracy: 0.9560 - val_loss: 0.3301 - val_accuracy: 0.9003\n",
            "Epoch 387/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0962 - accuracy: 0.9583 - val_loss: 0.3310 - val_accuracy: 0.8978\n",
            "Epoch 388/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0984 - accuracy: 0.9578 - val_loss: 0.3359 - val_accuracy: 0.9020\n",
            "Epoch 389/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1028 - accuracy: 0.9569 - val_loss: 0.3452 - val_accuracy: 0.8911\n",
            "Epoch 390/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0987 - accuracy: 0.9569 - val_loss: 0.3360 - val_accuracy: 0.8964\n",
            "Epoch 391/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1009 - accuracy: 0.9557 - val_loss: 0.3392 - val_accuracy: 0.8991\n",
            "Epoch 392/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9542 - val_loss: 0.3400 - val_accuracy: 0.8963\n",
            "Epoch 393/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0998 - accuracy: 0.9552 - val_loss: 0.3462 - val_accuracy: 0.9007\n",
            "Epoch 394/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0961 - accuracy: 0.9586 - val_loss: 0.3280 - val_accuracy: 0.8985\n",
            "Epoch 395/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0967 - accuracy: 0.9575 - val_loss: 0.3537 - val_accuracy: 0.9008\n",
            "Epoch 396/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0913 - accuracy: 0.9600 - val_loss: 0.3528 - val_accuracy: 0.8988\n",
            "Epoch 397/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0966 - accuracy: 0.9589 - val_loss: 0.3424 - val_accuracy: 0.8974\n",
            "Epoch 398/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1018 - accuracy: 0.9577 - val_loss: 0.3381 - val_accuracy: 0.8993\n",
            "Epoch 399/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0959 - accuracy: 0.9579 - val_loss: 0.3446 - val_accuracy: 0.8991\n",
            "Epoch 400/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0994 - accuracy: 0.9573 - val_loss: 0.3433 - val_accuracy: 0.8973\n",
            "Epoch 401/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0964 - accuracy: 0.9584 - val_loss: 0.3415 - val_accuracy: 0.8982\n",
            "Epoch 402/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0959 - accuracy: 0.9585 - val_loss: 0.3342 - val_accuracy: 0.8989\n",
            "Epoch 403/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0923 - accuracy: 0.9604 - val_loss: 0.3253 - val_accuracy: 0.9002\n",
            "Epoch 404/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0919 - accuracy: 0.9605 - val_loss: 0.3332 - val_accuracy: 0.8982\n",
            "Epoch 405/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0977 - accuracy: 0.9593 - val_loss: 0.3399 - val_accuracy: 0.9014\n",
            "Epoch 406/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0949 - accuracy: 0.9606 - val_loss: 0.3400 - val_accuracy: 0.8941\n",
            "Epoch 407/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0932 - accuracy: 0.9606 - val_loss: 0.3461 - val_accuracy: 0.9006\n",
            "Epoch 408/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1032 - accuracy: 0.9560 - val_loss: 0.3392 - val_accuracy: 0.8958\n",
            "Epoch 409/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0987 - accuracy: 0.9570 - val_loss: 0.3300 - val_accuracy: 0.9024\n",
            "Epoch 410/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0944 - accuracy: 0.9591 - val_loss: 0.3388 - val_accuracy: 0.8986\n",
            "Epoch 411/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0996 - accuracy: 0.9560 - val_loss: 0.3357 - val_accuracy: 0.9011\n",
            "Epoch 412/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0945 - accuracy: 0.9580 - val_loss: 0.3434 - val_accuracy: 0.9010\n",
            "Epoch 413/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0954 - accuracy: 0.9582 - val_loss: 0.3472 - val_accuracy: 0.9004\n",
            "Epoch 414/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0917 - accuracy: 0.9614 - val_loss: 0.3594 - val_accuracy: 0.8999\n",
            "Epoch 415/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0997 - accuracy: 0.9567 - val_loss: 0.3368 - val_accuracy: 0.9007\n",
            "Epoch 416/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0944 - accuracy: 0.9594 - val_loss: 0.3548 - val_accuracy: 0.9004\n",
            "Epoch 417/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0952 - accuracy: 0.9592 - val_loss: 0.3449 - val_accuracy: 0.8970\n",
            "Epoch 418/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0959 - accuracy: 0.9589 - val_loss: 0.3366 - val_accuracy: 0.9001\n",
            "Epoch 419/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0984 - accuracy: 0.9577 - val_loss: 0.3356 - val_accuracy: 0.8996\n",
            "Epoch 420/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.1023 - accuracy: 0.9579 - val_loss: 0.3666 - val_accuracy: 0.8975\n",
            "Epoch 421/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0955 - accuracy: 0.9612 - val_loss: 0.3250 - val_accuracy: 0.8963\n",
            "Epoch 422/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0946 - accuracy: 0.9585 - val_loss: 0.3529 - val_accuracy: 0.8973\n",
            "Epoch 423/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0908 - accuracy: 0.9601 - val_loss: 0.3526 - val_accuracy: 0.8999\n",
            "Epoch 424/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0921 - accuracy: 0.9608 - val_loss: 0.3508 - val_accuracy: 0.8990\n",
            "Epoch 425/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0963 - accuracy: 0.9594 - val_loss: 0.3528 - val_accuracy: 0.9004\n",
            "Epoch 426/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0908 - accuracy: 0.9607 - val_loss: 0.3446 - val_accuracy: 0.9067\n",
            "Epoch 427/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0944 - accuracy: 0.9598 - val_loss: 0.3496 - val_accuracy: 0.9019\n",
            "Epoch 428/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0976 - accuracy: 0.9578 - val_loss: 0.3549 - val_accuracy: 0.8976\n",
            "Epoch 429/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0946 - accuracy: 0.9597 - val_loss: 0.3575 - val_accuracy: 0.9003\n",
            "Epoch 430/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0969 - accuracy: 0.9574 - val_loss: 0.3445 - val_accuracy: 0.8978\n",
            "Epoch 431/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0968 - accuracy: 0.9585 - val_loss: 0.3598 - val_accuracy: 0.8972\n",
            "Epoch 432/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0956 - accuracy: 0.9601 - val_loss: 0.3489 - val_accuracy: 0.9024\n",
            "Epoch 433/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0968 - accuracy: 0.9589 - val_loss: 0.3492 - val_accuracy: 0.8982\n",
            "Epoch 434/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0914 - accuracy: 0.9605 - val_loss: 0.3481 - val_accuracy: 0.9025\n",
            "Epoch 435/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0982 - accuracy: 0.9574 - val_loss: 0.3371 - val_accuracy: 0.9013\n",
            "Epoch 436/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0901 - accuracy: 0.9610 - val_loss: 0.3352 - val_accuracy: 0.9006\n",
            "Epoch 437/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0983 - accuracy: 0.9580 - val_loss: 0.3594 - val_accuracy: 0.8982\n",
            "Epoch 438/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0885 - accuracy: 0.9629 - val_loss: 0.3349 - val_accuracy: 0.8965\n",
            "Epoch 439/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0982 - accuracy: 0.9567 - val_loss: 0.3674 - val_accuracy: 0.8975\n",
            "Epoch 440/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0939 - accuracy: 0.9597 - val_loss: 0.3555 - val_accuracy: 0.8995\n",
            "Epoch 441/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1029 - accuracy: 0.9545 - val_loss: 0.3575 - val_accuracy: 0.8992\n",
            "Epoch 442/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0937 - accuracy: 0.9598 - val_loss: 0.3568 - val_accuracy: 0.8999\n",
            "Epoch 443/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0936 - accuracy: 0.9599 - val_loss: 0.3616 - val_accuracy: 0.8989\n",
            "Epoch 444/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0952 - accuracy: 0.9583 - val_loss: 0.3427 - val_accuracy: 0.8999\n",
            "Epoch 445/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0954 - accuracy: 0.9574 - val_loss: 0.3639 - val_accuracy: 0.8931\n",
            "Epoch 446/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0936 - accuracy: 0.9597 - val_loss: 0.3723 - val_accuracy: 0.8998\n",
            "Epoch 447/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0975 - accuracy: 0.9593 - val_loss: 0.3583 - val_accuracy: 0.9023\n",
            "Epoch 448/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0923 - accuracy: 0.9594 - val_loss: 0.3501 - val_accuracy: 0.9014\n",
            "Epoch 449/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9617 - val_loss: 0.3575 - val_accuracy: 0.8996\n",
            "Epoch 450/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0943 - accuracy: 0.9603 - val_loss: 0.3389 - val_accuracy: 0.9030\n",
            "Epoch 451/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0915 - accuracy: 0.9614 - val_loss: 0.3773 - val_accuracy: 0.8983\n",
            "Epoch 452/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0912 - accuracy: 0.9596 - val_loss: 0.3584 - val_accuracy: 0.9004\n",
            "Epoch 453/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0957 - accuracy: 0.9591 - val_loss: 0.3604 - val_accuracy: 0.8965\n",
            "Epoch 454/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0945 - accuracy: 0.9589 - val_loss: 0.3647 - val_accuracy: 0.8978\n",
            "Epoch 455/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0934 - accuracy: 0.9598 - val_loss: 0.3462 - val_accuracy: 0.9006\n",
            "Epoch 456/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0896 - accuracy: 0.9606 - val_loss: 0.3733 - val_accuracy: 0.8966\n",
            "Epoch 457/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0932 - accuracy: 0.9613 - val_loss: 0.3638 - val_accuracy: 0.8954\n",
            "Epoch 458/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9602 - val_loss: 0.3561 - val_accuracy: 0.8985\n",
            "Epoch 459/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0905 - accuracy: 0.9602 - val_loss: 0.3617 - val_accuracy: 0.8960\n",
            "Epoch 460/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9598 - val_loss: 0.3513 - val_accuracy: 0.8995\n",
            "Epoch 461/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0909 - accuracy: 0.9595 - val_loss: 0.3693 - val_accuracy: 0.8990\n",
            "Epoch 462/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0926 - accuracy: 0.9609 - val_loss: 0.3621 - val_accuracy: 0.8948\n",
            "Epoch 463/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0932 - accuracy: 0.9591 - val_loss: 0.3778 - val_accuracy: 0.8951\n",
            "Epoch 464/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0869 - accuracy: 0.9622 - val_loss: 0.3553 - val_accuracy: 0.8998\n",
            "Epoch 465/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0916 - accuracy: 0.9607 - val_loss: 0.3571 - val_accuracy: 0.8977\n",
            "Epoch 466/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0920 - accuracy: 0.9608 - val_loss: 0.3536 - val_accuracy: 0.8992\n",
            "Epoch 467/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0938 - accuracy: 0.9589 - val_loss: 0.3557 - val_accuracy: 0.9024\n",
            "Epoch 468/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0958 - accuracy: 0.9579 - val_loss: 0.3620 - val_accuracy: 0.8962\n",
            "Epoch 469/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0952 - accuracy: 0.9586 - val_loss: 0.3678 - val_accuracy: 0.9001\n",
            "Epoch 470/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0918 - accuracy: 0.9611 - val_loss: 0.3629 - val_accuracy: 0.9024\n",
            "Epoch 471/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0934 - accuracy: 0.9597 - val_loss: 0.3638 - val_accuracy: 0.8981\n",
            "Epoch 472/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0938 - accuracy: 0.9603 - val_loss: 0.3792 - val_accuracy: 0.9001\n",
            "Epoch 473/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0932 - accuracy: 0.9594 - val_loss: 0.3736 - val_accuracy: 0.8969\n",
            "Epoch 474/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9609 - val_loss: 0.3812 - val_accuracy: 0.8979\n",
            "Epoch 475/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9591 - val_loss: 0.3708 - val_accuracy: 0.9021\n",
            "Epoch 476/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0929 - accuracy: 0.9595 - val_loss: 0.3629 - val_accuracy: 0.8951\n",
            "Epoch 477/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0927 - accuracy: 0.9595 - val_loss: 0.3588 - val_accuracy: 0.8984\n",
            "Epoch 478/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0949 - accuracy: 0.9584 - val_loss: 0.3619 - val_accuracy: 0.8976\n",
            "Epoch 479/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9624 - val_loss: 0.3764 - val_accuracy: 0.8977\n",
            "Epoch 480/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0884 - accuracy: 0.9621 - val_loss: 0.3637 - val_accuracy: 0.9018\n",
            "Epoch 481/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0940 - accuracy: 0.9587 - val_loss: 0.3722 - val_accuracy: 0.9015\n",
            "Epoch 482/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0900 - accuracy: 0.9608 - val_loss: 0.3557 - val_accuracy: 0.9007\n",
            "Epoch 483/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0950 - accuracy: 0.9599 - val_loss: 0.3809 - val_accuracy: 0.9005\n",
            "Epoch 484/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0891 - accuracy: 0.9622 - val_loss: 0.3672 - val_accuracy: 0.8999\n",
            "Epoch 485/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9613 - val_loss: 0.3795 - val_accuracy: 0.9022\n",
            "Epoch 486/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0930 - accuracy: 0.9588 - val_loss: 0.3775 - val_accuracy: 0.8911\n",
            "Epoch 487/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0954 - accuracy: 0.9584 - val_loss: 0.3745 - val_accuracy: 0.8983\n",
            "Epoch 488/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0927 - accuracy: 0.9602 - val_loss: 0.3815 - val_accuracy: 0.8987\n",
            "Epoch 489/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0897 - accuracy: 0.9607 - val_loss: 0.3761 - val_accuracy: 0.8999\n",
            "Epoch 490/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0924 - accuracy: 0.9610 - val_loss: 0.3816 - val_accuracy: 0.9023\n",
            "Epoch 491/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0877 - accuracy: 0.9629 - val_loss: 0.3730 - val_accuracy: 0.8977\n",
            "Epoch 492/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0887 - accuracy: 0.9617 - val_loss: 0.3846 - val_accuracy: 0.9010\n",
            "Epoch 493/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9627 - val_loss: 0.3798 - val_accuracy: 0.8993\n",
            "Epoch 494/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0853 - accuracy: 0.9650 - val_loss: 0.3725 - val_accuracy: 0.9051\n",
            "Epoch 495/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0879 - accuracy: 0.9621 - val_loss: 0.3756 - val_accuracy: 0.8996\n",
            "Epoch 496/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0871 - accuracy: 0.9608 - val_loss: 0.3798 - val_accuracy: 0.8973\n",
            "Epoch 497/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0894 - accuracy: 0.9616 - val_loss: 0.3955 - val_accuracy: 0.8920\n",
            "Epoch 498/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0881 - accuracy: 0.9625 - val_loss: 0.3867 - val_accuracy: 0.9002\n",
            "Epoch 499/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0937 - accuracy: 0.9588 - val_loss: 0.3772 - val_accuracy: 0.8944\n",
            "Epoch 500/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0940 - accuracy: 0.9591 - val_loss: 0.3821 - val_accuracy: 0.8961\n",
            "Epoch 501/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0888 - accuracy: 0.9601 - val_loss: 0.3906 - val_accuracy: 0.8988\n",
            "Epoch 502/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0876 - accuracy: 0.9620 - val_loss: 0.3702 - val_accuracy: 0.8967\n",
            "Epoch 503/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0867 - accuracy: 0.9628 - val_loss: 0.4045 - val_accuracy: 0.8943\n",
            "Epoch 504/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0906 - accuracy: 0.9609 - val_loss: 0.3778 - val_accuracy: 0.8977\n",
            "Epoch 505/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0864 - accuracy: 0.9618 - val_loss: 0.3688 - val_accuracy: 0.8997\n",
            "Epoch 506/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0937 - accuracy: 0.9570 - val_loss: 0.3945 - val_accuracy: 0.9003\n",
            "Epoch 507/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0962 - accuracy: 0.9587 - val_loss: 0.3907 - val_accuracy: 0.8905\n",
            "Epoch 508/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0925 - accuracy: 0.9616 - val_loss: 0.3821 - val_accuracy: 0.8993\n",
            "Epoch 509/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9642 - val_loss: 0.3942 - val_accuracy: 0.8994\n",
            "Epoch 510/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0899 - accuracy: 0.9622 - val_loss: 0.3809 - val_accuracy: 0.9003\n",
            "Epoch 511/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0920 - accuracy: 0.9613 - val_loss: 0.3775 - val_accuracy: 0.8987\n",
            "Epoch 512/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0841 - accuracy: 0.9640 - val_loss: 0.3866 - val_accuracy: 0.9007\n",
            "Epoch 513/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0932 - accuracy: 0.9597 - val_loss: 0.3828 - val_accuracy: 0.9020\n",
            "Epoch 514/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0900 - accuracy: 0.9623 - val_loss: 0.3634 - val_accuracy: 0.8955\n",
            "Epoch 515/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0914 - accuracy: 0.9596 - val_loss: 0.3877 - val_accuracy: 0.8948\n",
            "Epoch 516/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0901 - accuracy: 0.9611 - val_loss: 0.3907 - val_accuracy: 0.8996\n",
            "Epoch 517/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0905 - accuracy: 0.9623 - val_loss: 0.3808 - val_accuracy: 0.8990\n",
            "Epoch 518/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0903 - accuracy: 0.9596 - val_loss: 0.3952 - val_accuracy: 0.8984\n",
            "Epoch 519/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0923 - accuracy: 0.9609 - val_loss: 0.3686 - val_accuracy: 0.8987\n",
            "Epoch 520/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0909 - accuracy: 0.9598 - val_loss: 0.3758 - val_accuracy: 0.8982\n",
            "Epoch 521/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0885 - accuracy: 0.9610 - val_loss: 0.4102 - val_accuracy: 0.8965\n",
            "Epoch 522/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0889 - accuracy: 0.9615 - val_loss: 0.3709 - val_accuracy: 0.9000\n",
            "Epoch 523/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0906 - accuracy: 0.9618 - val_loss: 0.3911 - val_accuracy: 0.8948\n",
            "Epoch 524/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0892 - accuracy: 0.9619 - val_loss: 0.3915 - val_accuracy: 0.9020\n",
            "Epoch 525/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0901 - accuracy: 0.9618 - val_loss: 0.3967 - val_accuracy: 0.9006\n",
            "Epoch 526/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0852 - accuracy: 0.9632 - val_loss: 0.4070 - val_accuracy: 0.8968\n",
            "Epoch 527/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0907 - accuracy: 0.9617 - val_loss: 0.4202 - val_accuracy: 0.8912\n",
            "Epoch 528/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0924 - accuracy: 0.9603 - val_loss: 0.3922 - val_accuracy: 0.8976\n",
            "Epoch 529/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9667 - val_loss: 0.3827 - val_accuracy: 0.8963\n",
            "Epoch 530/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0922 - accuracy: 0.9603 - val_loss: 0.3881 - val_accuracy: 0.8996\n",
            "Epoch 531/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0903 - accuracy: 0.9610 - val_loss: 0.3865 - val_accuracy: 0.8961\n",
            "Epoch 532/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0840 - accuracy: 0.9651 - val_loss: 0.3777 - val_accuracy: 0.8992\n",
            "Epoch 533/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0952 - accuracy: 0.9615 - val_loss: 0.3807 - val_accuracy: 0.8983\n",
            "Epoch 534/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0875 - accuracy: 0.9628 - val_loss: 0.4020 - val_accuracy: 0.9018\n",
            "Epoch 535/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0846 - accuracy: 0.9650 - val_loss: 0.3833 - val_accuracy: 0.8976\n",
            "Epoch 536/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0868 - accuracy: 0.9624 - val_loss: 0.3944 - val_accuracy: 0.8953\n",
            "Epoch 537/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0920 - accuracy: 0.9598 - val_loss: 0.3744 - val_accuracy: 0.9024\n",
            "Epoch 538/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0844 - accuracy: 0.9642 - val_loss: 0.3887 - val_accuracy: 0.8947\n",
            "Epoch 539/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0844 - accuracy: 0.9645 - val_loss: 0.4021 - val_accuracy: 0.8981\n",
            "Epoch 540/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0867 - accuracy: 0.9620 - val_loss: 0.3911 - val_accuracy: 0.8994\n",
            "Epoch 541/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0863 - accuracy: 0.9628 - val_loss: 0.3918 - val_accuracy: 0.8996\n",
            "Epoch 542/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0851 - accuracy: 0.9645 - val_loss: 0.3980 - val_accuracy: 0.8957\n",
            "Epoch 543/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0894 - accuracy: 0.9623 - val_loss: 0.3935 - val_accuracy: 0.8956\n",
            "Epoch 544/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0862 - accuracy: 0.9630 - val_loss: 0.4047 - val_accuracy: 0.9010\n",
            "Epoch 545/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0846 - accuracy: 0.9640 - val_loss: 0.4059 - val_accuracy: 0.8983\n",
            "Epoch 546/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0871 - accuracy: 0.9618 - val_loss: 0.4059 - val_accuracy: 0.8963\n",
            "Epoch 547/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0909 - accuracy: 0.9615 - val_loss: 0.4015 - val_accuracy: 0.8985\n",
            "Epoch 548/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0895 - accuracy: 0.9622 - val_loss: 0.3926 - val_accuracy: 0.8967\n",
            "Epoch 549/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0875 - accuracy: 0.9624 - val_loss: 0.3738 - val_accuracy: 0.9007\n",
            "Epoch 550/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0872 - accuracy: 0.9623 - val_loss: 0.3894 - val_accuracy: 0.8927\n",
            "Epoch 551/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0865 - accuracy: 0.9608 - val_loss: 0.4096 - val_accuracy: 0.8928\n",
            "Epoch 552/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0903 - accuracy: 0.9631 - val_loss: 0.3943 - val_accuracy: 0.9007\n",
            "Epoch 553/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9626 - val_loss: 0.4040 - val_accuracy: 0.8974\n",
            "Epoch 554/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0867 - accuracy: 0.9627 - val_loss: 0.4038 - val_accuracy: 0.8971\n",
            "Epoch 555/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0875 - accuracy: 0.9614 - val_loss: 0.3931 - val_accuracy: 0.8982\n",
            "Epoch 556/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0859 - accuracy: 0.9648 - val_loss: 0.4345 - val_accuracy: 0.8854\n",
            "Epoch 557/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0948 - accuracy: 0.9597 - val_loss: 0.4099 - val_accuracy: 0.8984\n",
            "Epoch 558/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9644 - val_loss: 0.4028 - val_accuracy: 0.8917\n",
            "Epoch 559/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0878 - accuracy: 0.9611 - val_loss: 0.4010 - val_accuracy: 0.8989\n",
            "Epoch 560/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0832 - accuracy: 0.9645 - val_loss: 0.4158 - val_accuracy: 0.8964\n",
            "Epoch 561/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0872 - accuracy: 0.9639 - val_loss: 0.3923 - val_accuracy: 0.8951\n",
            "Epoch 562/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0872 - accuracy: 0.9637 - val_loss: 0.4197 - val_accuracy: 0.8960\n",
            "Epoch 563/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9640 - val_loss: 0.4145 - val_accuracy: 0.8983\n",
            "Epoch 564/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0879 - accuracy: 0.9618 - val_loss: 0.4027 - val_accuracy: 0.8979\n",
            "Epoch 565/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0879 - accuracy: 0.9628 - val_loss: 0.4152 - val_accuracy: 0.8979\n",
            "Epoch 566/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0885 - accuracy: 0.9632 - val_loss: 0.3778 - val_accuracy: 0.8977\n",
            "Epoch 567/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0843 - accuracy: 0.9640 - val_loss: 0.4087 - val_accuracy: 0.8933\n",
            "Epoch 568/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0881 - accuracy: 0.9618 - val_loss: 0.3851 - val_accuracy: 0.8933\n",
            "Epoch 569/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0833 - accuracy: 0.9639 - val_loss: 0.4093 - val_accuracy: 0.9015\n",
            "Epoch 570/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0795 - accuracy: 0.9665 - val_loss: 0.4306 - val_accuracy: 0.8963\n",
            "Epoch 571/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0904 - accuracy: 0.9607 - val_loss: 0.4075 - val_accuracy: 0.8990\n",
            "Epoch 572/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0921 - accuracy: 0.9599 - val_loss: 0.3967 - val_accuracy: 0.8945\n",
            "Epoch 573/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9659 - val_loss: 0.3939 - val_accuracy: 0.9011\n",
            "Epoch 574/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0863 - accuracy: 0.9620 - val_loss: 0.4045 - val_accuracy: 0.8954\n",
            "Epoch 575/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0845 - accuracy: 0.9626 - val_loss: 0.3977 - val_accuracy: 0.8990\n",
            "Epoch 576/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0895 - accuracy: 0.9614 - val_loss: 0.3839 - val_accuracy: 0.8966\n",
            "Epoch 577/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0859 - accuracy: 0.9635 - val_loss: 0.3854 - val_accuracy: 0.8937\n",
            "Epoch 578/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0846 - accuracy: 0.9643 - val_loss: 0.3924 - val_accuracy: 0.8958\n",
            "Epoch 579/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0888 - accuracy: 0.9613 - val_loss: 0.4102 - val_accuracy: 0.8997\n",
            "Epoch 580/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0842 - accuracy: 0.9636 - val_loss: 0.4177 - val_accuracy: 0.8996\n",
            "Epoch 581/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0814 - accuracy: 0.9659 - val_loss: 0.4222 - val_accuracy: 0.8966\n",
            "Epoch 582/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0841 - accuracy: 0.9651 - val_loss: 0.4166 - val_accuracy: 0.8950\n",
            "Epoch 583/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0850 - accuracy: 0.9625 - val_loss: 0.4172 - val_accuracy: 0.8922\n",
            "Epoch 584/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0833 - accuracy: 0.9642 - val_loss: 0.4322 - val_accuracy: 0.9046\n",
            "Epoch 585/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0875 - accuracy: 0.9626 - val_loss: 0.3896 - val_accuracy: 0.8988\n",
            "Epoch 586/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0856 - accuracy: 0.9639 - val_loss: 0.4293 - val_accuracy: 0.9007\n",
            "Epoch 587/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0845 - accuracy: 0.9634 - val_loss: 0.4072 - val_accuracy: 0.8948\n",
            "Epoch 588/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0856 - accuracy: 0.9636 - val_loss: 0.4006 - val_accuracy: 0.8982\n",
            "Epoch 589/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0826 - accuracy: 0.9659 - val_loss: 0.4329 - val_accuracy: 0.8927\n",
            "Epoch 590/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0845 - accuracy: 0.9626 - val_loss: 0.4146 - val_accuracy: 0.8998\n",
            "Epoch 591/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0847 - accuracy: 0.9643 - val_loss: 0.4203 - val_accuracy: 0.8965\n",
            "Epoch 592/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9652 - val_loss: 0.4092 - val_accuracy: 0.8969\n",
            "Epoch 593/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0769 - accuracy: 0.9669 - val_loss: 0.4253 - val_accuracy: 0.8885\n",
            "Epoch 594/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0919 - accuracy: 0.9596 - val_loss: 0.4191 - val_accuracy: 0.8968\n",
            "Epoch 595/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0861 - accuracy: 0.9633 - val_loss: 0.3981 - val_accuracy: 0.8994\n",
            "Epoch 596/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0875 - accuracy: 0.9616 - val_loss: 0.4148 - val_accuracy: 0.9002\n",
            "Epoch 597/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0894 - accuracy: 0.9614 - val_loss: 0.4097 - val_accuracy: 0.8999\n",
            "Epoch 598/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9635 - val_loss: 0.4110 - val_accuracy: 0.8963\n",
            "Epoch 599/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0891 - accuracy: 0.9613 - val_loss: 0.4164 - val_accuracy: 0.9034\n",
            "Epoch 600/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0849 - accuracy: 0.9637 - val_loss: 0.4168 - val_accuracy: 0.8985\n",
            "Epoch 601/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0855 - accuracy: 0.9651 - val_loss: 0.4098 - val_accuracy: 0.8939\n",
            "Epoch 602/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0801 - accuracy: 0.9661 - val_loss: 0.4190 - val_accuracy: 0.8997\n",
            "Epoch 603/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0865 - accuracy: 0.9618 - val_loss: 0.4214 - val_accuracy: 0.8952\n",
            "Epoch 604/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0870 - accuracy: 0.9635 - val_loss: 0.4172 - val_accuracy: 0.8995\n",
            "Epoch 605/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0847 - accuracy: 0.9630 - val_loss: 0.4311 - val_accuracy: 0.8969\n",
            "Epoch 606/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0900 - accuracy: 0.9609 - val_loss: 0.4119 - val_accuracy: 0.8925\n",
            "Epoch 607/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0782 - accuracy: 0.9657 - val_loss: 0.4106 - val_accuracy: 0.8922\n",
            "Epoch 608/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0952 - accuracy: 0.9603 - val_loss: 0.3903 - val_accuracy: 0.8928\n",
            "Epoch 609/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0851 - accuracy: 0.9640 - val_loss: 0.4176 - val_accuracy: 0.8984\n",
            "Epoch 610/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9640 - val_loss: 0.4082 - val_accuracy: 0.9033\n",
            "Epoch 611/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0795 - accuracy: 0.9672 - val_loss: 0.4171 - val_accuracy: 0.9007\n",
            "Epoch 612/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9659 - val_loss: 0.4451 - val_accuracy: 0.9011\n",
            "Epoch 613/1024\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.0904 - accuracy: 0.9616 - val_loss: 0.4168 - val_accuracy: 0.8976\n",
            "Epoch 614/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0824 - accuracy: 0.9655 - val_loss: 0.4040 - val_accuracy: 0.9012\n",
            "Epoch 615/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0815 - accuracy: 0.9644 - val_loss: 0.4399 - val_accuracy: 0.9021\n",
            "Epoch 616/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9644 - val_loss: 0.4138 - val_accuracy: 0.8979\n",
            "Epoch 617/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0842 - accuracy: 0.9639 - val_loss: 0.4361 - val_accuracy: 0.8999\n",
            "Epoch 618/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0939 - accuracy: 0.9608 - val_loss: 0.4019 - val_accuracy: 0.8977\n",
            "Epoch 619/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0822 - accuracy: 0.9651 - val_loss: 0.4315 - val_accuracy: 0.8903\n",
            "Epoch 620/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9654 - val_loss: 0.4324 - val_accuracy: 0.8911\n",
            "Epoch 621/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0847 - accuracy: 0.9642 - val_loss: 0.4149 - val_accuracy: 0.9011\n",
            "Epoch 622/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0832 - accuracy: 0.9650 - val_loss: 0.4220 - val_accuracy: 0.8953\n",
            "Epoch 623/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9670 - val_loss: 0.4324 - val_accuracy: 0.8943\n",
            "Epoch 624/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0848 - accuracy: 0.9638 - val_loss: 0.4277 - val_accuracy: 0.8979\n",
            "Epoch 625/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0827 - accuracy: 0.9645 - val_loss: 0.4382 - val_accuracy: 0.8997\n",
            "Epoch 626/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0816 - accuracy: 0.9655 - val_loss: 0.4261 - val_accuracy: 0.8947\n",
            "Epoch 627/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0818 - accuracy: 0.9643 - val_loss: 0.4317 - val_accuracy: 0.8948\n",
            "Epoch 628/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0837 - accuracy: 0.9638 - val_loss: 0.4211 - val_accuracy: 0.8971\n",
            "Epoch 629/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0821 - accuracy: 0.9655 - val_loss: 0.4387 - val_accuracy: 0.8955\n",
            "Epoch 630/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0882 - accuracy: 0.9613 - val_loss: 0.4352 - val_accuracy: 0.8990\n",
            "Epoch 631/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0770 - accuracy: 0.9673 - val_loss: 0.4299 - val_accuracy: 0.8924\n",
            "Epoch 632/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0863 - accuracy: 0.9626 - val_loss: 0.4413 - val_accuracy: 0.8972\n",
            "Epoch 633/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0803 - accuracy: 0.9666 - val_loss: 0.4368 - val_accuracy: 0.8961\n",
            "Epoch 634/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9646 - val_loss: 0.4187 - val_accuracy: 0.8978\n",
            "Epoch 635/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0818 - accuracy: 0.9648 - val_loss: 0.4320 - val_accuracy: 0.8976\n",
            "Epoch 636/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0874 - accuracy: 0.9638 - val_loss: 0.4242 - val_accuracy: 0.8947\n",
            "Epoch 637/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0813 - accuracy: 0.9675 - val_loss: 0.4095 - val_accuracy: 0.8953\n",
            "Epoch 638/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0828 - accuracy: 0.9641 - val_loss: 0.4314 - val_accuracy: 0.8922\n",
            "Epoch 639/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0833 - accuracy: 0.9647 - val_loss: 0.4398 - val_accuracy: 0.8968\n",
            "Epoch 640/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0856 - accuracy: 0.9634 - val_loss: 0.4287 - val_accuracy: 0.8965\n",
            "Epoch 641/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0911 - accuracy: 0.9621 - val_loss: 0.4224 - val_accuracy: 0.8943\n",
            "Epoch 642/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0851 - accuracy: 0.9634 - val_loss: 0.4354 - val_accuracy: 0.9007\n",
            "Epoch 643/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0879 - accuracy: 0.9613 - val_loss: 0.4371 - val_accuracy: 0.9002\n",
            "Epoch 644/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0930 - accuracy: 0.9608 - val_loss: 0.4208 - val_accuracy: 0.8931\n",
            "Epoch 645/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9637 - val_loss: 0.4459 - val_accuracy: 0.8943\n",
            "Epoch 646/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0834 - accuracy: 0.9661 - val_loss: 0.4462 - val_accuracy: 0.8977\n",
            "Epoch 647/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9651 - val_loss: 0.4144 - val_accuracy: 0.8999\n",
            "Epoch 648/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0799 - accuracy: 0.9666 - val_loss: 0.4457 - val_accuracy: 0.8975\n",
            "Epoch 649/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9672 - val_loss: 0.4390 - val_accuracy: 0.8943\n",
            "Epoch 650/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0792 - accuracy: 0.9659 - val_loss: 0.4310 - val_accuracy: 0.8935\n",
            "Epoch 651/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0805 - accuracy: 0.9656 - val_loss: 0.4236 - val_accuracy: 0.8948\n",
            "Epoch 652/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0830 - accuracy: 0.9634 - val_loss: 0.4303 - val_accuracy: 0.8956\n",
            "Epoch 653/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0850 - accuracy: 0.9633 - val_loss: 0.4276 - val_accuracy: 0.8972\n",
            "Epoch 654/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0795 - accuracy: 0.9661 - val_loss: 0.4329 - val_accuracy: 0.8942\n",
            "Epoch 655/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0852 - accuracy: 0.9632 - val_loss: 0.4578 - val_accuracy: 0.9000\n",
            "Epoch 656/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0822 - accuracy: 0.9643 - val_loss: 0.4492 - val_accuracy: 0.8987\n",
            "Epoch 657/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0828 - accuracy: 0.9640 - val_loss: 0.4430 - val_accuracy: 0.8979\n",
            "Epoch 658/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0845 - accuracy: 0.9648 - val_loss: 0.4448 - val_accuracy: 0.8948\n",
            "Epoch 659/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0924 - accuracy: 0.9609 - val_loss: 0.4444 - val_accuracy: 0.8969\n",
            "Epoch 660/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0829 - accuracy: 0.9639 - val_loss: 0.4387 - val_accuracy: 0.8982\n",
            "Epoch 661/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0902 - accuracy: 0.9614 - val_loss: 0.4245 - val_accuracy: 0.8996\n",
            "Epoch 662/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0808 - accuracy: 0.9665 - val_loss: 0.4175 - val_accuracy: 0.8965\n",
            "Epoch 663/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0788 - accuracy: 0.9657 - val_loss: 0.4345 - val_accuracy: 0.8976\n",
            "Epoch 664/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0830 - accuracy: 0.9646 - val_loss: 0.4403 - val_accuracy: 0.9006\n",
            "Epoch 665/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0836 - accuracy: 0.9647 - val_loss: 0.4469 - val_accuracy: 0.8950\n",
            "Epoch 666/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0812 - accuracy: 0.9651 - val_loss: 0.4590 - val_accuracy: 0.8969\n",
            "Epoch 667/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0824 - accuracy: 0.9648 - val_loss: 0.4419 - val_accuracy: 0.8960\n",
            "Epoch 668/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9678 - val_loss: 0.4272 - val_accuracy: 0.8982\n",
            "Epoch 669/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9663 - val_loss: 0.4551 - val_accuracy: 0.8908\n",
            "Epoch 670/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0780 - accuracy: 0.9677 - val_loss: 0.4452 - val_accuracy: 0.8969\n",
            "Epoch 671/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0795 - accuracy: 0.9665 - val_loss: 0.4519 - val_accuracy: 0.8969\n",
            "Epoch 672/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0838 - accuracy: 0.9638 - val_loss: 0.4224 - val_accuracy: 0.9002\n",
            "Epoch 673/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0837 - accuracy: 0.9651 - val_loss: 0.4569 - val_accuracy: 0.8905\n",
            "Epoch 674/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9654 - val_loss: 0.4597 - val_accuracy: 0.8999\n",
            "Epoch 675/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0830 - accuracy: 0.9630 - val_loss: 0.4417 - val_accuracy: 0.8952\n",
            "Epoch 676/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0810 - accuracy: 0.9655 - val_loss: 0.4460 - val_accuracy: 0.8965\n",
            "Epoch 677/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9665 - val_loss: 0.4612 - val_accuracy: 0.8960\n",
            "Epoch 678/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0806 - accuracy: 0.9638 - val_loss: 0.4540 - val_accuracy: 0.8968\n",
            "Epoch 679/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0818 - accuracy: 0.9653 - val_loss: 0.4334 - val_accuracy: 0.8950\n",
            "Epoch 680/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0817 - accuracy: 0.9656 - val_loss: 0.4572 - val_accuracy: 0.8978\n",
            "Epoch 681/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0775 - accuracy: 0.9657 - val_loss: 0.4412 - val_accuracy: 0.8946\n",
            "Epoch 682/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0823 - accuracy: 0.9648 - val_loss: 0.4812 - val_accuracy: 0.8980\n",
            "Epoch 683/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9648 - val_loss: 0.4415 - val_accuracy: 0.8973\n",
            "Epoch 684/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0864 - accuracy: 0.9639 - val_loss: 0.4354 - val_accuracy: 0.8965\n",
            "Epoch 685/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0817 - accuracy: 0.9670 - val_loss: 0.4284 - val_accuracy: 0.8977\n",
            "Epoch 686/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0789 - accuracy: 0.9664 - val_loss: 0.4614 - val_accuracy: 0.8997\n",
            "Epoch 687/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0845 - accuracy: 0.9639 - val_loss: 0.4624 - val_accuracy: 0.8964\n",
            "Epoch 688/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0774 - accuracy: 0.9678 - val_loss: 0.4439 - val_accuracy: 0.8921\n",
            "Epoch 689/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9653 - val_loss: 0.4476 - val_accuracy: 0.8971\n",
            "Epoch 690/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0785 - accuracy: 0.9668 - val_loss: 0.4648 - val_accuracy: 0.8959\n",
            "Epoch 691/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0852 - accuracy: 0.9632 - val_loss: 0.4503 - val_accuracy: 0.8973\n",
            "Epoch 692/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0774 - accuracy: 0.9674 - val_loss: 0.4117 - val_accuracy: 0.8983\n",
            "Epoch 693/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0827 - accuracy: 0.9648 - val_loss: 0.4279 - val_accuracy: 0.8933\n",
            "Epoch 694/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0791 - accuracy: 0.9664 - val_loss: 0.4255 - val_accuracy: 0.8952\n",
            "Epoch 695/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0756 - accuracy: 0.9687 - val_loss: 0.4300 - val_accuracy: 0.8994\n",
            "Epoch 696/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0831 - accuracy: 0.9640 - val_loss: 0.4468 - val_accuracy: 0.8982\n",
            "Epoch 697/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9671 - val_loss: 0.4384 - val_accuracy: 0.8953\n",
            "Epoch 698/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0716 - accuracy: 0.9696 - val_loss: 0.4609 - val_accuracy: 0.8964\n",
            "Epoch 699/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0776 - accuracy: 0.9692 - val_loss: 0.4545 - val_accuracy: 0.8967\n",
            "Epoch 700/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0765 - accuracy: 0.9692 - val_loss: 0.4706 - val_accuracy: 0.8953\n",
            "Epoch 701/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0825 - accuracy: 0.9665 - val_loss: 0.4506 - val_accuracy: 0.8973\n",
            "Epoch 702/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0810 - accuracy: 0.9646 - val_loss: 0.4634 - val_accuracy: 0.8939\n",
            "Epoch 703/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0787 - accuracy: 0.9673 - val_loss: 0.4491 - val_accuracy: 0.8926\n",
            "Epoch 704/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0784 - accuracy: 0.9662 - val_loss: 0.4483 - val_accuracy: 0.8974\n",
            "Epoch 705/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0857 - accuracy: 0.9636 - val_loss: 0.4383 - val_accuracy: 0.8988\n",
            "Epoch 706/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0824 - accuracy: 0.9650 - val_loss: 0.4483 - val_accuracy: 0.8994\n",
            "Epoch 707/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0776 - accuracy: 0.9663 - val_loss: 0.4600 - val_accuracy: 0.8935\n",
            "Epoch 708/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0831 - accuracy: 0.9644 - val_loss: 0.4132 - val_accuracy: 0.9024\n",
            "Epoch 709/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0766 - accuracy: 0.9674 - val_loss: 0.4379 - val_accuracy: 0.8971\n",
            "Epoch 710/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0815 - accuracy: 0.9658 - val_loss: 0.4412 - val_accuracy: 0.8968\n",
            "Epoch 711/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0776 - accuracy: 0.9672 - val_loss: 0.4360 - val_accuracy: 0.8947\n",
            "Epoch 712/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0765 - accuracy: 0.9681 - val_loss: 0.4118 - val_accuracy: 0.8970\n",
            "Epoch 713/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0787 - accuracy: 0.9657 - val_loss: 0.4285 - val_accuracy: 0.8973\n",
            "Epoch 714/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0801 - accuracy: 0.9666 - val_loss: 0.4528 - val_accuracy: 0.8892\n",
            "Epoch 715/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0772 - accuracy: 0.9670 - val_loss: 0.4374 - val_accuracy: 0.8962\n",
            "Epoch 716/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9674 - val_loss: 0.4809 - val_accuracy: 0.8982\n",
            "Epoch 717/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9669 - val_loss: 0.4417 - val_accuracy: 0.8993\n",
            "Epoch 718/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0817 - accuracy: 0.9643 - val_loss: 0.4802 - val_accuracy: 0.8894\n",
            "Epoch 719/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0801 - accuracy: 0.9658 - val_loss: 0.4544 - val_accuracy: 0.8945\n",
            "Epoch 720/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0844 - accuracy: 0.9644 - val_loss: 0.4556 - val_accuracy: 0.8952\n",
            "Epoch 721/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0791 - accuracy: 0.9653 - val_loss: 0.4634 - val_accuracy: 0.8935\n",
            "Epoch 722/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0802 - accuracy: 0.9656 - val_loss: 0.4500 - val_accuracy: 0.8979\n",
            "Epoch 723/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0809 - accuracy: 0.9655 - val_loss: 0.4499 - val_accuracy: 0.8946\n",
            "Epoch 724/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0764 - accuracy: 0.9681 - val_loss: 0.4711 - val_accuracy: 0.8982\n",
            "Epoch 725/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0819 - accuracy: 0.9661 - val_loss: 0.4498 - val_accuracy: 0.9037\n",
            "Epoch 726/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0845 - accuracy: 0.9652 - val_loss: 0.4456 - val_accuracy: 0.8953\n",
            "Epoch 727/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0825 - accuracy: 0.9661 - val_loss: 0.4485 - val_accuracy: 0.8978\n",
            "Epoch 728/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0757 - accuracy: 0.9673 - val_loss: 0.4588 - val_accuracy: 0.8982\n",
            "Epoch 729/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0812 - accuracy: 0.9657 - val_loss: 0.4340 - val_accuracy: 0.8950\n",
            "Epoch 730/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0767 - accuracy: 0.9675 - val_loss: 0.4402 - val_accuracy: 0.8927\n",
            "Epoch 731/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0850 - accuracy: 0.9642 - val_loss: 0.4497 - val_accuracy: 0.8986\n",
            "Epoch 732/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0810 - accuracy: 0.9662 - val_loss: 0.4598 - val_accuracy: 0.8947\n",
            "Epoch 733/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0794 - accuracy: 0.9662 - val_loss: 0.4775 - val_accuracy: 0.8946\n",
            "Epoch 734/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0850 - accuracy: 0.9644 - val_loss: 0.4284 - val_accuracy: 0.8954\n",
            "Epoch 735/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0772 - accuracy: 0.9674 - val_loss: 0.4590 - val_accuracy: 0.8974\n",
            "Epoch 736/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0828 - accuracy: 0.9654 - val_loss: 0.4574 - val_accuracy: 0.8941\n",
            "Epoch 737/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0752 - accuracy: 0.9676 - val_loss: 0.4691 - val_accuracy: 0.9003\n",
            "Epoch 738/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0820 - accuracy: 0.9657 - val_loss: 0.4408 - val_accuracy: 0.8930\n",
            "Epoch 739/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0770 - accuracy: 0.9684 - val_loss: 0.4524 - val_accuracy: 0.8953\n",
            "Epoch 740/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0827 - accuracy: 0.9642 - val_loss: 0.4335 - val_accuracy: 0.8976\n",
            "Epoch 741/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0874 - accuracy: 0.9634 - val_loss: 0.4451 - val_accuracy: 0.8960\n",
            "Epoch 742/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0759 - accuracy: 0.9679 - val_loss: 0.4545 - val_accuracy: 0.9002\n",
            "Epoch 743/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0783 - accuracy: 0.9671 - val_loss: 0.4476 - val_accuracy: 0.8992\n",
            "Epoch 744/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0740 - accuracy: 0.9698 - val_loss: 0.4520 - val_accuracy: 0.8960\n",
            "Epoch 745/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0812 - accuracy: 0.9645 - val_loss: 0.4555 - val_accuracy: 0.8983\n",
            "Epoch 746/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0783 - accuracy: 0.9663 - val_loss: 0.4570 - val_accuracy: 0.8974\n",
            "Epoch 747/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0779 - accuracy: 0.9668 - val_loss: 0.4712 - val_accuracy: 0.8926\n",
            "Epoch 748/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0812 - accuracy: 0.9656 - val_loss: 0.4520 - val_accuracy: 0.8955\n",
            "Epoch 749/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0828 - accuracy: 0.9672 - val_loss: 0.4795 - val_accuracy: 0.8872\n",
            "Epoch 750/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0804 - accuracy: 0.9655 - val_loss: 0.4352 - val_accuracy: 0.8972\n",
            "Epoch 751/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0746 - accuracy: 0.9676 - val_loss: 0.4417 - val_accuracy: 0.8941\n",
            "Epoch 752/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0765 - accuracy: 0.9685 - val_loss: 0.4548 - val_accuracy: 0.8993\n",
            "Epoch 753/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0722 - accuracy: 0.9697 - val_loss: 0.4633 - val_accuracy: 0.8994\n",
            "Epoch 754/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0794 - accuracy: 0.9661 - val_loss: 0.4653 - val_accuracy: 0.8970\n",
            "Epoch 755/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0767 - accuracy: 0.9683 - val_loss: 0.4565 - val_accuracy: 0.8965\n",
            "Epoch 756/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0730 - accuracy: 0.9686 - val_loss: 0.4578 - val_accuracy: 0.8986\n",
            "Epoch 757/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0841 - accuracy: 0.9640 - val_loss: 0.4581 - val_accuracy: 0.8954\n",
            "Epoch 758/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0711 - accuracy: 0.9710 - val_loss: 0.4420 - val_accuracy: 0.8944\n",
            "Epoch 759/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0752 - accuracy: 0.9692 - val_loss: 0.4541 - val_accuracy: 0.8930\n",
            "Epoch 760/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9696 - val_loss: 0.4601 - val_accuracy: 0.8995\n",
            "Epoch 761/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0781 - accuracy: 0.9679 - val_loss: 0.4459 - val_accuracy: 0.8995\n",
            "Epoch 762/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0819 - accuracy: 0.9647 - val_loss: 0.4595 - val_accuracy: 0.8916\n",
            "Epoch 763/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0791 - accuracy: 0.9672 - val_loss: 0.4493 - val_accuracy: 0.8948\n",
            "Epoch 764/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0790 - accuracy: 0.9666 - val_loss: 0.4398 - val_accuracy: 0.8926\n",
            "Epoch 765/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0807 - accuracy: 0.9658 - val_loss: 0.4475 - val_accuracy: 0.9023\n",
            "Epoch 766/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9675 - val_loss: 0.4487 - val_accuracy: 0.8964\n",
            "Epoch 767/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9679 - val_loss: 0.4548 - val_accuracy: 0.8969\n",
            "Epoch 768/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0743 - accuracy: 0.9685 - val_loss: 0.4620 - val_accuracy: 0.8971\n",
            "Epoch 769/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0810 - accuracy: 0.9656 - val_loss: 0.4566 - val_accuracy: 0.8980\n",
            "Epoch 770/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0807 - accuracy: 0.9653 - val_loss: 0.4670 - val_accuracy: 0.8983\n",
            "Epoch 771/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0776 - accuracy: 0.9661 - val_loss: 0.4662 - val_accuracy: 0.8955\n",
            "Epoch 772/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0764 - accuracy: 0.9680 - val_loss: 0.4788 - val_accuracy: 0.8973\n",
            "Epoch 773/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0738 - accuracy: 0.9690 - val_loss: 0.4658 - val_accuracy: 0.8961\n",
            "Epoch 774/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0741 - accuracy: 0.9692 - val_loss: 0.4554 - val_accuracy: 0.8986\n",
            "Epoch 775/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0733 - accuracy: 0.9696 - val_loss: 0.4634 - val_accuracy: 0.8912\n",
            "Epoch 776/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0788 - accuracy: 0.9663 - val_loss: 0.4568 - val_accuracy: 0.9008\n",
            "Epoch 777/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0824 - accuracy: 0.9633 - val_loss: 0.4454 - val_accuracy: 0.8963\n",
            "Epoch 778/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0752 - accuracy: 0.9693 - val_loss: 0.4821 - val_accuracy: 0.8973\n",
            "Epoch 779/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0758 - accuracy: 0.9683 - val_loss: 0.4621 - val_accuracy: 0.8960\n",
            "Epoch 780/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0748 - accuracy: 0.9684 - val_loss: 0.4774 - val_accuracy: 0.8982\n",
            "Epoch 781/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0748 - accuracy: 0.9672 - val_loss: 0.4710 - val_accuracy: 0.8961\n",
            "Epoch 782/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0820 - accuracy: 0.9663 - val_loss: 0.4441 - val_accuracy: 0.8969\n",
            "Epoch 783/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0745 - accuracy: 0.9686 - val_loss: 0.4516 - val_accuracy: 0.8939\n",
            "Epoch 784/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0776 - accuracy: 0.9671 - val_loss: 0.4657 - val_accuracy: 0.8962\n",
            "Epoch 785/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0789 - accuracy: 0.9670 - val_loss: 0.4575 - val_accuracy: 0.8901\n",
            "Epoch 786/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0757 - accuracy: 0.9677 - val_loss: 0.4805 - val_accuracy: 0.8922\n",
            "Epoch 787/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0829 - accuracy: 0.9661 - val_loss: 0.4587 - val_accuracy: 0.8927\n",
            "Epoch 788/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0737 - accuracy: 0.9701 - val_loss: 0.4718 - val_accuracy: 0.8956\n",
            "Epoch 789/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0791 - accuracy: 0.9653 - val_loss: 0.4606 - val_accuracy: 0.8960\n",
            "Epoch 790/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0738 - accuracy: 0.9688 - val_loss: 0.4732 - val_accuracy: 0.8965\n",
            "Epoch 791/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0748 - accuracy: 0.9680 - val_loss: 0.4815 - val_accuracy: 0.8939\n",
            "Epoch 792/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0796 - accuracy: 0.9673 - val_loss: 0.4509 - val_accuracy: 0.8935\n",
            "Epoch 793/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0826 - accuracy: 0.9660 - val_loss: 0.4495 - val_accuracy: 0.8952\n",
            "Epoch 794/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0734 - accuracy: 0.9687 - val_loss: 0.4843 - val_accuracy: 0.8966\n",
            "Epoch 795/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0761 - accuracy: 0.9695 - val_loss: 0.4857 - val_accuracy: 0.8982\n",
            "Epoch 796/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0843 - accuracy: 0.9658 - val_loss: 0.4493 - val_accuracy: 0.8977\n",
            "Epoch 797/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9699 - val_loss: 0.4793 - val_accuracy: 0.8939\n",
            "Epoch 798/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0780 - accuracy: 0.9675 - val_loss: 0.4803 - val_accuracy: 0.8965\n",
            "Epoch 799/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0764 - accuracy: 0.9680 - val_loss: 0.4488 - val_accuracy: 0.8952\n",
            "Epoch 800/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9683 - val_loss: 0.4860 - val_accuracy: 0.9007\n",
            "Epoch 801/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0742 - accuracy: 0.9684 - val_loss: 0.4738 - val_accuracy: 0.8964\n",
            "Epoch 802/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0699 - accuracy: 0.9714 - val_loss: 0.4777 - val_accuracy: 0.8955\n",
            "Epoch 803/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0902 - accuracy: 0.9628 - val_loss: 0.4810 - val_accuracy: 0.8981\n",
            "Epoch 804/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0743 - accuracy: 0.9691 - val_loss: 0.4709 - val_accuracy: 0.8981\n",
            "Epoch 805/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0741 - accuracy: 0.9690 - val_loss: 0.4734 - val_accuracy: 0.8948\n",
            "Epoch 806/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0825 - accuracy: 0.9655 - val_loss: 0.4720 - val_accuracy: 0.8966\n",
            "Epoch 807/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9705 - val_loss: 0.4621 - val_accuracy: 0.8941\n",
            "Epoch 808/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0784 - accuracy: 0.9668 - val_loss: 0.4823 - val_accuracy: 0.8949\n",
            "Epoch 809/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0734 - accuracy: 0.9698 - val_loss: 0.4961 - val_accuracy: 0.8967\n",
            "Epoch 810/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0828 - accuracy: 0.9656 - val_loss: 0.4638 - val_accuracy: 0.8947\n",
            "Epoch 811/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0720 - accuracy: 0.9703 - val_loss: 0.4729 - val_accuracy: 0.8961\n",
            "Epoch 812/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9704 - val_loss: 0.4622 - val_accuracy: 0.8996\n",
            "Epoch 813/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0771 - accuracy: 0.9681 - val_loss: 0.4581 - val_accuracy: 0.8884\n",
            "Epoch 814/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0809 - accuracy: 0.9643 - val_loss: 0.4624 - val_accuracy: 0.8984\n",
            "Epoch 815/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0736 - accuracy: 0.9686 - val_loss: 0.4828 - val_accuracy: 0.9015\n",
            "Epoch 816/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0711 - accuracy: 0.9700 - val_loss: 0.4727 - val_accuracy: 0.8969\n",
            "Epoch 817/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0824 - accuracy: 0.9659 - val_loss: 0.4923 - val_accuracy: 0.8952\n",
            "Epoch 818/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0768 - accuracy: 0.9680 - val_loss: 0.5104 - val_accuracy: 0.8917\n",
            "Epoch 819/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0823 - accuracy: 0.9661 - val_loss: 0.4750 - val_accuracy: 0.8937\n",
            "Epoch 820/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0708 - accuracy: 0.9711 - val_loss: 0.4698 - val_accuracy: 0.8960\n",
            "Epoch 821/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0764 - accuracy: 0.9672 - val_loss: 0.4556 - val_accuracy: 0.8942\n",
            "Epoch 822/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0748 - accuracy: 0.9680 - val_loss: 0.4817 - val_accuracy: 0.8998\n",
            "Epoch 823/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0781 - accuracy: 0.9670 - val_loss: 0.4624 - val_accuracy: 0.8956\n",
            "Epoch 824/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0773 - accuracy: 0.9683 - val_loss: 0.4666 - val_accuracy: 0.8960\n",
            "Epoch 825/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0733 - accuracy: 0.9692 - val_loss: 0.4890 - val_accuracy: 0.8926\n",
            "Epoch 826/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0755 - accuracy: 0.9700 - val_loss: 0.5036 - val_accuracy: 0.8940\n",
            "Epoch 827/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9694 - val_loss: 0.4832 - val_accuracy: 0.8960\n",
            "Epoch 828/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0818 - accuracy: 0.9681 - val_loss: 0.4530 - val_accuracy: 0.8948\n",
            "Epoch 829/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0731 - accuracy: 0.9700 - val_loss: 0.4836 - val_accuracy: 0.8986\n",
            "Epoch 830/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0703 - accuracy: 0.9704 - val_loss: 0.4948 - val_accuracy: 0.8946\n",
            "Epoch 831/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0780 - accuracy: 0.9685 - val_loss: 0.4793 - val_accuracy: 0.8975\n",
            "Epoch 832/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0755 - accuracy: 0.9677 - val_loss: 0.4713 - val_accuracy: 0.8971\n",
            "Epoch 833/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0739 - accuracy: 0.9694 - val_loss: 0.4942 - val_accuracy: 0.8951\n",
            "Epoch 834/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0768 - accuracy: 0.9678 - val_loss: 0.4732 - val_accuracy: 0.8936\n",
            "Epoch 835/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0699 - accuracy: 0.9710 - val_loss: 0.4709 - val_accuracy: 0.8974\n",
            "Epoch 836/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0748 - accuracy: 0.9681 - val_loss: 0.5114 - val_accuracy: 0.8973\n",
            "Epoch 837/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0823 - accuracy: 0.9659 - val_loss: 0.4860 - val_accuracy: 0.8950\n",
            "Epoch 838/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0864 - accuracy: 0.9645 - val_loss: 0.4638 - val_accuracy: 0.8969\n",
            "Epoch 839/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0760 - accuracy: 0.9693 - val_loss: 0.4923 - val_accuracy: 0.8953\n",
            "Epoch 840/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0737 - accuracy: 0.9687 - val_loss: 0.4738 - val_accuracy: 0.8978\n",
            "Epoch 841/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0835 - accuracy: 0.9645 - val_loss: 0.4742 - val_accuracy: 0.8944\n",
            "Epoch 842/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9682 - val_loss: 0.4577 - val_accuracy: 0.8992\n",
            "Epoch 843/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0730 - accuracy: 0.9696 - val_loss: 0.4959 - val_accuracy: 0.9012\n",
            "Epoch 844/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0709 - accuracy: 0.9689 - val_loss: 0.4974 - val_accuracy: 0.8909\n",
            "Epoch 845/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0734 - accuracy: 0.9688 - val_loss: 0.4860 - val_accuracy: 0.8986\n",
            "Epoch 846/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9694 - val_loss: 0.4833 - val_accuracy: 0.8931\n",
            "Epoch 847/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0716 - accuracy: 0.9686 - val_loss: 0.4842 - val_accuracy: 0.8881\n",
            "Epoch 848/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0719 - accuracy: 0.9701 - val_loss: 0.5034 - val_accuracy: 0.8975\n",
            "Epoch 849/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0698 - accuracy: 0.9703 - val_loss: 0.4977 - val_accuracy: 0.8959\n",
            "Epoch 850/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0845 - accuracy: 0.9660 - val_loss: 0.4728 - val_accuracy: 0.8968\n",
            "Epoch 851/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0761 - accuracy: 0.9675 - val_loss: 0.4739 - val_accuracy: 0.8958\n",
            "Epoch 852/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9694 - val_loss: 0.4815 - val_accuracy: 0.8935\n",
            "Epoch 853/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0793 - accuracy: 0.9668 - val_loss: 0.4342 - val_accuracy: 0.8939\n",
            "Epoch 854/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0796 - accuracy: 0.9656 - val_loss: 0.4980 - val_accuracy: 0.8903\n",
            "Epoch 855/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0723 - accuracy: 0.9699 - val_loss: 0.4933 - val_accuracy: 0.8920\n",
            "Epoch 856/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0830 - accuracy: 0.9680 - val_loss: 0.4796 - val_accuracy: 0.8997\n",
            "Epoch 857/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0796 - accuracy: 0.9657 - val_loss: 0.4866 - val_accuracy: 0.8956\n",
            "Epoch 858/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0734 - accuracy: 0.9694 - val_loss: 0.4939 - val_accuracy: 0.8919\n",
            "Epoch 859/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0685 - accuracy: 0.9701 - val_loss: 0.5102 - val_accuracy: 0.8956\n",
            "Epoch 860/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0808 - accuracy: 0.9674 - val_loss: 0.4999 - val_accuracy: 0.8965\n",
            "Epoch 861/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0758 - accuracy: 0.9694 - val_loss: 0.4979 - val_accuracy: 0.8932\n",
            "Epoch 862/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0713 - accuracy: 0.9700 - val_loss: 0.4963 - val_accuracy: 0.8948\n",
            "Epoch 863/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0707 - accuracy: 0.9694 - val_loss: 0.4904 - val_accuracy: 0.8988\n",
            "Epoch 864/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0746 - accuracy: 0.9699 - val_loss: 0.5069 - val_accuracy: 0.8931\n",
            "Epoch 865/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0785 - accuracy: 0.9665 - val_loss: 0.5078 - val_accuracy: 0.8932\n",
            "Epoch 866/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0757 - accuracy: 0.9685 - val_loss: 0.5071 - val_accuracy: 0.9009\n",
            "Epoch 867/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0818 - accuracy: 0.9657 - val_loss: 0.4798 - val_accuracy: 0.8962\n",
            "Epoch 868/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0711 - accuracy: 0.9707 - val_loss: 0.4958 - val_accuracy: 0.8912\n",
            "Epoch 869/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0735 - accuracy: 0.9687 - val_loss: 0.4891 - val_accuracy: 0.8990\n",
            "Epoch 870/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0795 - accuracy: 0.9667 - val_loss: 0.4906 - val_accuracy: 0.8952\n",
            "Epoch 871/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0821 - accuracy: 0.9663 - val_loss: 0.4738 - val_accuracy: 0.8937\n",
            "Epoch 872/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0750 - accuracy: 0.9685 - val_loss: 0.4884 - val_accuracy: 0.8953\n",
            "Epoch 873/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0729 - accuracy: 0.9690 - val_loss: 0.4702 - val_accuracy: 0.8959\n",
            "Epoch 874/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0861 - accuracy: 0.9620 - val_loss: 0.5066 - val_accuracy: 0.8905\n",
            "Epoch 875/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0766 - accuracy: 0.9676 - val_loss: 0.4983 - val_accuracy: 0.8952\n",
            "Epoch 876/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0738 - accuracy: 0.9694 - val_loss: 0.4748 - val_accuracy: 0.8926\n",
            "Epoch 877/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9699 - val_loss: 0.4947 - val_accuracy: 0.8975\n",
            "Epoch 878/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0716 - accuracy: 0.9694 - val_loss: 0.5079 - val_accuracy: 0.8977\n",
            "Epoch 879/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0744 - accuracy: 0.9687 - val_loss: 0.5088 - val_accuracy: 0.8954\n",
            "Epoch 880/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0731 - accuracy: 0.9693 - val_loss: 0.5299 - val_accuracy: 0.8971\n",
            "Epoch 881/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9691 - val_loss: 0.5037 - val_accuracy: 0.8957\n",
            "Epoch 882/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0782 - accuracy: 0.9666 - val_loss: 0.5012 - val_accuracy: 0.8918\n",
            "Epoch 883/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0771 - accuracy: 0.9684 - val_loss: 0.4917 - val_accuracy: 0.8977\n",
            "Epoch 884/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0770 - accuracy: 0.9672 - val_loss: 0.5081 - val_accuracy: 0.8919\n",
            "Epoch 885/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0733 - accuracy: 0.9685 - val_loss: 0.5100 - val_accuracy: 0.8973\n",
            "Epoch 886/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0763 - accuracy: 0.9686 - val_loss: 0.4918 - val_accuracy: 0.8951\n",
            "Epoch 887/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0741 - accuracy: 0.9701 - val_loss: 0.5102 - val_accuracy: 0.8951\n",
            "Epoch 888/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0789 - accuracy: 0.9669 - val_loss: 0.4843 - val_accuracy: 0.8949\n",
            "Epoch 889/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0722 - accuracy: 0.9704 - val_loss: 0.5151 - val_accuracy: 0.8901\n",
            "Epoch 890/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0784 - accuracy: 0.9669 - val_loss: 0.5064 - val_accuracy: 0.8900\n",
            "Epoch 891/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0751 - accuracy: 0.9690 - val_loss: 0.5062 - val_accuracy: 0.8955\n",
            "Epoch 892/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0737 - accuracy: 0.9705 - val_loss: 0.4963 - val_accuracy: 0.8963\n",
            "Epoch 893/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9680 - val_loss: 0.5022 - val_accuracy: 0.9007\n",
            "Epoch 894/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0721 - accuracy: 0.9703 - val_loss: 0.4799 - val_accuracy: 0.8951\n",
            "Epoch 895/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0732 - accuracy: 0.9699 - val_loss: 0.5271 - val_accuracy: 0.8968\n",
            "Epoch 896/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0737 - accuracy: 0.9680 - val_loss: 0.5005 - val_accuracy: 0.8982\n",
            "Epoch 897/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0746 - accuracy: 0.9686 - val_loss: 0.5302 - val_accuracy: 0.8965\n",
            "Epoch 898/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0799 - accuracy: 0.9669 - val_loss: 0.4958 - val_accuracy: 0.8915\n",
            "Epoch 899/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0760 - accuracy: 0.9682 - val_loss: 0.5174 - val_accuracy: 0.8952\n",
            "Epoch 900/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0799 - accuracy: 0.9658 - val_loss: 0.5546 - val_accuracy: 0.9010\n",
            "Epoch 901/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0780 - accuracy: 0.9670 - val_loss: 0.4980 - val_accuracy: 0.8873\n",
            "Epoch 902/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0727 - accuracy: 0.9694 - val_loss: 0.4788 - val_accuracy: 0.8977\n",
            "Epoch 903/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0712 - accuracy: 0.9709 - val_loss: 0.5114 - val_accuracy: 0.8928\n",
            "Epoch 904/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0716 - accuracy: 0.9696 - val_loss: 0.5186 - val_accuracy: 0.8977\n",
            "Epoch 905/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0766 - accuracy: 0.9686 - val_loss: 0.4956 - val_accuracy: 0.8977\n",
            "Epoch 906/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0753 - accuracy: 0.9691 - val_loss: 0.5330 - val_accuracy: 0.8993\n",
            "Epoch 907/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0691 - accuracy: 0.9714 - val_loss: 0.5326 - val_accuracy: 0.8994\n",
            "Epoch 908/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0772 - accuracy: 0.9683 - val_loss: 0.5140 - val_accuracy: 0.8952\n",
            "Epoch 909/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0691 - accuracy: 0.9725 - val_loss: 0.5054 - val_accuracy: 0.9009\n",
            "Epoch 910/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0763 - accuracy: 0.9689 - val_loss: 0.5107 - val_accuracy: 0.8952\n",
            "Epoch 911/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0794 - accuracy: 0.9661 - val_loss: 0.4900 - val_accuracy: 0.8948\n",
            "Epoch 912/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0715 - accuracy: 0.9703 - val_loss: 0.4958 - val_accuracy: 0.8980\n",
            "Epoch 913/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0744 - accuracy: 0.9685 - val_loss: 0.5193 - val_accuracy: 0.8986\n",
            "Epoch 914/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0691 - accuracy: 0.9715 - val_loss: 0.5238 - val_accuracy: 0.8970\n",
            "Epoch 915/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0809 - accuracy: 0.9681 - val_loss: 0.5021 - val_accuracy: 0.8928\n",
            "Epoch 916/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0732 - accuracy: 0.9687 - val_loss: 0.5446 - val_accuracy: 0.8920\n",
            "Epoch 917/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0759 - accuracy: 0.9692 - val_loss: 0.5115 - val_accuracy: 0.8973\n",
            "Epoch 918/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0769 - accuracy: 0.9685 - val_loss: 0.4960 - val_accuracy: 0.8961\n",
            "Epoch 919/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0682 - accuracy: 0.9719 - val_loss: 0.5225 - val_accuracy: 0.8957\n",
            "Epoch 920/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0700 - accuracy: 0.9720 - val_loss: 0.5005 - val_accuracy: 0.8971\n",
            "Epoch 921/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0726 - accuracy: 0.9689 - val_loss: 0.5444 - val_accuracy: 0.8925\n",
            "Epoch 922/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0753 - accuracy: 0.9693 - val_loss: 0.5068 - val_accuracy: 0.8910\n",
            "Epoch 923/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0808 - accuracy: 0.9658 - val_loss: 0.5085 - val_accuracy: 0.8909\n",
            "Epoch 924/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0726 - accuracy: 0.9686 - val_loss: 0.5224 - val_accuracy: 0.8967\n",
            "Epoch 925/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0689 - accuracy: 0.9710 - val_loss: 0.5118 - val_accuracy: 0.8960\n",
            "Epoch 926/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0752 - accuracy: 0.9685 - val_loss: 0.5088 - val_accuracy: 0.8973\n",
            "Epoch 927/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0773 - accuracy: 0.9677 - val_loss: 0.5165 - val_accuracy: 0.8934\n",
            "Epoch 928/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0752 - accuracy: 0.9708 - val_loss: 0.5178 - val_accuracy: 0.8961\n",
            "Epoch 929/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9697 - val_loss: 0.5471 - val_accuracy: 0.8927\n",
            "Epoch 930/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0676 - accuracy: 0.9711 - val_loss: 0.5167 - val_accuracy: 0.8948\n",
            "Epoch 931/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0774 - accuracy: 0.9683 - val_loss: 0.4922 - val_accuracy: 0.8974\n",
            "Epoch 932/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0737 - accuracy: 0.9696 - val_loss: 0.5290 - val_accuracy: 0.8945\n",
            "Epoch 933/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0748 - accuracy: 0.9695 - val_loss: 0.4981 - val_accuracy: 0.8949\n",
            "Epoch 934/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0724 - accuracy: 0.9703 - val_loss: 0.4872 - val_accuracy: 0.8940\n",
            "Epoch 935/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0743 - accuracy: 0.9696 - val_loss: 0.4989 - val_accuracy: 0.8950\n",
            "Epoch 936/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0704 - accuracy: 0.9710 - val_loss: 0.5228 - val_accuracy: 0.8948\n",
            "Epoch 937/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0728 - accuracy: 0.9692 - val_loss: 0.5257 - val_accuracy: 0.8989\n",
            "Epoch 938/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0715 - accuracy: 0.9704 - val_loss: 0.5513 - val_accuracy: 0.8970\n",
            "Epoch 939/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0697 - accuracy: 0.9709 - val_loss: 0.5151 - val_accuracy: 0.8947\n",
            "Epoch 940/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0751 - accuracy: 0.9689 - val_loss: 0.5027 - val_accuracy: 0.8994\n",
            "Epoch 941/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0733 - accuracy: 0.9701 - val_loss: 0.4911 - val_accuracy: 0.8905\n",
            "Epoch 942/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0738 - accuracy: 0.9699 - val_loss: 0.5142 - val_accuracy: 0.8956\n",
            "Epoch 943/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0724 - accuracy: 0.9701 - val_loss: 0.5277 - val_accuracy: 0.8935\n",
            "Epoch 944/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0681 - accuracy: 0.9720 - val_loss: 0.5200 - val_accuracy: 0.8982\n",
            "Epoch 945/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0734 - accuracy: 0.9688 - val_loss: 0.5322 - val_accuracy: 0.8932\n",
            "Epoch 946/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0685 - accuracy: 0.9710 - val_loss: 0.5350 - val_accuracy: 0.8955\n",
            "Epoch 947/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0747 - accuracy: 0.9695 - val_loss: 0.5210 - val_accuracy: 0.8955\n",
            "Epoch 948/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0739 - accuracy: 0.9698 - val_loss: 0.5209 - val_accuracy: 0.8956\n",
            "Epoch 949/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0707 - accuracy: 0.9719 - val_loss: 0.5472 - val_accuracy: 0.8956\n",
            "Epoch 950/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0715 - accuracy: 0.9705 - val_loss: 0.5251 - val_accuracy: 0.8924\n",
            "Epoch 951/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0763 - accuracy: 0.9692 - val_loss: 0.5117 - val_accuracy: 0.8970\n",
            "Epoch 952/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0782 - accuracy: 0.9667 - val_loss: 0.5292 - val_accuracy: 0.9001\n",
            "Epoch 953/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0743 - accuracy: 0.9701 - val_loss: 0.4497 - val_accuracy: 0.8979\n",
            "Epoch 954/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0714 - accuracy: 0.9713 - val_loss: 0.5169 - val_accuracy: 0.8964\n",
            "Epoch 955/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0702 - accuracy: 0.9699 - val_loss: 0.5495 - val_accuracy: 0.8901\n",
            "Epoch 956/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0765 - accuracy: 0.9682 - val_loss: 0.4926 - val_accuracy: 0.8899\n",
            "Epoch 957/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0703 - accuracy: 0.9706 - val_loss: 0.5220 - val_accuracy: 0.8960\n",
            "Epoch 958/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0720 - accuracy: 0.9702 - val_loss: 0.5143 - val_accuracy: 0.9023\n",
            "Epoch 959/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0725 - accuracy: 0.9709 - val_loss: 0.5225 - val_accuracy: 0.8953\n",
            "Epoch 960/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0715 - accuracy: 0.9709 - val_loss: 0.5172 - val_accuracy: 0.8912\n",
            "Epoch 961/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0763 - accuracy: 0.9681 - val_loss: 0.5285 - val_accuracy: 0.8971\n",
            "Epoch 962/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0723 - accuracy: 0.9696 - val_loss: 0.5532 - val_accuracy: 0.8960\n",
            "Epoch 963/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0685 - accuracy: 0.9712 - val_loss: 0.5689 - val_accuracy: 0.8914\n",
            "Epoch 964/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0823 - accuracy: 0.9662 - val_loss: 0.5272 - val_accuracy: 0.8960\n",
            "Epoch 965/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0724 - accuracy: 0.9698 - val_loss: 0.5353 - val_accuracy: 0.8965\n",
            "Epoch 966/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0790 - accuracy: 0.9662 - val_loss: 0.5242 - val_accuracy: 0.8954\n",
            "Epoch 967/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0764 - accuracy: 0.9676 - val_loss: 0.4932 - val_accuracy: 0.8895\n",
            "Epoch 968/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0701 - accuracy: 0.9697 - val_loss: 0.5298 - val_accuracy: 0.8959\n",
            "Epoch 969/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0736 - accuracy: 0.9694 - val_loss: 0.5488 - val_accuracy: 0.8904\n",
            "Epoch 970/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0743 - accuracy: 0.9686 - val_loss: 0.5499 - val_accuracy: 0.8926\n",
            "Epoch 971/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0725 - accuracy: 0.9692 - val_loss: 0.5434 - val_accuracy: 0.8967\n",
            "Epoch 972/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0710 - accuracy: 0.9703 - val_loss: 0.5248 - val_accuracy: 0.8922\n",
            "Epoch 973/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0764 - accuracy: 0.9684 - val_loss: 0.5207 - val_accuracy: 0.8943\n",
            "Epoch 974/1024\n",
            "901/901 [==============================] - 2s 3ms/step - loss: 0.0725 - accuracy: 0.9706 - val_loss: 0.5000 - val_accuracy: 0.8952\n",
            "Epoch 975/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0703 - accuracy: 0.9703 - val_loss: 0.5370 - val_accuracy: 0.8958\n",
            "Epoch 976/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0732 - accuracy: 0.9696 - val_loss: 0.5301 - val_accuracy: 0.8951\n",
            "Epoch 977/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0718 - accuracy: 0.9708 - val_loss: 0.5250 - val_accuracy: 0.8931\n",
            "Epoch 978/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0701 - accuracy: 0.9706 - val_loss: 0.5487 - val_accuracy: 0.8950\n",
            "Epoch 979/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0719 - accuracy: 0.9701 - val_loss: 0.5187 - val_accuracy: 0.8962\n",
            "Epoch 980/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0710 - accuracy: 0.9716 - val_loss: 0.5315 - val_accuracy: 0.8905\n",
            "Epoch 981/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0749 - accuracy: 0.9679 - val_loss: 0.5056 - val_accuracy: 0.8934\n",
            "Epoch 982/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0658 - accuracy: 0.9734 - val_loss: 0.5249 - val_accuracy: 0.8939\n",
            "Epoch 983/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0765 - accuracy: 0.9686 - val_loss: 0.5106 - val_accuracy: 0.8961\n",
            "Epoch 984/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0697 - accuracy: 0.9711 - val_loss: 0.5129 - val_accuracy: 0.8956\n",
            "Epoch 985/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0697 - accuracy: 0.9718 - val_loss: 0.5424 - val_accuracy: 0.8938\n",
            "Epoch 986/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0739 - accuracy: 0.9704 - val_loss: 0.5213 - val_accuracy: 0.8934\n",
            "Epoch 987/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0772 - accuracy: 0.9677 - val_loss: 0.5311 - val_accuracy: 0.8970\n",
            "Epoch 988/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0749 - accuracy: 0.9687 - val_loss: 0.4938 - val_accuracy: 0.8952\n",
            "Epoch 989/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0767 - accuracy: 0.9688 - val_loss: 0.5092 - val_accuracy: 0.8969\n",
            "Epoch 990/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0704 - accuracy: 0.9696 - val_loss: 0.5432 - val_accuracy: 0.8935\n",
            "Epoch 991/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0740 - accuracy: 0.9682 - val_loss: 0.5442 - val_accuracy: 0.8982\n",
            "Epoch 992/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0753 - accuracy: 0.9689 - val_loss: 0.5167 - val_accuracy: 0.8953\n",
            "Epoch 993/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0671 - accuracy: 0.9709 - val_loss: 0.5312 - val_accuracy: 0.8968\n",
            "Epoch 994/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0770 - accuracy: 0.9683 - val_loss: 0.5360 - val_accuracy: 0.8948\n",
            "Epoch 995/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0744 - accuracy: 0.9687 - val_loss: 0.4984 - val_accuracy: 0.8916\n",
            "Epoch 996/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0735 - accuracy: 0.9720 - val_loss: 0.5165 - val_accuracy: 0.8908\n",
            "Epoch 997/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0727 - accuracy: 0.9689 - val_loss: 0.5644 - val_accuracy: 0.9005\n",
            "Epoch 998/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0689 - accuracy: 0.9713 - val_loss: 0.5496 - val_accuracy: 0.8969\n",
            "Epoch 999/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0762 - accuracy: 0.9687 - val_loss: 0.5785 - val_accuracy: 0.8899\n",
            "Epoch 1000/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0744 - accuracy: 0.9687 - val_loss: 0.5414 - val_accuracy: 0.8969\n",
            "Epoch 1001/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0697 - accuracy: 0.9707 - val_loss: 0.5318 - val_accuracy: 0.8922\n",
            "Epoch 1002/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0800 - accuracy: 0.9673 - val_loss: 0.5223 - val_accuracy: 0.8969\n",
            "Epoch 1003/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0705 - accuracy: 0.9713 - val_loss: 0.5114 - val_accuracy: 0.8948\n",
            "Epoch 1004/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0701 - accuracy: 0.9723 - val_loss: 0.5244 - val_accuracy: 0.8929\n",
            "Epoch 1005/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0709 - accuracy: 0.9700 - val_loss: 0.4867 - val_accuracy: 0.8982\n",
            "Epoch 1006/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0701 - accuracy: 0.9712 - val_loss: 0.5325 - val_accuracy: 0.8956\n",
            "Epoch 1007/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0734 - accuracy: 0.9691 - val_loss: 0.5095 - val_accuracy: 0.8949\n",
            "Epoch 1008/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0706 - accuracy: 0.9695 - val_loss: 0.5382 - val_accuracy: 0.8927\n",
            "Epoch 1009/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0685 - accuracy: 0.9708 - val_loss: 0.5678 - val_accuracy: 0.8918\n",
            "Epoch 1010/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0731 - accuracy: 0.9698 - val_loss: 0.5399 - val_accuracy: 0.9003\n",
            "Epoch 1011/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0725 - accuracy: 0.9714 - val_loss: 0.5159 - val_accuracy: 0.8949\n",
            "Epoch 1012/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0688 - accuracy: 0.9719 - val_loss: 0.4977 - val_accuracy: 0.8960\n",
            "Epoch 1013/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0814 - accuracy: 0.9667 - val_loss: 0.5490 - val_accuracy: 0.8955\n",
            "Epoch 1014/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.5314 - val_accuracy: 0.8979\n",
            "Epoch 1015/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0733 - accuracy: 0.9698 - val_loss: 0.5473 - val_accuracy: 0.8935\n",
            "Epoch 1016/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0748 - accuracy: 0.9707 - val_loss: 0.5398 - val_accuracy: 0.8947\n",
            "Epoch 1017/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0727 - accuracy: 0.9681 - val_loss: 0.5383 - val_accuracy: 0.8919\n",
            "Epoch 1018/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0712 - accuracy: 0.9709 - val_loss: 0.5095 - val_accuracy: 0.8953\n",
            "Epoch 1019/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0710 - accuracy: 0.9716 - val_loss: 0.5244 - val_accuracy: 0.8952\n",
            "Epoch 1020/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0672 - accuracy: 0.9716 - val_loss: 0.5134 - val_accuracy: 0.8954\n",
            "Epoch 1021/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0692 - accuracy: 0.9708 - val_loss: 0.5107 - val_accuracy: 0.8985\n",
            "Epoch 1022/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0670 - accuracy: 0.9718 - val_loss: 0.5414 - val_accuracy: 0.8976\n",
            "Epoch 1023/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0695 - accuracy: 0.9710 - val_loss: 0.5648 - val_accuracy: 0.8987\n",
            "Epoch 1024/1024\n",
            "901/901 [==============================] - 3s 3ms/step - loss: 0.0732 - accuracy: 0.9697 - val_loss: 0.5477 - val_accuracy: 0.8962\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy8ZJVIG9_0P"
      },
      "source": [
        "#0.3\n",
        "model_r_l = Sequential()\n",
        "model_r_l.add(Dense(1, input_dim = 20,activation='sigmoid'))\n",
        "model_r_l.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "history = model_r_l.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Stlm2jvNsWx0",
        "outputId": "014c29cf-38e6-4343-f541-0ecec6416d55"
      },
      "source": [
        "#0.3\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim = 20,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3235 - accuracy: 0.8870 - val_loss: 0.2940 - val_accuracy: 0.8931\n",
            "Epoch 2/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.3066 - accuracy: 0.8843 - val_loss: 0.2830 - val_accuracy: 0.8954\n",
            "Epoch 3/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2937 - accuracy: 0.8865 - val_loss: 0.2751 - val_accuracy: 0.8978\n",
            "Epoch 4/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2847 - accuracy: 0.8918 - val_loss: 0.2694 - val_accuracy: 0.9010\n",
            "Epoch 5/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2789 - accuracy: 0.8953 - val_loss: 0.2642 - val_accuracy: 0.9034\n",
            "Epoch 6/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2722 - accuracy: 0.8989 - val_loss: 0.2598 - val_accuracy: 0.9039\n",
            "Epoch 7/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2718 - accuracy: 0.8946 - val_loss: 0.2558 - val_accuracy: 0.9040\n",
            "Epoch 8/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2690 - accuracy: 0.8978 - val_loss: 0.2521 - val_accuracy: 0.9044\n",
            "Epoch 9/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2587 - accuracy: 0.8987 - val_loss: 0.2491 - val_accuracy: 0.9050\n",
            "Epoch 10/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2566 - accuracy: 0.9007 - val_loss: 0.2466 - val_accuracy: 0.9051\n",
            "Epoch 11/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2594 - accuracy: 0.8977 - val_loss: 0.2436 - val_accuracy: 0.9051\n",
            "Epoch 12/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2510 - accuracy: 0.9004 - val_loss: 0.2413 - val_accuracy: 0.9059\n",
            "Epoch 13/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2475 - accuracy: 0.9033 - val_loss: 0.2391 - val_accuracy: 0.9063\n",
            "Epoch 14/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2487 - accuracy: 0.9008 - val_loss: 0.2379 - val_accuracy: 0.9072\n",
            "Epoch 15/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2495 - accuracy: 0.9008 - val_loss: 0.2352 - val_accuracy: 0.9072\n",
            "Epoch 16/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2443 - accuracy: 0.9015 - val_loss: 0.2336 - val_accuracy: 0.9077\n",
            "Epoch 17/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2421 - accuracy: 0.9034 - val_loss: 0.2317 - val_accuracy: 0.9076\n",
            "Epoch 18/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2409 - accuracy: 0.9025 - val_loss: 0.2307 - val_accuracy: 0.9080\n",
            "Epoch 19/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2396 - accuracy: 0.9024 - val_loss: 0.2290 - val_accuracy: 0.9081\n",
            "Epoch 20/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2389 - accuracy: 0.9032 - val_loss: 0.2282 - val_accuracy: 0.9080\n",
            "Epoch 21/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2335 - accuracy: 0.9063 - val_loss: 0.2265 - val_accuracy: 0.9084\n",
            "Epoch 22/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2368 - accuracy: 0.9041 - val_loss: 0.2256 - val_accuracy: 0.9089\n",
            "Epoch 23/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2314 - accuracy: 0.9059 - val_loss: 0.2242 - val_accuracy: 0.9092\n",
            "Epoch 24/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2409 - accuracy: 0.9017 - val_loss: 0.2232 - val_accuracy: 0.9097\n",
            "Epoch 25/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2327 - accuracy: 0.9036 - val_loss: 0.2226 - val_accuracy: 0.9096\n",
            "Epoch 26/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2360 - accuracy: 0.9034 - val_loss: 0.2215 - val_accuracy: 0.9097\n",
            "Epoch 27/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2287 - accuracy: 0.9076 - val_loss: 0.2208 - val_accuracy: 0.9098\n",
            "Epoch 28/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2335 - accuracy: 0.9058 - val_loss: 0.2204 - val_accuracy: 0.9099\n",
            "Epoch 29/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2330 - accuracy: 0.9041 - val_loss: 0.2194 - val_accuracy: 0.9099\n",
            "Epoch 30/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2294 - accuracy: 0.9061 - val_loss: 0.2194 - val_accuracy: 0.9109\n",
            "Epoch 31/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2280 - accuracy: 0.9066 - val_loss: 0.2182 - val_accuracy: 0.9104\n",
            "Epoch 32/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2287 - accuracy: 0.9048 - val_loss: 0.2178 - val_accuracy: 0.9107\n",
            "Epoch 33/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2254 - accuracy: 0.9073 - val_loss: 0.2173 - val_accuracy: 0.9106\n",
            "Epoch 34/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2312 - accuracy: 0.9064 - val_loss: 0.2168 - val_accuracy: 0.9109\n",
            "Epoch 35/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2245 - accuracy: 0.9092 - val_loss: 0.2165 - val_accuracy: 0.9110\n",
            "Epoch 36/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2230 - accuracy: 0.9072 - val_loss: 0.2163 - val_accuracy: 0.9114\n",
            "Epoch 37/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2239 - accuracy: 0.9072 - val_loss: 0.2157 - val_accuracy: 0.9112\n",
            "Epoch 38/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2272 - accuracy: 0.9068 - val_loss: 0.2159 - val_accuracy: 0.9124\n",
            "Epoch 39/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2257 - accuracy: 0.9075 - val_loss: 0.2152 - val_accuracy: 0.9119\n",
            "Epoch 40/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2206 - accuracy: 0.9088 - val_loss: 0.2154 - val_accuracy: 0.9129\n",
            "Epoch 41/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2245 - accuracy: 0.9074 - val_loss: 0.2148 - val_accuracy: 0.9125\n",
            "Epoch 42/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2287 - accuracy: 0.9063 - val_loss: 0.2144 - val_accuracy: 0.9122\n",
            "Epoch 43/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2189 - accuracy: 0.9098 - val_loss: 0.2141 - val_accuracy: 0.9116\n",
            "Epoch 44/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2221 - accuracy: 0.9076 - val_loss: 0.2139 - val_accuracy: 0.9123\n",
            "Epoch 45/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2172 - accuracy: 0.9105 - val_loss: 0.2140 - val_accuracy: 0.9129\n",
            "Epoch 46/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2248 - accuracy: 0.9070 - val_loss: 0.2135 - val_accuracy: 0.9121\n",
            "Epoch 47/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2148 - accuracy: 0.9124 - val_loss: 0.2135 - val_accuracy: 0.9125\n",
            "Epoch 48/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2219 - accuracy: 0.9074 - val_loss: 0.2134 - val_accuracy: 0.9126\n",
            "Epoch 49/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2206 - accuracy: 0.9099 - val_loss: 0.2138 - val_accuracy: 0.9126\n",
            "Epoch 50/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2173 - accuracy: 0.9108 - val_loss: 0.2133 - val_accuracy: 0.9126\n",
            "Epoch 51/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2246 - accuracy: 0.9067 - val_loss: 0.2128 - val_accuracy: 0.9124\n",
            "Epoch 52/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2214 - accuracy: 0.9071 - val_loss: 0.2132 - val_accuracy: 0.9125\n",
            "Epoch 53/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2227 - accuracy: 0.9065 - val_loss: 0.2126 - val_accuracy: 0.9126\n",
            "Epoch 54/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2167 - accuracy: 0.9099 - val_loss: 0.2128 - val_accuracy: 0.9126\n",
            "Epoch 55/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2178 - accuracy: 0.9097 - val_loss: 0.2124 - val_accuracy: 0.9126\n",
            "Epoch 56/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2209 - accuracy: 0.9085 - val_loss: 0.2128 - val_accuracy: 0.9134\n",
            "Epoch 57/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2135 - accuracy: 0.9120 - val_loss: 0.2123 - val_accuracy: 0.9125\n",
            "Epoch 58/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2224 - accuracy: 0.9067 - val_loss: 0.2122 - val_accuracy: 0.9122\n",
            "Epoch 59/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2191 - accuracy: 0.9082 - val_loss: 0.2124 - val_accuracy: 0.9132\n",
            "Epoch 60/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2231 - accuracy: 0.9063 - val_loss: 0.2122 - val_accuracy: 0.9130\n",
            "Epoch 61/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2223 - accuracy: 0.9079 - val_loss: 0.2120 - val_accuracy: 0.9128\n",
            "Epoch 62/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2183 - accuracy: 0.9100 - val_loss: 0.2122 - val_accuracy: 0.9135\n",
            "Epoch 63/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2195 - accuracy: 0.9086 - val_loss: 0.2120 - val_accuracy: 0.9133\n",
            "Epoch 64/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2192 - accuracy: 0.9096 - val_loss: 0.2122 - val_accuracy: 0.9135\n",
            "Epoch 65/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2211 - accuracy: 0.9083 - val_loss: 0.2119 - val_accuracy: 0.9131\n",
            "Epoch 66/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2251 - accuracy: 0.9050 - val_loss: 0.2117 - val_accuracy: 0.9130\n",
            "Epoch 67/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2160 - accuracy: 0.9109 - val_loss: 0.2121 - val_accuracy: 0.9135\n",
            "Epoch 68/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2230 - accuracy: 0.9066 - val_loss: 0.2116 - val_accuracy: 0.9130\n",
            "Epoch 69/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2279 - accuracy: 0.9036 - val_loss: 0.2116 - val_accuracy: 0.9136\n",
            "Epoch 70/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2155 - accuracy: 0.9112 - val_loss: 0.2116 - val_accuracy: 0.9133\n",
            "Epoch 71/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2183 - accuracy: 0.9097 - val_loss: 0.2115 - val_accuracy: 0.9134\n",
            "Epoch 72/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2224 - accuracy: 0.9073 - val_loss: 0.2115 - val_accuracy: 0.9137\n",
            "Epoch 73/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2182 - accuracy: 0.9090 - val_loss: 0.2115 - val_accuracy: 0.9138\n",
            "Epoch 74/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2170 - accuracy: 0.9096 - val_loss: 0.2115 - val_accuracy: 0.9132\n",
            "Epoch 75/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2192 - accuracy: 0.9069 - val_loss: 0.2115 - val_accuracy: 0.9133\n",
            "Epoch 76/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2129 - accuracy: 0.9100 - val_loss: 0.2116 - val_accuracy: 0.9133\n",
            "Epoch 77/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2207 - accuracy: 0.9085 - val_loss: 0.2114 - val_accuracy: 0.9133\n",
            "Epoch 78/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2205 - accuracy: 0.9089 - val_loss: 0.2114 - val_accuracy: 0.9135\n",
            "Epoch 79/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2195 - accuracy: 0.9082 - val_loss: 0.2114 - val_accuracy: 0.9135\n",
            "Epoch 80/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2207 - accuracy: 0.9092 - val_loss: 0.2116 - val_accuracy: 0.9135\n",
            "Epoch 81/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2177 - accuracy: 0.9091 - val_loss: 0.2126 - val_accuracy: 0.9133\n",
            "Epoch 82/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2182 - accuracy: 0.9109 - val_loss: 0.2114 - val_accuracy: 0.9135\n",
            "Epoch 83/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2165 - accuracy: 0.9104 - val_loss: 0.2112 - val_accuracy: 0.9137\n",
            "Epoch 84/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2168 - accuracy: 0.9099 - val_loss: 0.2117 - val_accuracy: 0.9135\n",
            "Epoch 85/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2187 - accuracy: 0.9113 - val_loss: 0.2113 - val_accuracy: 0.9135\n",
            "Epoch 86/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2168 - accuracy: 0.9109 - val_loss: 0.2114 - val_accuracy: 0.9135\n",
            "Epoch 87/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2197 - accuracy: 0.9071 - val_loss: 0.2111 - val_accuracy: 0.9137\n",
            "Epoch 88/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2201 - accuracy: 0.9075 - val_loss: 0.2113 - val_accuracy: 0.9138\n",
            "Epoch 89/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2176 - accuracy: 0.9080 - val_loss: 0.2111 - val_accuracy: 0.9137\n",
            "Epoch 90/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2260 - accuracy: 0.9047 - val_loss: 0.2111 - val_accuracy: 0.9137\n",
            "Epoch 91/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2196 - accuracy: 0.9105 - val_loss: 0.2112 - val_accuracy: 0.9136\n",
            "Epoch 92/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2209 - accuracy: 0.9089 - val_loss: 0.2114 - val_accuracy: 0.9139\n",
            "Epoch 93/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2244 - accuracy: 0.9069 - val_loss: 0.2112 - val_accuracy: 0.9131\n",
            "Epoch 94/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2175 - accuracy: 0.9090 - val_loss: 0.2115 - val_accuracy: 0.9135\n",
            "Epoch 95/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2199 - accuracy: 0.9083 - val_loss: 0.2111 - val_accuracy: 0.9132\n",
            "Epoch 96/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2161 - accuracy: 0.9100 - val_loss: 0.2111 - val_accuracy: 0.9132\n",
            "Epoch 97/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2169 - accuracy: 0.9108 - val_loss: 0.2112 - val_accuracy: 0.9138\n",
            "Epoch 98/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2182 - accuracy: 0.9098 - val_loss: 0.2111 - val_accuracy: 0.9131\n",
            "Epoch 99/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2219 - accuracy: 0.9093 - val_loss: 0.2112 - val_accuracy: 0.9139\n",
            "Epoch 100/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2165 - accuracy: 0.9099 - val_loss: 0.2112 - val_accuracy: 0.9139\n",
            "Epoch 101/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2148 - accuracy: 0.9103 - val_loss: 0.2112 - val_accuracy: 0.9139\n",
            "Epoch 102/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9106 - val_loss: 0.2110 - val_accuracy: 0.9135\n",
            "Epoch 103/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2130 - accuracy: 0.9119 - val_loss: 0.2112 - val_accuracy: 0.9140\n",
            "Epoch 104/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2144 - accuracy: 0.9119 - val_loss: 0.2110 - val_accuracy: 0.9133\n",
            "Epoch 105/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2202 - accuracy: 0.9081 - val_loss: 0.2111 - val_accuracy: 0.9134\n",
            "Epoch 106/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2190 - accuracy: 0.9094 - val_loss: 0.2110 - val_accuracy: 0.9131\n",
            "Epoch 107/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2218 - accuracy: 0.9070 - val_loss: 0.2110 - val_accuracy: 0.9132\n",
            "Epoch 108/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2188 - accuracy: 0.9093 - val_loss: 0.2113 - val_accuracy: 0.9138\n",
            "Epoch 109/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2249 - accuracy: 0.9073 - val_loss: 0.2110 - val_accuracy: 0.9131\n",
            "Epoch 110/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2239 - accuracy: 0.9077 - val_loss: 0.2111 - val_accuracy: 0.9138\n",
            "Epoch 111/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2156 - accuracy: 0.9088 - val_loss: 0.2114 - val_accuracy: 0.9137\n",
            "Epoch 112/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2148 - accuracy: 0.9102 - val_loss: 0.2113 - val_accuracy: 0.9139\n",
            "Epoch 113/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2234 - accuracy: 0.9085 - val_loss: 0.2111 - val_accuracy: 0.9139\n",
            "Epoch 114/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2168 - accuracy: 0.9091 - val_loss: 0.2110 - val_accuracy: 0.9135\n",
            "Epoch 115/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2149 - accuracy: 0.9110 - val_loss: 0.2114 - val_accuracy: 0.9138\n",
            "Epoch 116/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2134 - accuracy: 0.9125 - val_loss: 0.2110 - val_accuracy: 0.9135\n",
            "Epoch 117/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2168 - accuracy: 0.9117 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 118/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2207 - accuracy: 0.9073 - val_loss: 0.2110 - val_accuracy: 0.9133\n",
            "Epoch 119/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2168 - accuracy: 0.9083 - val_loss: 0.2112 - val_accuracy: 0.9137\n",
            "Epoch 120/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2126 - accuracy: 0.9111 - val_loss: 0.2111 - val_accuracy: 0.9138\n",
            "Epoch 121/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2196 - accuracy: 0.9092 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 122/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9098 - val_loss: 0.2110 - val_accuracy: 0.9138\n",
            "Epoch 123/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2203 - accuracy: 0.9099 - val_loss: 0.2114 - val_accuracy: 0.9136\n",
            "Epoch 124/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2172 - accuracy: 0.9101 - val_loss: 0.2111 - val_accuracy: 0.9136\n",
            "Epoch 125/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2191 - accuracy: 0.9100 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 126/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2236 - accuracy: 0.9070 - val_loss: 0.2109 - val_accuracy: 0.9135\n",
            "Epoch 127/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2128 - accuracy: 0.9130 - val_loss: 0.2111 - val_accuracy: 0.9138\n",
            "Epoch 128/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2173 - accuracy: 0.9105 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 129/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2163 - accuracy: 0.9098 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 130/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2186 - accuracy: 0.9079 - val_loss: 0.2109 - val_accuracy: 0.9132\n",
            "Epoch 131/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2207 - accuracy: 0.9100 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 132/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2187 - accuracy: 0.9101 - val_loss: 0.2110 - val_accuracy: 0.9138\n",
            "Epoch 133/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2180 - accuracy: 0.9088 - val_loss: 0.2112 - val_accuracy: 0.9139\n",
            "Epoch 134/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2195 - accuracy: 0.9093 - val_loss: 0.2109 - val_accuracy: 0.9137\n",
            "Epoch 135/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2204 - accuracy: 0.9072 - val_loss: 0.2109 - val_accuracy: 0.9132\n",
            "Epoch 136/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2179 - accuracy: 0.9090 - val_loss: 0.2115 - val_accuracy: 0.9136\n",
            "Epoch 137/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2178 - accuracy: 0.9088 - val_loss: 0.2112 - val_accuracy: 0.9138\n",
            "Epoch 138/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2171 - accuracy: 0.9113 - val_loss: 0.2110 - val_accuracy: 0.9137\n",
            "Epoch 139/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2213 - accuracy: 0.9078 - val_loss: 0.2111 - val_accuracy: 0.9139\n",
            "Epoch 140/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2171 - accuracy: 0.9103 - val_loss: 0.2110 - val_accuracy: 0.9138\n",
            "Epoch 141/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2180 - accuracy: 0.9088 - val_loss: 0.2109 - val_accuracy: 0.9137\n",
            "Epoch 142/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2188 - accuracy: 0.9081 - val_loss: 0.2110 - val_accuracy: 0.9137\n",
            "Epoch 143/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2165 - accuracy: 0.9103 - val_loss: 0.2109 - val_accuracy: 0.9131\n",
            "Epoch 144/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2137 - accuracy: 0.9108 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 145/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2113 - accuracy: 0.9133 - val_loss: 0.2111 - val_accuracy: 0.9139\n",
            "Epoch 146/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2215 - accuracy: 0.9075 - val_loss: 0.2109 - val_accuracy: 0.9140\n",
            "Epoch 147/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2201 - accuracy: 0.9091 - val_loss: 0.2118 - val_accuracy: 0.9131\n",
            "Epoch 148/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2194 - accuracy: 0.9105 - val_loss: 0.2108 - val_accuracy: 0.9137\n",
            "Epoch 149/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2161 - accuracy: 0.9097 - val_loss: 0.2114 - val_accuracy: 0.9139\n",
            "Epoch 150/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2180 - accuracy: 0.9108 - val_loss: 0.2109 - val_accuracy: 0.9137\n",
            "Epoch 151/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2165 - accuracy: 0.9095 - val_loss: 0.2110 - val_accuracy: 0.9138\n",
            "Epoch 152/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2210 - accuracy: 0.9080 - val_loss: 0.2110 - val_accuracy: 0.9137\n",
            "Epoch 153/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2123 - accuracy: 0.9130 - val_loss: 0.2110 - val_accuracy: 0.9138\n",
            "Epoch 154/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2218 - accuracy: 0.9082 - val_loss: 0.2109 - val_accuracy: 0.9139\n",
            "Epoch 155/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2175 - accuracy: 0.9095 - val_loss: 0.2113 - val_accuracy: 0.9137\n",
            "Epoch 156/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2172 - accuracy: 0.9087 - val_loss: 0.2111 - val_accuracy: 0.9137\n",
            "Epoch 157/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2128 - accuracy: 0.9136 - val_loss: 0.2110 - val_accuracy: 0.9137\n",
            "Epoch 158/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2141 - accuracy: 0.9110 - val_loss: 0.2108 - val_accuracy: 0.9137\n",
            "Epoch 159/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2165 - accuracy: 0.9120 - val_loss: 0.2108 - val_accuracy: 0.9136\n",
            "Epoch 160/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2166 - accuracy: 0.9095 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 161/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2169 - accuracy: 0.9107 - val_loss: 0.2108 - val_accuracy: 0.9134\n",
            "Epoch 162/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2186 - accuracy: 0.9073 - val_loss: 0.2108 - val_accuracy: 0.9136\n",
            "Epoch 163/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2204 - accuracy: 0.9078 - val_loss: 0.2108 - val_accuracy: 0.9136\n",
            "Epoch 164/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2182 - accuracy: 0.9094 - val_loss: 0.2108 - val_accuracy: 0.9136\n",
            "Epoch 165/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2218 - accuracy: 0.9080 - val_loss: 0.2108 - val_accuracy: 0.9136\n",
            "Epoch 166/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2181 - accuracy: 0.9087 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 167/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2215 - accuracy: 0.9091 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 168/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2188 - accuracy: 0.9099 - val_loss: 0.2113 - val_accuracy: 0.9139\n",
            "Epoch 169/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2171 - accuracy: 0.9109 - val_loss: 0.2108 - val_accuracy: 0.9138\n",
            "Epoch 170/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2186 - accuracy: 0.9076 - val_loss: 0.2108 - val_accuracy: 0.9136\n",
            "Epoch 171/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2201 - accuracy: 0.9089 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 172/256\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2121 - accuracy: 0.9119 - val_loss: 0.2108 - val_accuracy: 0.9135\n",
            "Epoch 173/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2144 - accuracy: 0.9114 - val_loss: 0.2112 - val_accuracy: 0.9138\n",
            "Epoch 174/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2106 - accuracy: 0.9128 - val_loss: 0.2108 - val_accuracy: 0.9138\n",
            "Epoch 175/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2108 - accuracy: 0.9124 - val_loss: 0.2108 - val_accuracy: 0.9137\n",
            "Epoch 176/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2165 - accuracy: 0.9100 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 177/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2204 - accuracy: 0.9079 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 178/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2157 - accuracy: 0.9120 - val_loss: 0.2107 - val_accuracy: 0.9134\n",
            "Epoch 179/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2226 - accuracy: 0.9091 - val_loss: 0.2112 - val_accuracy: 0.9137\n",
            "Epoch 180/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2209 - accuracy: 0.9087 - val_loss: 0.2108 - val_accuracy: 0.9140\n",
            "Epoch 181/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2213 - accuracy: 0.9078 - val_loss: 0.2110 - val_accuracy: 0.9139\n",
            "Epoch 182/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2163 - accuracy: 0.9114 - val_loss: 0.2108 - val_accuracy: 0.9139\n",
            "Epoch 183/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2212 - accuracy: 0.9082 - val_loss: 0.2108 - val_accuracy: 0.9138\n",
            "Epoch 184/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2207 - accuracy: 0.9081 - val_loss: 0.2108 - val_accuracy: 0.9139\n",
            "Epoch 185/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2162 - accuracy: 0.9096 - val_loss: 0.2107 - val_accuracy: 0.9132\n",
            "Epoch 186/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2177 - accuracy: 0.9101 - val_loss: 0.2108 - val_accuracy: 0.9131\n",
            "Epoch 187/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2148 - accuracy: 0.9112 - val_loss: 0.2114 - val_accuracy: 0.9135\n",
            "Epoch 188/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2162 - accuracy: 0.9123 - val_loss: 0.2109 - val_accuracy: 0.9139\n",
            "Epoch 189/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2136 - accuracy: 0.9104 - val_loss: 0.2108 - val_accuracy: 0.9137\n",
            "Epoch 190/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2180 - accuracy: 0.9084 - val_loss: 0.2111 - val_accuracy: 0.9136\n",
            "Epoch 191/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2222 - accuracy: 0.9085 - val_loss: 0.2108 - val_accuracy: 0.9142\n",
            "Epoch 192/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2139 - accuracy: 0.9115 - val_loss: 0.2112 - val_accuracy: 0.9135\n",
            "Epoch 193/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2193 - accuracy: 0.9100 - val_loss: 0.2107 - val_accuracy: 0.9139\n",
            "Epoch 194/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9105 - val_loss: 0.2107 - val_accuracy: 0.9139\n",
            "Epoch 195/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2205 - accuracy: 0.9076 - val_loss: 0.2110 - val_accuracy: 0.9137\n",
            "Epoch 196/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2216 - accuracy: 0.9072 - val_loss: 0.2109 - val_accuracy: 0.9140\n",
            "Epoch 197/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2176 - accuracy: 0.9095 - val_loss: 0.2116 - val_accuracy: 0.9132\n",
            "Epoch 198/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2179 - accuracy: 0.9108 - val_loss: 0.2108 - val_accuracy: 0.9141\n",
            "Epoch 199/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2122 - accuracy: 0.9134 - val_loss: 0.2111 - val_accuracy: 0.9137\n",
            "Epoch 200/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2205 - accuracy: 0.9091 - val_loss: 0.2107 - val_accuracy: 0.9136\n",
            "Epoch 201/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2135 - accuracy: 0.9121 - val_loss: 0.2109 - val_accuracy: 0.9139\n",
            "Epoch 202/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2201 - accuracy: 0.9102 - val_loss: 0.2109 - val_accuracy: 0.9140\n",
            "Epoch 203/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2153 - accuracy: 0.9145 - val_loss: 0.2107 - val_accuracy: 0.9139\n",
            "Epoch 204/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2177 - accuracy: 0.9087 - val_loss: 0.2108 - val_accuracy: 0.9143\n",
            "Epoch 205/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9102 - val_loss: 0.2107 - val_accuracy: 0.9136\n",
            "Epoch 206/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2193 - accuracy: 0.9091 - val_loss: 0.2108 - val_accuracy: 0.9143\n",
            "Epoch 207/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2163 - accuracy: 0.9102 - val_loss: 0.2107 - val_accuracy: 0.9135\n",
            "Epoch 208/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2164 - accuracy: 0.9098 - val_loss: 0.2108 - val_accuracy: 0.9143\n",
            "Epoch 209/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2190 - accuracy: 0.9080 - val_loss: 0.2114 - val_accuracy: 0.9135\n",
            "Epoch 210/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2252 - accuracy: 0.9080 - val_loss: 0.2107 - val_accuracy: 0.9139\n",
            "Epoch 211/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2172 - accuracy: 0.9125 - val_loss: 0.2116 - val_accuracy: 0.9135\n",
            "Epoch 212/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2146 - accuracy: 0.9112 - val_loss: 0.2107 - val_accuracy: 0.9140\n",
            "Epoch 213/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2250 - accuracy: 0.9067 - val_loss: 0.2107 - val_accuracy: 0.9143\n",
            "Epoch 214/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2171 - accuracy: 0.9109 - val_loss: 0.2110 - val_accuracy: 0.9136\n",
            "Epoch 215/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2124 - accuracy: 0.9113 - val_loss: 0.2107 - val_accuracy: 0.9144\n",
            "Epoch 216/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2188 - accuracy: 0.9099 - val_loss: 0.2108 - val_accuracy: 0.9142\n",
            "Epoch 217/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2153 - accuracy: 0.9111 - val_loss: 0.2106 - val_accuracy: 0.9137\n",
            "Epoch 218/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2178 - accuracy: 0.9084 - val_loss: 0.2112 - val_accuracy: 0.9135\n",
            "Epoch 219/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2171 - accuracy: 0.9103 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 220/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2170 - accuracy: 0.9090 - val_loss: 0.2107 - val_accuracy: 0.9141\n",
            "Epoch 221/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2174 - accuracy: 0.9100 - val_loss: 0.2107 - val_accuracy: 0.9143\n",
            "Epoch 222/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2178 - accuracy: 0.9095 - val_loss: 0.2106 - val_accuracy: 0.9140\n",
            "Epoch 223/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2110 - accuracy: 0.9132 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 224/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2141 - accuracy: 0.9129 - val_loss: 0.2106 - val_accuracy: 0.9135\n",
            "Epoch 225/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2163 - accuracy: 0.9107 - val_loss: 0.2109 - val_accuracy: 0.9137\n",
            "Epoch 226/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2167 - accuracy: 0.9093 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 227/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2184 - accuracy: 0.9099 - val_loss: 0.2111 - val_accuracy: 0.9135\n",
            "Epoch 228/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2193 - accuracy: 0.9071 - val_loss: 0.2106 - val_accuracy: 0.9134\n",
            "Epoch 229/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2163 - accuracy: 0.9097 - val_loss: 0.2108 - val_accuracy: 0.9141\n",
            "Epoch 230/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2177 - accuracy: 0.9090 - val_loss: 0.2110 - val_accuracy: 0.9137\n",
            "Epoch 231/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2144 - accuracy: 0.9104 - val_loss: 0.2114 - val_accuracy: 0.9134\n",
            "Epoch 232/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2117 - accuracy: 0.9127 - val_loss: 0.2107 - val_accuracy: 0.9142\n",
            "Epoch 233/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2150 - accuracy: 0.9116 - val_loss: 0.2106 - val_accuracy: 0.9139\n",
            "Epoch 234/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2161 - accuracy: 0.9111 - val_loss: 0.2106 - val_accuracy: 0.9137\n",
            "Epoch 235/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2181 - accuracy: 0.9095 - val_loss: 0.2109 - val_accuracy: 0.9139\n",
            "Epoch 236/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2210 - accuracy: 0.9102 - val_loss: 0.2106 - val_accuracy: 0.9139\n",
            "Epoch 237/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2148 - accuracy: 0.9120 - val_loss: 0.2108 - val_accuracy: 0.9140\n",
            "Epoch 238/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2142 - accuracy: 0.9118 - val_loss: 0.2107 - val_accuracy: 0.9142\n",
            "Epoch 239/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2179 - accuracy: 0.9103 - val_loss: 0.2108 - val_accuracy: 0.9141\n",
            "Epoch 240/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2219 - accuracy: 0.9070 - val_loss: 0.2106 - val_accuracy: 0.9133\n",
            "Epoch 241/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2192 - accuracy: 0.9077 - val_loss: 0.2106 - val_accuracy: 0.9142\n",
            "Epoch 242/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2216 - accuracy: 0.9083 - val_loss: 0.2106 - val_accuracy: 0.9142\n",
            "Epoch 243/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2168 - accuracy: 0.9098 - val_loss: 0.2106 - val_accuracy: 0.9131\n",
            "Epoch 244/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2154 - accuracy: 0.9077 - val_loss: 0.2110 - val_accuracy: 0.9137\n",
            "Epoch 245/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2199 - accuracy: 0.9102 - val_loss: 0.2107 - val_accuracy: 0.9142\n",
            "Epoch 246/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2157 - accuracy: 0.9109 - val_loss: 0.2109 - val_accuracy: 0.9139\n",
            "Epoch 247/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2165 - accuracy: 0.9090 - val_loss: 0.2109 - val_accuracy: 0.9138\n",
            "Epoch 248/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2119 - accuracy: 0.9124 - val_loss: 0.2107 - val_accuracy: 0.9143\n",
            "Epoch 249/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2167 - accuracy: 0.9099 - val_loss: 0.2106 - val_accuracy: 0.9143\n",
            "Epoch 250/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2147 - accuracy: 0.9112 - val_loss: 0.2106 - val_accuracy: 0.9140\n",
            "Epoch 251/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2165 - accuracy: 0.9096 - val_loss: 0.2106 - val_accuracy: 0.9139\n",
            "Epoch 252/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2170 - accuracy: 0.9115 - val_loss: 0.2106 - val_accuracy: 0.9135\n",
            "Epoch 253/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2214 - accuracy: 0.9079 - val_loss: 0.2106 - val_accuracy: 0.9138\n",
            "Epoch 254/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2192 - accuracy: 0.9077 - val_loss: 0.2106 - val_accuracy: 0.9139\n",
            "Epoch 255/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2192 - accuracy: 0.9089 - val_loss: 0.2105 - val_accuracy: 0.9138\n",
            "Epoch 256/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2145 - accuracy: 0.9108 - val_loss: 0.2106 - val_accuracy: 0.9138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "EFvB7Jms4YKP",
        "outputId": "0e895ecb-d702-427a-9e61-c65d566c979b"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSm0U7nOtaZ0",
        "outputId": "8ed2e5be-8de2-4618-8592-b54bc75684b8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "P = model.predict(XVALID)\n",
        "accuracy = model.evaluate(XVALID, YVALID)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "386/386 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4pok0dA47Omk",
        "outputId": "d4cd8ec4-9e73-4f6f-83ec-b99ce66a0e37"
      },
      "source": [
        "#4-1\n",
        "model_2_r = Sequential()\n",
        "model_2_r.add(Dense(4, input_dim = 20,activation='relu'))\n",
        "#model2.add(Dense(4,activation='relu'))\n",
        "model_2_r.add(Dense(1,activation='sigmoid'))\n",
        "model_2_r.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "history = model_2_r.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4393 - accuracy: 0.8835 - val_loss: 0.2941 - val_accuracy: 0.8931\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3027 - accuracy: 0.8853 - val_loss: 0.2786 - val_accuracy: 0.8931\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2892 - accuracy: 0.8847 - val_loss: 0.2695 - val_accuracy: 0.8991\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2809 - accuracy: 0.8925 - val_loss: 0.2618 - val_accuracy: 0.9039\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2696 - accuracy: 0.8957 - val_loss: 0.2509 - val_accuracy: 0.9048\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2603 - accuracy: 0.8986 - val_loss: 0.2394 - val_accuracy: 0.9069\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2538 - accuracy: 0.8989 - val_loss: 0.2285 - val_accuracy: 0.9080\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2352 - accuracy: 0.9055 - val_loss: 0.2202 - val_accuracy: 0.9101\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2254 - accuracy: 0.9067 - val_loss: 0.2136 - val_accuracy: 0.9113\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2202 - accuracy: 0.9086 - val_loss: 0.2082 - val_accuracy: 0.9122\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2185 - accuracy: 0.9076 - val_loss: 0.2046 - val_accuracy: 0.9128\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2116 - accuracy: 0.9097 - val_loss: 0.2028 - val_accuracy: 0.9132\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2146 - accuracy: 0.9079 - val_loss: 0.2005 - val_accuracy: 0.9126\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2041 - accuracy: 0.9111 - val_loss: 0.1994 - val_accuracy: 0.9133\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2128 - accuracy: 0.9084 - val_loss: 0.1987 - val_accuracy: 0.9127\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2022 - accuracy: 0.9141 - val_loss: 0.1988 - val_accuracy: 0.9139\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2050 - accuracy: 0.9102 - val_loss: 0.1973 - val_accuracy: 0.9138\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9134 - val_loss: 0.1970 - val_accuracy: 0.9141\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2034 - accuracy: 0.9101 - val_loss: 0.1970 - val_accuracy: 0.9144\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2043 - accuracy: 0.9102 - val_loss: 0.1962 - val_accuracy: 0.9128\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2067 - accuracy: 0.9103 - val_loss: 0.1961 - val_accuracy: 0.9126\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1966 - accuracy: 0.9145 - val_loss: 0.1959 - val_accuracy: 0.9130\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2003 - accuracy: 0.9123 - val_loss: 0.1954 - val_accuracy: 0.9147\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2073 - accuracy: 0.9095 - val_loss: 0.1955 - val_accuracy: 0.9145\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9116 - val_loss: 0.1956 - val_accuracy: 0.9141\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2038 - accuracy: 0.9085 - val_loss: 0.1951 - val_accuracy: 0.9150\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9106 - val_loss: 0.1951 - val_accuracy: 0.9140\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1941 - accuracy: 0.9156 - val_loss: 0.1951 - val_accuracy: 0.9141\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2003 - accuracy: 0.9121 - val_loss: 0.1946 - val_accuracy: 0.9139\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2017 - accuracy: 0.9112 - val_loss: 0.1950 - val_accuracy: 0.9140\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2030 - accuracy: 0.9106 - val_loss: 0.1968 - val_accuracy: 0.9133\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1985 - accuracy: 0.9138 - val_loss: 0.1952 - val_accuracy: 0.9146\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2034 - accuracy: 0.9097 - val_loss: 0.1948 - val_accuracy: 0.9143\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2056 - accuracy: 0.9085 - val_loss: 0.1947 - val_accuracy: 0.9145\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1977 - accuracy: 0.9120 - val_loss: 0.1943 - val_accuracy: 0.9143\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2040 - accuracy: 0.9089 - val_loss: 0.1942 - val_accuracy: 0.9152\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2044 - accuracy: 0.9090 - val_loss: 0.1949 - val_accuracy: 0.9152\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2021 - accuracy: 0.9111 - val_loss: 0.1937 - val_accuracy: 0.9136\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2007 - accuracy: 0.9134 - val_loss: 0.1946 - val_accuracy: 0.9139\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2049 - accuracy: 0.9092 - val_loss: 0.1935 - val_accuracy: 0.9144\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2056 - accuracy: 0.9078 - val_loss: 0.1935 - val_accuracy: 0.9144\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1998 - accuracy: 0.9119 - val_loss: 0.1931 - val_accuracy: 0.9140\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2020 - accuracy: 0.9121 - val_loss: 0.1938 - val_accuracy: 0.9135\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2001 - accuracy: 0.9106 - val_loss: 0.1930 - val_accuracy: 0.9144\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2018 - accuracy: 0.9107 - val_loss: 0.1935 - val_accuracy: 0.9149\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1996 - accuracy: 0.9120 - val_loss: 0.1925 - val_accuracy: 0.9144\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2032 - accuracy: 0.9100 - val_loss: 0.1930 - val_accuracy: 0.9139\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2022 - accuracy: 0.9112 - val_loss: 0.1930 - val_accuracy: 0.9146\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2010 - accuracy: 0.9121 - val_loss: 0.1931 - val_accuracy: 0.9145\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1991 - accuracy: 0.9113 - val_loss: 0.1937 - val_accuracy: 0.9144\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2012 - accuracy: 0.9105 - val_loss: 0.1923 - val_accuracy: 0.9149\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2026 - accuracy: 0.9108 - val_loss: 0.1929 - val_accuracy: 0.9147\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1980 - accuracy: 0.9138 - val_loss: 0.1920 - val_accuracy: 0.9149\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2003 - accuracy: 0.9095 - val_loss: 0.1924 - val_accuracy: 0.9141\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2029 - accuracy: 0.9096 - val_loss: 0.1921 - val_accuracy: 0.9152\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1970 - accuracy: 0.9112 - val_loss: 0.1925 - val_accuracy: 0.9147\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1945 - accuracy: 0.9162 - val_loss: 0.1919 - val_accuracy: 0.9143\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2004 - accuracy: 0.9124 - val_loss: 0.1920 - val_accuracy: 0.9142\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2024 - accuracy: 0.9097 - val_loss: 0.1922 - val_accuracy: 0.9137\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1968 - accuracy: 0.9136 - val_loss: 0.1917 - val_accuracy: 0.9145\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1995 - accuracy: 0.9116 - val_loss: 0.1923 - val_accuracy: 0.9135\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2008 - accuracy: 0.9104 - val_loss: 0.1943 - val_accuracy: 0.9134\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9141 - val_loss: 0.1931 - val_accuracy: 0.9144\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1988 - accuracy: 0.9118 - val_loss: 0.1932 - val_accuracy: 0.9142\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1981 - accuracy: 0.9154 - val_loss: 0.1918 - val_accuracy: 0.9151\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9154 - val_loss: 0.1917 - val_accuracy: 0.9145\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9128 - val_loss: 0.1921 - val_accuracy: 0.9145\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2018 - accuracy: 0.9125 - val_loss: 0.1926 - val_accuracy: 0.9144\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2028 - accuracy: 0.9084 - val_loss: 0.1914 - val_accuracy: 0.9152\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1962 - accuracy: 0.9145 - val_loss: 0.1915 - val_accuracy: 0.9146\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1959 - accuracy: 0.9138 - val_loss: 0.1918 - val_accuracy: 0.9149\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2029 - accuracy: 0.9100 - val_loss: 0.1918 - val_accuracy: 0.9146\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9122 - val_loss: 0.1923 - val_accuracy: 0.9141\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1944 - accuracy: 0.9128 - val_loss: 0.1914 - val_accuracy: 0.9139\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9100 - val_loss: 0.1911 - val_accuracy: 0.9139\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1987 - accuracy: 0.9119 - val_loss: 0.1921 - val_accuracy: 0.9145\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1984 - accuracy: 0.9116 - val_loss: 0.1913 - val_accuracy: 0.9143\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2017 - accuracy: 0.9108 - val_loss: 0.1911 - val_accuracy: 0.9144\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2012 - accuracy: 0.9101 - val_loss: 0.1912 - val_accuracy: 0.9152\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1959 - accuracy: 0.9125 - val_loss: 0.1925 - val_accuracy: 0.9137\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1966 - accuracy: 0.9130 - val_loss: 0.1911 - val_accuracy: 0.9144\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1972 - accuracy: 0.9117 - val_loss: 0.1920 - val_accuracy: 0.9138\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1974 - accuracy: 0.9125 - val_loss: 0.1913 - val_accuracy: 0.9137\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9152 - val_loss: 0.1918 - val_accuracy: 0.9143\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1997 - accuracy: 0.9136 - val_loss: 0.1916 - val_accuracy: 0.9148\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1965 - accuracy: 0.9136 - val_loss: 0.1924 - val_accuracy: 0.9138\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2000 - accuracy: 0.9128 - val_loss: 0.1917 - val_accuracy: 0.9147\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1968 - accuracy: 0.9143 - val_loss: 0.1924 - val_accuracy: 0.9140\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1993 - accuracy: 0.9130 - val_loss: 0.1928 - val_accuracy: 0.9142\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9170 - val_loss: 0.1911 - val_accuracy: 0.9145\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1921 - accuracy: 0.9152 - val_loss: 0.1916 - val_accuracy: 0.9149\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2045 - accuracy: 0.9096 - val_loss: 0.1913 - val_accuracy: 0.9148\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9118 - val_loss: 0.1910 - val_accuracy: 0.9145\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1958 - accuracy: 0.9126 - val_loss: 0.1914 - val_accuracy: 0.9148\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1974 - accuracy: 0.9144 - val_loss: 0.1910 - val_accuracy: 0.9137\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1937 - accuracy: 0.9133 - val_loss: 0.1910 - val_accuracy: 0.9140\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1949 - accuracy: 0.9141 - val_loss: 0.1910 - val_accuracy: 0.9138\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1952 - accuracy: 0.9128 - val_loss: 0.1915 - val_accuracy: 0.9140\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1941 - accuracy: 0.9150 - val_loss: 0.1927 - val_accuracy: 0.9139\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1971 - accuracy: 0.9124 - val_loss: 0.1935 - val_accuracy: 0.9140\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2010 - accuracy: 0.9099 - val_loss: 0.1916 - val_accuracy: 0.9137\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1966 - accuracy: 0.9113 - val_loss: 0.1909 - val_accuracy: 0.9135\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2059 - accuracy: 0.9092 - val_loss: 0.1912 - val_accuracy: 0.9141\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9103 - val_loss: 0.1913 - val_accuracy: 0.9142\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1981 - accuracy: 0.9115 - val_loss: 0.1911 - val_accuracy: 0.9143\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1980 - accuracy: 0.9108 - val_loss: 0.1908 - val_accuracy: 0.9142\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9134 - val_loss: 0.1914 - val_accuracy: 0.9140\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1986 - accuracy: 0.9146 - val_loss: 0.1909 - val_accuracy: 0.9151\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1964 - accuracy: 0.9130 - val_loss: 0.1908 - val_accuracy: 0.9150\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1972 - accuracy: 0.9110 - val_loss: 0.1908 - val_accuracy: 0.9140\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9120 - val_loss: 0.1918 - val_accuracy: 0.9140\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1994 - accuracy: 0.9103 - val_loss: 0.1909 - val_accuracy: 0.9149\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2033 - accuracy: 0.9100 - val_loss: 0.1908 - val_accuracy: 0.9140\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1939 - accuracy: 0.9145 - val_loss: 0.1911 - val_accuracy: 0.9142\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1990 - accuracy: 0.9122 - val_loss: 0.1910 - val_accuracy: 0.9143\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2032 - accuracy: 0.9093 - val_loss: 0.1912 - val_accuracy: 0.9151\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1959 - accuracy: 0.9145 - val_loss: 0.1910 - val_accuracy: 0.9143\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1998 - accuracy: 0.9109 - val_loss: 0.1919 - val_accuracy: 0.9141\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9135 - val_loss: 0.1918 - val_accuracy: 0.9141\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1958 - accuracy: 0.9124 - val_loss: 0.1912 - val_accuracy: 0.9151\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2000 - accuracy: 0.9103 - val_loss: 0.1923 - val_accuracy: 0.9147\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9111 - val_loss: 0.1919 - val_accuracy: 0.9138\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2024 - accuracy: 0.9099 - val_loss: 0.1915 - val_accuracy: 0.9144\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2000 - accuracy: 0.9122 - val_loss: 0.1912 - val_accuracy: 0.9148\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1950 - accuracy: 0.9114 - val_loss: 0.1911 - val_accuracy: 0.9144\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1962 - accuracy: 0.9141 - val_loss: 0.1909 - val_accuracy: 0.9139\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1984 - accuracy: 0.9127 - val_loss: 0.1914 - val_accuracy: 0.9144\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1969 - accuracy: 0.9118 - val_loss: 0.1912 - val_accuracy: 0.9152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sXQI1bZczPSj",
        "outputId": "cfd295a7-e464-4fdc-ba93-b3be9c238b25"
      },
      "source": [
        "#4-1\n",
        "model_2 = Sequential()\n",
        "model_2.add(Dense(4, input_dim = 20,activation='relu'))\n",
        "#model2.add(Dense(4,activation='relu'))\n",
        "model_2.add(Dense(1,activation='sigmoid'))\n",
        "model_2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_2.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3734 - accuracy: 0.8840 - val_loss: 0.2921 - val_accuracy: 0.8930\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2996 - accuracy: 0.8857 - val_loss: 0.2710 - val_accuracy: 0.8930\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2802 - accuracy: 0.8835 - val_loss: 0.2524 - val_accuracy: 0.8982\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2575 - accuracy: 0.8949 - val_loss: 0.2352 - val_accuracy: 0.9027\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2347 - accuracy: 0.9026 - val_loss: 0.2218 - val_accuracy: 0.9067\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2265 - accuracy: 0.9065 - val_loss: 0.2125 - val_accuracy: 0.9091\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2150 - accuracy: 0.9088 - val_loss: 0.2068 - val_accuracy: 0.9118\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2092 - accuracy: 0.9094 - val_loss: 0.2028 - val_accuracy: 0.9133\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2022 - accuracy: 0.9121 - val_loss: 0.2001 - val_accuracy: 0.9141\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2084 - accuracy: 0.9091 - val_loss: 0.1993 - val_accuracy: 0.9141\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2063 - accuracy: 0.9103 - val_loss: 0.1989 - val_accuracy: 0.9135\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2011 - accuracy: 0.9099 - val_loss: 0.1988 - val_accuracy: 0.9140\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2035 - accuracy: 0.9099 - val_loss: 0.1973 - val_accuracy: 0.9129\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2013 - accuracy: 0.9111 - val_loss: 0.1966 - val_accuracy: 0.9139\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1977 - accuracy: 0.9123 - val_loss: 0.1958 - val_accuracy: 0.9137\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2055 - accuracy: 0.9074 - val_loss: 0.1955 - val_accuracy: 0.9142\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1978 - accuracy: 0.9103 - val_loss: 0.1958 - val_accuracy: 0.9145\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1993 - accuracy: 0.9117 - val_loss: 0.1950 - val_accuracy: 0.9147\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1975 - accuracy: 0.9133 - val_loss: 0.1952 - val_accuracy: 0.9148\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9129 - val_loss: 0.1947 - val_accuracy: 0.9151\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1956 - accuracy: 0.9145 - val_loss: 0.1938 - val_accuracy: 0.9149\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.2004 - accuracy: 0.9118 - val_loss: 0.1936 - val_accuracy: 0.9137\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1985 - accuracy: 0.9123 - val_loss: 0.1945 - val_accuracy: 0.9145\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9123 - val_loss: 0.1933 - val_accuracy: 0.9147\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1985 - accuracy: 0.9107 - val_loss: 0.1933 - val_accuracy: 0.9147\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1943 - accuracy: 0.9118 - val_loss: 0.1926 - val_accuracy: 0.9147\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1912 - accuracy: 0.9143 - val_loss: 0.1924 - val_accuracy: 0.9147\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1976 - accuracy: 0.9125 - val_loss: 0.1926 - val_accuracy: 0.9139\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1966 - accuracy: 0.9109 - val_loss: 0.1927 - val_accuracy: 0.9138\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1907 - accuracy: 0.9127 - val_loss: 0.1922 - val_accuracy: 0.9144\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1962 - accuracy: 0.9134 - val_loss: 0.1931 - val_accuracy: 0.9143\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1947 - accuracy: 0.9119 - val_loss: 0.1915 - val_accuracy: 0.9137\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1908 - accuracy: 0.9155 - val_loss: 0.1906 - val_accuracy: 0.9149\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1927 - accuracy: 0.9132 - val_loss: 0.1903 - val_accuracy: 0.9151\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9159 - val_loss: 0.1900 - val_accuracy: 0.9143\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1943 - accuracy: 0.9139 - val_loss: 0.1901 - val_accuracy: 0.9144\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1933 - accuracy: 0.9131 - val_loss: 0.1893 - val_accuracy: 0.9136\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9134 - val_loss: 0.1893 - val_accuracy: 0.9145\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1868 - accuracy: 0.9179 - val_loss: 0.1902 - val_accuracy: 0.9146\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1934 - accuracy: 0.9126 - val_loss: 0.1884 - val_accuracy: 0.9143\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1884 - accuracy: 0.9149 - val_loss: 0.1901 - val_accuracy: 0.9155\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1905 - accuracy: 0.9139 - val_loss: 0.1892 - val_accuracy: 0.9143\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1926 - accuracy: 0.9126 - val_loss: 0.1881 - val_accuracy: 0.9129\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9145 - val_loss: 0.1876 - val_accuracy: 0.9147\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1942 - accuracy: 0.9132 - val_loss: 0.1875 - val_accuracy: 0.9152\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1950 - accuracy: 0.9120 - val_loss: 0.1876 - val_accuracy: 0.9152\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1907 - accuracy: 0.9135 - val_loss: 0.1875 - val_accuracy: 0.9152\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1904 - accuracy: 0.9113 - val_loss: 0.1874 - val_accuracy: 0.9150\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1905 - accuracy: 0.9117 - val_loss: 0.1874 - val_accuracy: 0.9149\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1900 - accuracy: 0.9127 - val_loss: 0.1874 - val_accuracy: 0.9153\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1924 - accuracy: 0.9113 - val_loss: 0.1868 - val_accuracy: 0.9156\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9149 - val_loss: 0.1880 - val_accuracy: 0.9135\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9117 - val_loss: 0.1871 - val_accuracy: 0.9152\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1877 - accuracy: 0.9155 - val_loss: 0.1865 - val_accuracy: 0.9154\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9143 - val_loss: 0.1866 - val_accuracy: 0.9153\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9147 - val_loss: 0.1864 - val_accuracy: 0.9160\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9127 - val_loss: 0.1886 - val_accuracy: 0.9159\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1888 - accuracy: 0.9142 - val_loss: 0.1874 - val_accuracy: 0.9157\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9120 - val_loss: 0.1865 - val_accuracy: 0.9144\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1913 - accuracy: 0.9137 - val_loss: 0.1864 - val_accuracy: 0.9157\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9126 - val_loss: 0.1864 - val_accuracy: 0.9154\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9149 - val_loss: 0.1863 - val_accuracy: 0.9153\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1901 - accuracy: 0.9146 - val_loss: 0.1862 - val_accuracy: 0.9156\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1922 - accuracy: 0.9104 - val_loss: 0.1867 - val_accuracy: 0.9158\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1880 - accuracy: 0.9142 - val_loss: 0.1864 - val_accuracy: 0.9144\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.9132 - val_loss: 0.1865 - val_accuracy: 0.9150\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9157 - val_loss: 0.1862 - val_accuracy: 0.9151\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9101 - val_loss: 0.1860 - val_accuracy: 0.9159\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1921 - accuracy: 0.9109 - val_loss: 0.1863 - val_accuracy: 0.9154\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9132 - val_loss: 0.1875 - val_accuracy: 0.9149\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1909 - accuracy: 0.9118 - val_loss: 0.1862 - val_accuracy: 0.9165\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9129 - val_loss: 0.1863 - val_accuracy: 0.9151\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9155 - val_loss: 0.1856 - val_accuracy: 0.9165\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1890 - accuracy: 0.9105 - val_loss: 0.1856 - val_accuracy: 0.9155\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9121 - val_loss: 0.1857 - val_accuracy: 0.9156\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9153 - val_loss: 0.1862 - val_accuracy: 0.9153\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1884 - accuracy: 0.9139 - val_loss: 0.1867 - val_accuracy: 0.9145\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1903 - accuracy: 0.9122 - val_loss: 0.1856 - val_accuracy: 0.9152\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9114 - val_loss: 0.1858 - val_accuracy: 0.9148\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9121 - val_loss: 0.1857 - val_accuracy: 0.9160\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9133 - val_loss: 0.1854 - val_accuracy: 0.9156\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9127 - val_loss: 0.1870 - val_accuracy: 0.9155\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9177 - val_loss: 0.1859 - val_accuracy: 0.9160\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9136 - val_loss: 0.1860 - val_accuracy: 0.9148\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9119 - val_loss: 0.1852 - val_accuracy: 0.9148\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9132 - val_loss: 0.1853 - val_accuracy: 0.9154\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9150 - val_loss: 0.1857 - val_accuracy: 0.9159\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9138 - val_loss: 0.1857 - val_accuracy: 0.9153\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9154 - val_loss: 0.1854 - val_accuracy: 0.9152\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9146 - val_loss: 0.1859 - val_accuracy: 0.9156\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1887 - accuracy: 0.9135 - val_loss: 0.1851 - val_accuracy: 0.9163\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1896 - accuracy: 0.9136 - val_loss: 0.1854 - val_accuracy: 0.9151\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9094 - val_loss: 0.1861 - val_accuracy: 0.9151\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9124 - val_loss: 0.1862 - val_accuracy: 0.9154\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9136 - val_loss: 0.1850 - val_accuracy: 0.9152\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9149 - val_loss: 0.1855 - val_accuracy: 0.9171\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9101 - val_loss: 0.1851 - val_accuracy: 0.9145\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 1s 1ms/step - loss: 0.1873 - accuracy: 0.9135 - val_loss: 0.1852 - val_accuracy: 0.9141\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1846 - accuracy: 0.9148 - val_loss: 0.1850 - val_accuracy: 0.9149\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1895 - accuracy: 0.9126 - val_loss: 0.1859 - val_accuracy: 0.9154\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1891 - accuracy: 0.9101 - val_loss: 0.1855 - val_accuracy: 0.9156\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9148 - val_loss: 0.1876 - val_accuracy: 0.9164\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9158 - val_loss: 0.1849 - val_accuracy: 0.9159\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9113 - val_loss: 0.1849 - val_accuracy: 0.9141\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9117 - val_loss: 0.1874 - val_accuracy: 0.9155\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9123 - val_loss: 0.1847 - val_accuracy: 0.9143\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9149 - val_loss: 0.1852 - val_accuracy: 0.9162\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1828 - accuracy: 0.9169 - val_loss: 0.1856 - val_accuracy: 0.9161\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1907 - accuracy: 0.9121 - val_loss: 0.1856 - val_accuracy: 0.9152\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9115 - val_loss: 0.1849 - val_accuracy: 0.9150\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9134 - val_loss: 0.1847 - val_accuracy: 0.9149\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9097 - val_loss: 0.1851 - val_accuracy: 0.9166\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9135 - val_loss: 0.1845 - val_accuracy: 0.9160\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9114 - val_loss: 0.1858 - val_accuracy: 0.9144\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9149 - val_loss: 0.1847 - val_accuracy: 0.9152\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1928 - accuracy: 0.9103 - val_loss: 0.1851 - val_accuracy: 0.9156\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9128 - val_loss: 0.1854 - val_accuracy: 0.9160\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9096 - val_loss: 0.1843 - val_accuracy: 0.9157\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9129 - val_loss: 0.1855 - val_accuracy: 0.9135\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.9143 - val_loss: 0.1841 - val_accuracy: 0.9141\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1884 - accuracy: 0.9121 - val_loss: 0.1853 - val_accuracy: 0.9150\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9117 - val_loss: 0.1846 - val_accuracy: 0.9151\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9121 - val_loss: 0.1854 - val_accuracy: 0.9173\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9098 - val_loss: 0.1842 - val_accuracy: 0.9156\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1871 - accuracy: 0.9128 - val_loss: 0.1843 - val_accuracy: 0.9143\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9133 - val_loss: 0.1849 - val_accuracy: 0.9140\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1892 - accuracy: 0.9099 - val_loss: 0.1843 - val_accuracy: 0.9147\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1838 - accuracy: 0.9135 - val_loss: 0.1842 - val_accuracy: 0.9160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4YALTE8-wiED",
        "outputId": "2df2cba8-5c7e-41d4-f084-749bcf1ce8a9"
      },
      "source": [
        "#0.1\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model2.add(Dense(4,activation='relu'))\n",
        "model2.add(Dense(1,activation='sigmoid'))\n",
        "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model2.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.6319 - accuracy: 0.7755 - val_loss: 0.4240 - val_accuracy: 0.8941\n",
            "Epoch 2/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.3943 - accuracy: 0.8949 - val_loss: 0.3233 - val_accuracy: 0.9019\n",
            "Epoch 3/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.3118 - accuracy: 0.9006 - val_loss: 0.2684 - val_accuracy: 0.9065\n",
            "Epoch 4/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.2575 - accuracy: 0.9062 - val_loss: 0.2173 - val_accuracy: 0.9128\n",
            "Epoch 5/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.2160 - accuracy: 0.9110 - val_loss: 0.2168 - val_accuracy: 0.9126\n",
            "Epoch 6/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.2109 - accuracy: 0.9113 - val_loss: 0.2080 - val_accuracy: 0.9118\n",
            "Epoch 7/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.2040 - accuracy: 0.9123 - val_loss: 0.2052 - val_accuracy: 0.9092\n",
            "Epoch 8/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.2075 - accuracy: 0.9097 - val_loss: 0.2039 - val_accuracy: 0.9104\n",
            "Epoch 9/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.2013 - accuracy: 0.9127 - val_loss: 0.2027 - val_accuracy: 0.9094\n",
            "Epoch 10/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1996 - accuracy: 0.9125 - val_loss: 0.2015 - val_accuracy: 0.9087\n",
            "Epoch 11/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.2009 - accuracy: 0.9118 - val_loss: 0.1993 - val_accuracy: 0.9094\n",
            "Epoch 12/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.2029 - accuracy: 0.9101 - val_loss: 0.1991 - val_accuracy: 0.9099\n",
            "Epoch 13/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1959 - accuracy: 0.9128 - val_loss: 0.1996 - val_accuracy: 0.9096\n",
            "Epoch 14/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1964 - accuracy: 0.9111 - val_loss: 0.1989 - val_accuracy: 0.9077\n",
            "Epoch 15/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1937 - accuracy: 0.9132 - val_loss: 0.1987 - val_accuracy: 0.9096\n",
            "Epoch 16/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1947 - accuracy: 0.9125 - val_loss: 0.1960 - val_accuracy: 0.9087\n",
            "Epoch 17/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1958 - accuracy: 0.9103 - val_loss: 0.1956 - val_accuracy: 0.9106\n",
            "Epoch 18/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1920 - accuracy: 0.9139 - val_loss: 0.1961 - val_accuracy: 0.9079\n",
            "Epoch 19/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1905 - accuracy: 0.9126 - val_loss: 0.1951 - val_accuracy: 0.9089\n",
            "Epoch 20/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1888 - accuracy: 0.9143 - val_loss: 0.1952 - val_accuracy: 0.9062\n",
            "Epoch 21/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1913 - accuracy: 0.9128 - val_loss: 0.1965 - val_accuracy: 0.9079\n",
            "Epoch 22/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1898 - accuracy: 0.9143 - val_loss: 0.1947 - val_accuracy: 0.9062\n",
            "Epoch 23/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1940 - accuracy: 0.9122 - val_loss: 0.1946 - val_accuracy: 0.9072\n",
            "Epoch 24/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1958 - accuracy: 0.9097 - val_loss: 0.1958 - val_accuracy: 0.9104\n",
            "Epoch 25/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1940 - accuracy: 0.9098 - val_loss: 0.2021 - val_accuracy: 0.9101\n",
            "Epoch 26/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9153 - val_loss: 0.2004 - val_accuracy: 0.9077\n",
            "Epoch 27/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9111 - val_loss: 0.1944 - val_accuracy: 0.9092\n",
            "Epoch 28/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1915 - accuracy: 0.9129 - val_loss: 0.2006 - val_accuracy: 0.9077\n",
            "Epoch 29/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1919 - accuracy: 0.9104 - val_loss: 0.1966 - val_accuracy: 0.9062\n",
            "Epoch 30/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1922 - accuracy: 0.9114 - val_loss: 0.1930 - val_accuracy: 0.9084\n",
            "Epoch 31/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1924 - accuracy: 0.9126 - val_loss: 0.1928 - val_accuracy: 0.9055\n",
            "Epoch 32/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1874 - accuracy: 0.9132 - val_loss: 0.1957 - val_accuracy: 0.9082\n",
            "Epoch 33/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1909 - accuracy: 0.9124 - val_loss: 0.1918 - val_accuracy: 0.9094\n",
            "Epoch 34/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1908 - accuracy: 0.9130 - val_loss: 0.1941 - val_accuracy: 0.9094\n",
            "Epoch 35/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1916 - accuracy: 0.9129 - val_loss: 0.1923 - val_accuracy: 0.9106\n",
            "Epoch 36/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1892 - accuracy: 0.9141 - val_loss: 0.1917 - val_accuracy: 0.9099\n",
            "Epoch 37/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1901 - accuracy: 0.9131 - val_loss: 0.1924 - val_accuracy: 0.9084\n",
            "Epoch 38/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1918 - accuracy: 0.9109 - val_loss: 0.1914 - val_accuracy: 0.9096\n",
            "Epoch 39/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1883 - accuracy: 0.9116 - val_loss: 0.1906 - val_accuracy: 0.9101\n",
            "Epoch 40/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1920 - accuracy: 0.9116 - val_loss: 0.1932 - val_accuracy: 0.9087\n",
            "Epoch 41/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1909 - accuracy: 0.9120 - val_loss: 0.1900 - val_accuracy: 0.9113\n",
            "Epoch 42/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1879 - accuracy: 0.9119 - val_loss: 0.1898 - val_accuracy: 0.9106\n",
            "Epoch 43/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1836 - accuracy: 0.9144 - val_loss: 0.1903 - val_accuracy: 0.9099\n",
            "Epoch 44/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1896 - accuracy: 0.9104 - val_loss: 0.1939 - val_accuracy: 0.9087\n",
            "Epoch 45/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1869 - accuracy: 0.9133 - val_loss: 0.1900 - val_accuracy: 0.9096\n",
            "Epoch 46/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9122 - val_loss: 0.1904 - val_accuracy: 0.9096\n",
            "Epoch 47/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1893 - accuracy: 0.9116 - val_loss: 0.1896 - val_accuracy: 0.9101\n",
            "Epoch 48/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1848 - accuracy: 0.9141 - val_loss: 0.1893 - val_accuracy: 0.9111\n",
            "Epoch 49/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1873 - accuracy: 0.9112 - val_loss: 0.1899 - val_accuracy: 0.9094\n",
            "Epoch 50/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1830 - accuracy: 0.9155 - val_loss: 0.1901 - val_accuracy: 0.9096\n",
            "Epoch 51/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1831 - accuracy: 0.9127 - val_loss: 0.1898 - val_accuracy: 0.9104\n",
            "Epoch 52/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1866 - accuracy: 0.9131 - val_loss: 0.1900 - val_accuracy: 0.9126\n",
            "Epoch 53/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1864 - accuracy: 0.9127 - val_loss: 0.1895 - val_accuracy: 0.9121\n",
            "Epoch 54/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1871 - accuracy: 0.9128 - val_loss: 0.1897 - val_accuracy: 0.9096\n",
            "Epoch 55/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1904 - accuracy: 0.9095 - val_loss: 0.1897 - val_accuracy: 0.9135\n",
            "Epoch 56/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1866 - accuracy: 0.9129 - val_loss: 0.1910 - val_accuracy: 0.9099\n",
            "Epoch 57/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9146 - val_loss: 0.1898 - val_accuracy: 0.9099\n",
            "Epoch 58/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1866 - accuracy: 0.9131 - val_loss: 0.1887 - val_accuracy: 0.9109\n",
            "Epoch 59/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1910 - accuracy: 0.9104 - val_loss: 0.1900 - val_accuracy: 0.9106\n",
            "Epoch 60/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1852 - accuracy: 0.9122 - val_loss: 0.1894 - val_accuracy: 0.9138\n",
            "Epoch 61/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1868 - accuracy: 0.9121 - val_loss: 0.1917 - val_accuracy: 0.9116\n",
            "Epoch 62/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1825 - accuracy: 0.9138 - val_loss: 0.1902 - val_accuracy: 0.9109\n",
            "Epoch 63/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1850 - accuracy: 0.9124 - val_loss: 0.1904 - val_accuracy: 0.9096\n",
            "Epoch 64/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1851 - accuracy: 0.9128 - val_loss: 0.1903 - val_accuracy: 0.9111\n",
            "Epoch 65/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1876 - accuracy: 0.9112 - val_loss: 0.1891 - val_accuracy: 0.9094\n",
            "Epoch 66/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1852 - accuracy: 0.9136 - val_loss: 0.1897 - val_accuracy: 0.9082\n",
            "Epoch 67/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1927 - accuracy: 0.9080 - val_loss: 0.1900 - val_accuracy: 0.9099\n",
            "Epoch 68/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1853 - accuracy: 0.9144 - val_loss: 0.1896 - val_accuracy: 0.9116\n",
            "Epoch 69/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1860 - accuracy: 0.9119 - val_loss: 0.1897 - val_accuracy: 0.9104\n",
            "Epoch 70/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1840 - accuracy: 0.9126 - val_loss: 0.1894 - val_accuracy: 0.9079\n",
            "Epoch 71/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1852 - accuracy: 0.9135 - val_loss: 0.1898 - val_accuracy: 0.9101\n",
            "Epoch 72/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1863 - accuracy: 0.9130 - val_loss: 0.1898 - val_accuracy: 0.9092\n",
            "Epoch 73/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9129 - val_loss: 0.1876 - val_accuracy: 0.9084\n",
            "Epoch 74/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1808 - accuracy: 0.9153 - val_loss: 0.1892 - val_accuracy: 0.9072\n",
            "Epoch 75/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1812 - accuracy: 0.9141 - val_loss: 0.1882 - val_accuracy: 0.9106\n",
            "Epoch 76/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1820 - accuracy: 0.9127 - val_loss: 0.1873 - val_accuracy: 0.9106\n",
            "Epoch 77/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1854 - accuracy: 0.9108 - val_loss: 0.1878 - val_accuracy: 0.9118\n",
            "Epoch 78/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1802 - accuracy: 0.9140 - val_loss: 0.1887 - val_accuracy: 0.9118\n",
            "Epoch 79/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1786 - accuracy: 0.9173 - val_loss: 0.1892 - val_accuracy: 0.9106\n",
            "Epoch 80/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1793 - accuracy: 0.9143 - val_loss: 0.1864 - val_accuracy: 0.9118\n",
            "Epoch 81/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1786 - accuracy: 0.9145 - val_loss: 0.1850 - val_accuracy: 0.9138\n",
            "Epoch 82/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1839 - accuracy: 0.9135 - val_loss: 0.1853 - val_accuracy: 0.9116\n",
            "Epoch 83/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1846 - accuracy: 0.9113 - val_loss: 0.1882 - val_accuracy: 0.9121\n",
            "Epoch 84/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1812 - accuracy: 0.9135 - val_loss: 0.1868 - val_accuracy: 0.9106\n",
            "Epoch 85/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1795 - accuracy: 0.9150 - val_loss: 0.1853 - val_accuracy: 0.9118\n",
            "Epoch 86/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1796 - accuracy: 0.9141 - val_loss: 0.1881 - val_accuracy: 0.9104\n",
            "Epoch 87/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1809 - accuracy: 0.9128 - val_loss: 0.1850 - val_accuracy: 0.9130\n",
            "Epoch 88/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1795 - accuracy: 0.9149 - val_loss: 0.1847 - val_accuracy: 0.9101\n",
            "Epoch 89/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1786 - accuracy: 0.9150 - val_loss: 0.1934 - val_accuracy: 0.9079\n",
            "Epoch 90/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1821 - accuracy: 0.9137 - val_loss: 0.1854 - val_accuracy: 0.9133\n",
            "Epoch 91/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1754 - accuracy: 0.9156 - val_loss: 0.1849 - val_accuracy: 0.9089\n",
            "Epoch 92/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1780 - accuracy: 0.9161 - val_loss: 0.1843 - val_accuracy: 0.9118\n",
            "Epoch 93/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1783 - accuracy: 0.9160 - val_loss: 0.1845 - val_accuracy: 0.9133\n",
            "Epoch 94/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1747 - accuracy: 0.9156 - val_loss: 0.1843 - val_accuracy: 0.9126\n",
            "Epoch 95/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1802 - accuracy: 0.9129 - val_loss: 0.1841 - val_accuracy: 0.9140\n",
            "Epoch 96/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1751 - accuracy: 0.9164 - val_loss: 0.1844 - val_accuracy: 0.9118\n",
            "Epoch 97/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1819 - accuracy: 0.9128 - val_loss: 0.1878 - val_accuracy: 0.9109\n",
            "Epoch 98/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1798 - accuracy: 0.9145 - val_loss: 0.1885 - val_accuracy: 0.9109\n",
            "Epoch 99/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1776 - accuracy: 0.9148 - val_loss: 0.1862 - val_accuracy: 0.9133\n",
            "Epoch 100/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1754 - accuracy: 0.9167 - val_loss: 0.1865 - val_accuracy: 0.9150\n",
            "Epoch 101/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1772 - accuracy: 0.9153 - val_loss: 0.1845 - val_accuracy: 0.9126\n",
            "Epoch 102/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1796 - accuracy: 0.9149 - val_loss: 0.1848 - val_accuracy: 0.9111\n",
            "Epoch 103/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1750 - accuracy: 0.9163 - val_loss: 0.1856 - val_accuracy: 0.9128\n",
            "Epoch 104/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1811 - accuracy: 0.9133 - val_loss: 0.1849 - val_accuracy: 0.9143\n",
            "Epoch 105/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1816 - accuracy: 0.9132 - val_loss: 0.1869 - val_accuracy: 0.9164\n",
            "Epoch 106/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1767 - accuracy: 0.9167 - val_loss: 0.1867 - val_accuracy: 0.9155\n",
            "Epoch 107/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1788 - accuracy: 0.9147 - val_loss: 0.1840 - val_accuracy: 0.9162\n",
            "Epoch 108/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1798 - accuracy: 0.9135 - val_loss: 0.1845 - val_accuracy: 0.9123\n",
            "Epoch 109/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1807 - accuracy: 0.9148 - val_loss: 0.1846 - val_accuracy: 0.9143\n",
            "Epoch 110/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1798 - accuracy: 0.9138 - val_loss: 0.1867 - val_accuracy: 0.9133\n",
            "Epoch 111/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1781 - accuracy: 0.9169 - val_loss: 0.1879 - val_accuracy: 0.9138\n",
            "Epoch 112/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9148 - val_loss: 0.1853 - val_accuracy: 0.9126\n",
            "Epoch 113/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1819 - accuracy: 0.9136 - val_loss: 0.1852 - val_accuracy: 0.9145\n",
            "Epoch 114/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1811 - accuracy: 0.9133 - val_loss: 0.1858 - val_accuracy: 0.9113\n",
            "Epoch 115/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1783 - accuracy: 0.9150 - val_loss: 0.1871 - val_accuracy: 0.9106\n",
            "Epoch 116/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1835 - accuracy: 0.9118 - val_loss: 0.1877 - val_accuracy: 0.9130\n",
            "Epoch 117/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1773 - accuracy: 0.9173 - val_loss: 0.1892 - val_accuracy: 0.9113\n",
            "Epoch 118/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1796 - accuracy: 0.9135 - val_loss: 0.1858 - val_accuracy: 0.9126\n",
            "Epoch 119/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1791 - accuracy: 0.9151 - val_loss: 0.1851 - val_accuracy: 0.9092\n",
            "Epoch 120/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1771 - accuracy: 0.9164 - val_loss: 0.1844 - val_accuracy: 0.9121\n",
            "Epoch 121/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1771 - accuracy: 0.9160 - val_loss: 0.1839 - val_accuracy: 0.9138\n",
            "Epoch 122/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1771 - accuracy: 0.9156 - val_loss: 0.1847 - val_accuracy: 0.9130\n",
            "Epoch 123/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1809 - accuracy: 0.9147 - val_loss: 0.1845 - val_accuracy: 0.9143\n",
            "Epoch 124/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1775 - accuracy: 0.9153 - val_loss: 0.1880 - val_accuracy: 0.9150\n",
            "Epoch 125/128\n",
            "1159/1159 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9153 - val_loss: 0.1867 - val_accuracy: 0.9118\n",
            "Epoch 126/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1763 - accuracy: 0.9156 - val_loss: 0.1849 - val_accuracy: 0.9140\n",
            "Epoch 127/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1778 - accuracy: 0.9162 - val_loss: 0.1858 - val_accuracy: 0.9140\n",
            "Epoch 128/128\n",
            "1159/1159 [==============================] - 2s 1ms/step - loss: 0.1784 - accuracy: 0.9144 - val_loss: 0.1844 - val_accuracy: 0.9140\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AhDQoEewuUjA",
        "outputId": "c726bfd5-d31e-4434-ef52-83aecb14d40b"
      },
      "source": [
        "#0.1\n",
        "model4 = Sequential()\n",
        "model4.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model4.add(Dense(4,activation='relu'))\n",
        "model4.add(Dense(4,activation='relu'))\n",
        "model4.add(Dense(1,activation='sigmoid'))\n",
        "model4.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model4.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128, batch_size=20)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.4091 - accuracy: 0.8187 - val_loss: 0.2307 - val_accuracy: 0.9053\n",
            "Epoch 2/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.2255 - accuracy: 0.9050 - val_loss: 0.2059 - val_accuracy: 0.9116\n",
            "Epoch 3/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.2069 - accuracy: 0.9124 - val_loss: 0.2009 - val_accuracy: 0.9113\n",
            "Epoch 4/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.2082 - accuracy: 0.9106 - val_loss: 0.2005 - val_accuracy: 0.9116\n",
            "Epoch 5/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.2026 - accuracy: 0.9116 - val_loss: 0.1993 - val_accuracy: 0.9116\n",
            "Epoch 6/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.2046 - accuracy: 0.9095 - val_loss: 0.1994 - val_accuracy: 0.9104\n",
            "Epoch 7/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.2049 - accuracy: 0.9089 - val_loss: 0.2052 - val_accuracy: 0.9094\n",
            "Epoch 8/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1984 - accuracy: 0.9108 - val_loss: 0.2025 - val_accuracy: 0.9104\n",
            "Epoch 9/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1949 - accuracy: 0.9113 - val_loss: 0.1940 - val_accuracy: 0.9116\n",
            "Epoch 10/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1994 - accuracy: 0.9125 - val_loss: 0.1929 - val_accuracy: 0.9130\n",
            "Epoch 11/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1979 - accuracy: 0.9102 - val_loss: 0.1924 - val_accuracy: 0.9116\n",
            "Epoch 12/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1945 - accuracy: 0.9126 - val_loss: 0.1946 - val_accuracy: 0.9116\n",
            "Epoch 13/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1965 - accuracy: 0.9077 - val_loss: 0.1912 - val_accuracy: 0.9101\n",
            "Epoch 14/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1956 - accuracy: 0.9102 - val_loss: 0.1915 - val_accuracy: 0.9106\n",
            "Epoch 15/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1962 - accuracy: 0.9084 - val_loss: 0.1985 - val_accuracy: 0.9048\n",
            "Epoch 16/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1895 - accuracy: 0.9129 - val_loss: 0.1918 - val_accuracy: 0.9121\n",
            "Epoch 17/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1935 - accuracy: 0.9087 - val_loss: 0.1909 - val_accuracy: 0.9099\n",
            "Epoch 18/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1913 - accuracy: 0.9103 - val_loss: 0.1914 - val_accuracy: 0.9106\n",
            "Epoch 19/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1886 - accuracy: 0.9118 - val_loss: 0.1925 - val_accuracy: 0.9096\n",
            "Epoch 20/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1902 - accuracy: 0.9080 - val_loss: 0.1908 - val_accuracy: 0.9116\n",
            "Epoch 21/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1912 - accuracy: 0.9088 - val_loss: 0.1923 - val_accuracy: 0.9101\n",
            "Epoch 22/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1904 - accuracy: 0.9098 - val_loss: 0.1919 - val_accuracy: 0.9065\n",
            "Epoch 23/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1878 - accuracy: 0.9107 - val_loss: 0.1902 - val_accuracy: 0.9092\n",
            "Epoch 24/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1858 - accuracy: 0.9119 - val_loss: 0.1896 - val_accuracy: 0.9111\n",
            "Epoch 25/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1886 - accuracy: 0.9114 - val_loss: 0.1897 - val_accuracy: 0.9099\n",
            "Epoch 26/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1908 - accuracy: 0.9092 - val_loss: 0.1911 - val_accuracy: 0.9089\n",
            "Epoch 27/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1883 - accuracy: 0.9097 - val_loss: 0.1907 - val_accuracy: 0.9096\n",
            "Epoch 28/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1864 - accuracy: 0.9113 - val_loss: 0.1911 - val_accuracy: 0.9111\n",
            "Epoch 29/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1836 - accuracy: 0.9127 - val_loss: 0.1941 - val_accuracy: 0.9041\n",
            "Epoch 30/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1831 - accuracy: 0.9122 - val_loss: 0.1901 - val_accuracy: 0.9121\n",
            "Epoch 31/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1879 - accuracy: 0.9111 - val_loss: 0.1896 - val_accuracy: 0.9067\n",
            "Epoch 32/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1874 - accuracy: 0.9094 - val_loss: 0.1884 - val_accuracy: 0.9126\n",
            "Epoch 33/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1866 - accuracy: 0.9101 - val_loss: 0.1883 - val_accuracy: 0.9065\n",
            "Epoch 34/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1855 - accuracy: 0.9102 - val_loss: 0.1918 - val_accuracy: 0.9087\n",
            "Epoch 35/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1852 - accuracy: 0.9097 - val_loss: 0.1885 - val_accuracy: 0.9106\n",
            "Epoch 36/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1866 - accuracy: 0.9088 - val_loss: 0.1945 - val_accuracy: 0.9016\n",
            "Epoch 37/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1841 - accuracy: 0.9107 - val_loss: 0.1886 - val_accuracy: 0.9143\n",
            "Epoch 38/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1854 - accuracy: 0.9118 - val_loss: 0.1899 - val_accuracy: 0.9113\n",
            "Epoch 39/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1837 - accuracy: 0.9118 - val_loss: 0.1906 - val_accuracy: 0.9133\n",
            "Epoch 40/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1868 - accuracy: 0.9102 - val_loss: 0.1889 - val_accuracy: 0.9118\n",
            "Epoch 41/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1846 - accuracy: 0.9106 - val_loss: 0.1876 - val_accuracy: 0.9126\n",
            "Epoch 42/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1867 - accuracy: 0.9108 - val_loss: 0.1902 - val_accuracy: 0.9096\n",
            "Epoch 43/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1849 - accuracy: 0.9125 - val_loss: 0.1874 - val_accuracy: 0.9138\n",
            "Epoch 44/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1855 - accuracy: 0.9094 - val_loss: 0.1896 - val_accuracy: 0.9126\n",
            "Epoch 45/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1834 - accuracy: 0.9116 - val_loss: 0.1919 - val_accuracy: 0.9130\n",
            "Epoch 46/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1865 - accuracy: 0.9114 - val_loss: 0.1884 - val_accuracy: 0.9152\n",
            "Epoch 47/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1830 - accuracy: 0.9126 - val_loss: 0.1898 - val_accuracy: 0.9123\n",
            "Epoch 48/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1856 - accuracy: 0.9115 - val_loss: 0.1880 - val_accuracy: 0.9104\n",
            "Epoch 49/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1869 - accuracy: 0.9109 - val_loss: 0.1860 - val_accuracy: 0.9126\n",
            "Epoch 50/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1860 - accuracy: 0.9110 - val_loss: 0.1866 - val_accuracy: 0.9133\n",
            "Epoch 51/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1891 - accuracy: 0.9095 - val_loss: 0.1883 - val_accuracy: 0.9133\n",
            "Epoch 52/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1810 - accuracy: 0.9143 - val_loss: 0.1876 - val_accuracy: 0.9116\n",
            "Epoch 53/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1871 - accuracy: 0.9117 - val_loss: 0.1866 - val_accuracy: 0.9123\n",
            "Epoch 54/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1838 - accuracy: 0.9115 - val_loss: 0.1890 - val_accuracy: 0.9138\n",
            "Epoch 55/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1828 - accuracy: 0.9139 - val_loss: 0.1875 - val_accuracy: 0.9138\n",
            "Epoch 56/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1846 - accuracy: 0.9129 - val_loss: 0.1881 - val_accuracy: 0.9130\n",
            "Epoch 57/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1847 - accuracy: 0.9092 - val_loss: 0.1880 - val_accuracy: 0.9116\n",
            "Epoch 58/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1816 - accuracy: 0.9136 - val_loss: 0.1868 - val_accuracy: 0.9101\n",
            "Epoch 59/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1876 - accuracy: 0.9095 - val_loss: 0.1882 - val_accuracy: 0.9096\n",
            "Epoch 60/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1780 - accuracy: 0.9153 - val_loss: 0.1858 - val_accuracy: 0.9111\n",
            "Epoch 61/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1819 - accuracy: 0.9153 - val_loss: 0.1881 - val_accuracy: 0.9104\n",
            "Epoch 62/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1858 - accuracy: 0.9115 - val_loss: 0.1856 - val_accuracy: 0.9113\n",
            "Epoch 63/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1817 - accuracy: 0.9122 - val_loss: 0.1917 - val_accuracy: 0.9113\n",
            "Epoch 64/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1844 - accuracy: 0.9121 - val_loss: 0.1892 - val_accuracy: 0.9109\n",
            "Epoch 65/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1841 - accuracy: 0.9111 - val_loss: 0.1911 - val_accuracy: 0.9070\n",
            "Epoch 66/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1833 - accuracy: 0.9119 - val_loss: 0.1859 - val_accuracy: 0.9118\n",
            "Epoch 67/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1799 - accuracy: 0.9129 - val_loss: 0.1870 - val_accuracy: 0.9113\n",
            "Epoch 68/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1796 - accuracy: 0.9135 - val_loss: 0.1875 - val_accuracy: 0.9121\n",
            "Epoch 69/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1815 - accuracy: 0.9127 - val_loss: 0.1863 - val_accuracy: 0.9135\n",
            "Epoch 70/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1802 - accuracy: 0.9129 - val_loss: 0.1899 - val_accuracy: 0.9101\n",
            "Epoch 71/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1796 - accuracy: 0.9148 - val_loss: 0.1870 - val_accuracy: 0.9094\n",
            "Epoch 72/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1820 - accuracy: 0.9130 - val_loss: 0.1923 - val_accuracy: 0.9116\n",
            "Epoch 73/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1851 - accuracy: 0.9109 - val_loss: 0.1858 - val_accuracy: 0.9140\n",
            "Epoch 74/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1826 - accuracy: 0.9125 - val_loss: 0.1862 - val_accuracy: 0.9094\n",
            "Epoch 75/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1847 - accuracy: 0.9120 - val_loss: 0.1884 - val_accuracy: 0.9130\n",
            "Epoch 76/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1832 - accuracy: 0.9130 - val_loss: 0.1872 - val_accuracy: 0.9116\n",
            "Epoch 77/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1808 - accuracy: 0.9123 - val_loss: 0.1860 - val_accuracy: 0.9094\n",
            "Epoch 78/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1820 - accuracy: 0.9147 - val_loss: 0.1885 - val_accuracy: 0.9104\n",
            "Epoch 79/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1817 - accuracy: 0.9133 - val_loss: 0.1856 - val_accuracy: 0.9130\n",
            "Epoch 80/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1803 - accuracy: 0.9138 - val_loss: 0.1859 - val_accuracy: 0.9121\n",
            "Epoch 81/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1803 - accuracy: 0.9132 - val_loss: 0.1898 - val_accuracy: 0.9113\n",
            "Epoch 82/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1817 - accuracy: 0.9144 - val_loss: 0.1860 - val_accuracy: 0.9130\n",
            "Epoch 83/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1873 - accuracy: 0.9086 - val_loss: 0.1888 - val_accuracy: 0.9118\n",
            "Epoch 84/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1834 - accuracy: 0.9125 - val_loss: 0.1852 - val_accuracy: 0.9130\n",
            "Epoch 85/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1828 - accuracy: 0.9140 - val_loss: 0.1863 - val_accuracy: 0.9113\n",
            "Epoch 86/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1815 - accuracy: 0.9145 - val_loss: 0.1871 - val_accuracy: 0.9116\n",
            "Epoch 87/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1756 - accuracy: 0.9168 - val_loss: 0.1851 - val_accuracy: 0.9128\n",
            "Epoch 88/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1838 - accuracy: 0.9121 - val_loss: 0.1855 - val_accuracy: 0.9116\n",
            "Epoch 89/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1826 - accuracy: 0.9102 - val_loss: 0.1895 - val_accuracy: 0.9113\n",
            "Epoch 90/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1813 - accuracy: 0.9156 - val_loss: 0.1905 - val_accuracy: 0.9118\n",
            "Epoch 91/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1774 - accuracy: 0.9147 - val_loss: 0.1871 - val_accuracy: 0.9126\n",
            "Epoch 92/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1825 - accuracy: 0.9112 - val_loss: 0.1850 - val_accuracy: 0.9135\n",
            "Epoch 93/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1822 - accuracy: 0.9127 - val_loss: 0.1869 - val_accuracy: 0.9123\n",
            "Epoch 94/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1820 - accuracy: 0.9126 - val_loss: 0.1862 - val_accuracy: 0.9130\n",
            "Epoch 95/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1810 - accuracy: 0.9133 - val_loss: 0.1909 - val_accuracy: 0.9121\n",
            "Epoch 96/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1832 - accuracy: 0.9119 - val_loss: 0.1875 - val_accuracy: 0.9121\n",
            "Epoch 97/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1849 - accuracy: 0.9129 - val_loss: 0.1870 - val_accuracy: 0.9116\n",
            "Epoch 98/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1801 - accuracy: 0.9131 - val_loss: 0.1871 - val_accuracy: 0.9121\n",
            "Epoch 99/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1830 - accuracy: 0.9127 - val_loss: 0.1869 - val_accuracy: 0.9113\n",
            "Epoch 100/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1848 - accuracy: 0.9116 - val_loss: 0.1850 - val_accuracy: 0.9111\n",
            "Epoch 101/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1827 - accuracy: 0.9123 - val_loss: 0.1877 - val_accuracy: 0.9126\n",
            "Epoch 102/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1843 - accuracy: 0.9119 - val_loss: 0.1848 - val_accuracy: 0.9113\n",
            "Epoch 103/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1815 - accuracy: 0.9127 - val_loss: 0.1872 - val_accuracy: 0.9084\n",
            "Epoch 104/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1825 - accuracy: 0.9122 - val_loss: 0.1898 - val_accuracy: 0.9109\n",
            "Epoch 105/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1806 - accuracy: 0.9135 - val_loss: 0.1847 - val_accuracy: 0.9111\n",
            "Epoch 106/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1782 - accuracy: 0.9150 - val_loss: 0.1871 - val_accuracy: 0.9121\n",
            "Epoch 107/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1851 - accuracy: 0.9107 - val_loss: 0.1864 - val_accuracy: 0.9130\n",
            "Epoch 108/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1823 - accuracy: 0.9126 - val_loss: 0.1848 - val_accuracy: 0.9118\n",
            "Epoch 109/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1814 - accuracy: 0.9137 - val_loss: 0.1872 - val_accuracy: 0.9133\n",
            "Epoch 110/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1816 - accuracy: 0.9138 - val_loss: 0.1851 - val_accuracy: 0.9077\n",
            "Epoch 111/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1774 - accuracy: 0.9144 - val_loss: 0.1840 - val_accuracy: 0.9143\n",
            "Epoch 112/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1763 - accuracy: 0.9144 - val_loss: 0.1930 - val_accuracy: 0.9092\n",
            "Epoch 113/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1781 - accuracy: 0.9160 - val_loss: 0.1867 - val_accuracy: 0.9079\n",
            "Epoch 114/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1786 - accuracy: 0.9145 - val_loss: 0.1856 - val_accuracy: 0.9147\n",
            "Epoch 115/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1836 - accuracy: 0.9129 - val_loss: 0.1854 - val_accuracy: 0.9121\n",
            "Epoch 116/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1752 - accuracy: 0.9168 - val_loss: 0.1897 - val_accuracy: 0.9121\n",
            "Epoch 117/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1792 - accuracy: 0.9143 - val_loss: 0.1865 - val_accuracy: 0.9130\n",
            "Epoch 118/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1819 - accuracy: 0.9137 - val_loss: 0.1846 - val_accuracy: 0.9111\n",
            "Epoch 119/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1813 - accuracy: 0.9119 - val_loss: 0.1857 - val_accuracy: 0.9123\n",
            "Epoch 120/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1826 - accuracy: 0.9128 - val_loss: 0.1860 - val_accuracy: 0.9147\n",
            "Epoch 121/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1800 - accuracy: 0.9155 - val_loss: 0.1872 - val_accuracy: 0.9111\n",
            "Epoch 122/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1793 - accuracy: 0.9152 - val_loss: 0.1852 - val_accuracy: 0.9121\n",
            "Epoch 123/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1785 - accuracy: 0.9138 - val_loss: 0.1856 - val_accuracy: 0.9111\n",
            "Epoch 124/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1833 - accuracy: 0.9122 - val_loss: 0.1888 - val_accuracy: 0.9111\n",
            "Epoch 125/128\n",
            "1853/1853 [==============================] - 3s 1ms/step - loss: 0.1775 - accuracy: 0.9156 - val_loss: 0.1843 - val_accuracy: 0.9138\n",
            "Epoch 126/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1796 - accuracy: 0.9127 - val_loss: 0.1866 - val_accuracy: 0.9140\n",
            "Epoch 127/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1772 - accuracy: 0.9160 - val_loss: 0.1846 - val_accuracy: 0.9130\n",
            "Epoch 128/128\n",
            "1853/1853 [==============================] - 3s 2ms/step - loss: 0.1773 - accuracy: 0.9145 - val_loss: 0.1841 - val_accuracy: 0.9116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEGCAYAAABy53LJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hUVfrHP+9MeqUkAUKA0HvvTYrSbCgq9t7b7trbru66v113LatrXXXFtlasWFEpKgJC6AQhhFASEkhCei9zfn+cezMzISSTkFDP53nyZHLn3jt3JnfO97z1iFIKg8FgMBh8xXG0L8BgMBgMxxdGOAwGg8HQKIxwGAwGg6FRGOEwGAwGQ6MwwmEwGAyGRuF3tC/gSBAVFaXi4+OP9mUYDAbDccWaNWuylVLRtbefFMIRHx9PQkLC0b4Mg8FgOK4Qkd11bTeuKoPBYDA0CiMcBoPBYGgURjgMBoPB0CiMcBgMBoOhURjhMBgMBkOjMMJhMBgMhkbRosIhIjNFZJuIJIvI/XU830VEFonIRhFZKiJxHs99KyJ5IvJlrWPeEJGdIrLe+hnSku/BYDAYDN60mHCIiBN4AZgF9AMuFpF+tXZ7EnhLKTUIeBR4zOO5J4DLD3H6e5RSQ6yf9c186YbjlZSlkLn1aF+FwXDC05IWxyggWSmVopSqAN4HZtfapx+w2Hq8xPN5pdQioLAFr89wIqEUzL8KFv/1aF+JwXDC05LC0RFI9fg7zdrmyQZgjvX4XCBcRNr6cO6/We6tp0Uk8PAv1XDck7cHSnPhwI6jfSUGwwnP0Q6O3w1MEpF1wCRgL1DdwDEPAH2AkUAb4L66dhKRG0QkQUQSsrKymvGSDcckGRv075wUcDV0CxkMzU9ltYtvN+/D5VKw6K/w25cNH1SbnT/Bl3c22zUppWiJVV5bUjj2Ap08/o6zttWglEpXSs1RSg0FHrK25dV3UqVUhtKUA6+jXWJ17feKUmqEUmpEdPRBPboMJxq2cFSXQ37a0b0Ww0nJ/1bu5qb/reGrTRmw+lVY93bjT7L+PUh4DcoKGn1oTnEFHyakct2bCUx9cinD//o9vf74DbsPlDT+OhqgJZscrgZ6ikhXtGBcBFziuYOIRAE5SikX2pKY19BJRaSDUipDRAQ4B9jc7FduaHmUApHmO1/GehAnqGrI2QGtuzTfuQ2GBlBK8fZK3Q/wf8t3clZZAWRtY+u+Ar7dvI8rxsbTJjQAgJKKKgBCAuoYfvdbw1l+GgTVziU6NL9lFDDnxeWUVlYTGxnE0C6tiQz2JzLYn5BA5+G9uTpoMeFQSlWJyG3AQsAJzFNKJYrIo0CCUmoBMBl4TEQU8BNwq328iPyMdkmFiUgacK1SaiHwjohEAwKsB25qqfdgaCF++xK+/AP8bj0Ehh3++ZSC9PXQbTLsWKTjHN2nHv55DYekqtoFgJ+zcU6L5Mwi3lqxiz+e0Y8Av6PtKT8MKorhmYEw+wXoPYtfkg+QklXMkE6t2LI7HYIUKm83d72zksSsSl5btpPrJ3Zj14Fivtm0jw6RQSy4fQJhgR5DcHUVZG3Tj/PToJ1vwlHtUtz/8UZCApy8f8MYBsVFIs05KauDFv3PKaW+Vkr1Ukp1V0r9zdr2sCUaKKU+Ukr1tPa5znI/2cdOVEpFK6WClVJxlmiglJqqlBqolBqglLpMKVXUku/B0AIcSIbiLDiwvXnOV5gBJdnQawb4h+rzn4xUlsEXf4D8vQ3ve5hc+foqbn5nrdc2l6thX/r/Vu7mrRW7+WhN092JmQVlJGd6J1wmZxayL7+syeesdike/3Yr2/b5mMhZuA9KDsC+TQC8tWIXbUIDeOXy4UT7adeQKBeu7GQePrMfw7u05l/fJ/F94n6m92/HrgPFPPJ5ovc5c1K0qxUgP9Wnz9N+bfau4dPY/zE45ECLiwacJOtxGI4xqqwv+IEdEDu0ccemroJ2AyAgxL3Njm90GAJtuzdPZlXuLti9Qj8OjYIepzWva60lSFsFa16H2CEw/KoWe5nMwjJ+ST4AwNo9uQzr3JrsonLOffEXLhzRidum9jzkscuSswF4YUky5w+Pa5LV8YcP1pOYXsCKB6YSEuBHWWU1c19eSVRYAF/9biL+taygFTsOsHpXDlP7xNA/NqLOgTVhVw4vLt3BFxvT+fL2iUQG+9d/EWVWKLY4m715pfzw235unNSdmIggzuodCtYteG6nIq6Z0JVrJnQlJauIDpHBBAc46dI2lGcXbeeUXlHMHmIlm2a6hWTFug1cs2Ah5w+P449n9iXQz9vdlJiez/6CMlxFB4j+7m4+D/xF57Au9Yfz/tuoz7MpHMe2ouG4pbJU/27sAF+aB/Nmwo//9N6esQHEAe0HQNsezWNxfHYLfHaT/nnnfFjzxuGfs6XZv0X/LmrZLMIlWzMBCPJ38O8ftNX46BdbSM0p5V/fJ7Fmd06dx2Xkl5KcWcTEnlHszSvl47V1Wx2rduYw698/c8/8DQdlBCVnFrF8xwHySyv5bF06AAvWp5NTXEHS/iLmLdvptX+1S3Hvxxv41/dJnPncMk55YglLt2Ue9JrfbN5HgNNBRl4Z93508OtmFpaxamcOq3bmsGZ3DnszMvR7Sk/loU+11XHp6M4AnNPH7X69KL605nG36DCCA7QA/G5qD4Z3ac1Dn252W0r7E1HiIJtW7EtNZkDHCN5euZvzXlrOrykHSMkqYtXOHC5/7VfOeHYZ17yRQMpn/8dMVlAw4nd6spD4KRSk1/m5NidGOAxHnirLHG/sAF+wVwe/N30ELpd7e/p6iOoFAaHa4sjbDVUVTb++khzYswJG36zjMJ3HwaJHdZ3I0WLB7fDF7+vdpThVW17FOS3rqlr0WyaxkUH87tSe/JiUxTM/JLFgQzo3nNKN2FbB3PHBBorLqw46btl2bW08MKsvg+MieWFJMhVVLv3Zvn4GpeVV3P/xRua+vIK0nBLmr0ljfoK3uLy3ag9+DqFbdChvLN+JUorXl++iT/twTusbwzM/bCc9zz1Yf79lH6k5pTw2ZyCPnzeIEH8/rnp9NU8s3FoTp3G5FAsT93FKr2jun9WHhYn7efXnlJpzJGcWcupTPzL35RXMfXkF5720gsc+WQlAyu7drNqZwy2TexDXWlvBXcP0e1cIEUXu83ji53Twr7mDKa2s5pWf9D5qfyKpEkuqI5ZpsZXMv2kcr14xgj0HSrjwlZVMta4hMb2AB2b14fNbx3Nx5wJoP4CIM/8KE+4A5YJVrzTp/9oYjHAYjjxVtsXhIRwpP8KaN+s/rlDP8ihI0wO7TcYG6DBYP27bQ395cnfVfY7ibPjyDvjoGv2TsvTgfXYs1ucYcB606QqnP65dE0v+7su7q+HjNWlsSstv1DF1UlECGz/Un1EdKKX4bN1ediSuAiAxaXuTc/eVUmQXldcMqrUpq6zm5+3ZTO0bw5Vj42kd4s8zP2ynR0wYd03vxb/mDiE1t4RHv9hy0DUsS84mKiyAPu3D+cNpvUjLLeX5r1ehVr4Eu5fxf699wAcJqdx4SjdWPHgqY7u15ZEFiezIKqp57Y/WpDFjQHtumdyDpP1FPP3Ddn7LKOCqcfE8clZ/FIo/L0isee3Xlu2kU5tg5o7oxNyRnfjs1vFcOKITLyzZwX0fa0thQ1oeGfllzBrQnmsndGVm//b8/eutPLdoOznFFVzzRgIXORbxyemKd64bzRtXj+SGUW0AGNa2ig2PTOfuGb09PiT9P5foPpCddMjPukvbUGYPieXdVbs5UFROadomNlZ2pG1sD8LK9gEwrV87Ft01mXlXjeCZC4fw9IWD+fneKdw4qTuDO7UivCgFv3Z99Qlbx0OfMyDhdX3PtCAmxnE02LcZYvqCo/nT5I4LKi3TPGeHOy33x8e1j37AeYfOtCqwhEMcsGk+xI+HokwoTPcWDvvc0b0OPsfCh2DzR/pLVpCuj+822XufpIUQEgUdh+m/2w+EEdfA6v9qd0C7/g2+xczCMu75aAMD41rx+a3jG9y/Xnb9rONC+Wm6uNHjvimrrOahTzfzydo9bAlOAwVSnMkna/dy3vA4vtmYxsKlP1IU2YuOrYIBKCiroqLaRUSQP+0jgrhmQjzhFdngcDL/t3Lu/Xgjfg6hfWQQIQFOQlQxQ9sq7rtkJitSDlBaWc2sLorQylxuntydf367jcfmDCTQz8morm24eVJ3Xly6g7jWwdx+qo53uFyKX5KzGd8jCodDmNw7mguGx1H564uIfwnVOIhP/4onz3+c84bHQd4enjkrjpmvFnDrO2t5+sIhJKYXkF9ayWWjuzC0cyse+/o3nl20nVYh/swe0pHgACd3nNaLx77ZysOfJzJnWEdW78rlT2f2w+nQcY3gACf/PH8QbcMCeHHpDs4eEsvy5Gz8HMJpfdshIjw7uzP/kGye+j6JN1fsorqsgAf9X0P2Z8EpZ1r/YC1MwZW5UDuzzBIOOo2CDe/pbCmnx1BblAXVFRDZkVsm9+DTdXt588dE7ixOJTN4InHxsfDLVzXHRYcHMrVPu4Pvi/JCbYVHedznY26F377Qrzvy2qbfcw1ghONIsz8R/jMeznoWhl95tK/m6GAHx8vydWZKUCtIX6u/TDt/1LOmuijUszD6nAlbPoNZj8OyZ/S2uJH6d5tu+nddbrA9v8LG92HiXXDqw1pEVr2qhcw/SO/jqobk76HXTHA4UUqRVVROzJSHtFj9/C/e7/wIb6/czb0z+zCpV93FpV9tzMClYENqHhtS8xjcqZXX8/sLyqhyqZrBvF6SvrWurVJbXZFxNee48e01rE/N40/jQgleW4YSJ3H+hdzw1RaS9heyd9k7PB/wHDdVPscnKe1wOISIYD/8HQ4KyqrILiont6SCP2fdiaos4YWCv9CrXRjT+rVjb24p5VUuLt73HwanLOW+9z8mNCycYH8HY5ddA5s7cf1lnzBnWBxRYe7OP3dP782+/DKe+j6JiGB/rhwXz9Z9hWQXVTChRxQAIsLj5/aldMdilpcMoFgFclnYaoKHdtCxrFem0C6qF09f+Ca3v7uOWf/+mfAgP7pHhzKmWxtEhItHdeb5JclcNLJzTezghlO6kVNcwcs/pbBgQzphgX7MHRFHbX53ak++TdxXE58Y1yOKyBAdEA/4+Er+lLeHkMnzeemnnbw3vgBZXQWlHrEbWxxKsrXb1OE4+Lm4kbD2Te06bdtdb1MK3p2rXa43/kSPmDBOH9CBn3/5mTsDoN+QMThaufTzRftq/td1Ylsz0R7WTucxOklkzRtGOE4otn2tf2/9yggH6AHePxgqLdM6aWE9wpEOwW1g6GXw2wLd1HDbVzD6JrdwhLTR+9QWDlc1fH03RHTUwgEQPxFWPA9pq6HrRL0tbTWU5lLR7TQ+S0hl3rKdbN1XyD/PG8iFvc+g+rev+Mv6c6lWTq6ct4rZQ2J56PS+xEQEeb3cZ+vT6RETRkZeKW+t2M1THsJRUFbJnBeXU1BayVvXjmJo59Zex2YWlLE7p4SR8W30QJP0HQS3htJckrdvYV5qLonpBWzNKMDpEP5z2TBmOtfAWpC4EbTbt5mikipe/imFlzsVQxb8Z+R+OOWKgz7SBz/dxLu/7uThkI04KkuIrUjgyksuZ+aADu6dnt4MUkLFb9+yQI3muq65ONK3Q34qUlVOVJj3e3f8+hJPVv1IYd+7eWRBIhvS8ogI0oPyxJ5uoZUtnxNSlkn8mU9QVJhP8M+/g92/wNav9YC8J5vJM9JYdt9UXluWwlsrd3PjpO41WVFXj48nPb+UaybEu88pwv2z+hAe5MeT3yVx3YSuhAcdnCEV5O/kH3MGMfdl7fK8dYplqe5dA7t/QYC7++Rw69QZBH99u36u5ID7BKVWVpVy6dhXqEeLvbI8CIzUXgXQtRm2cOxZoSdJiK4OD4rglindeXvLOwCMGDURcq24SH5a/cKRZQlHlIdwiED/c+CHP2uLOiL20McfBibGcaRJ+k7/3vmTO7voeKcoE359RQ9yvlBZCqHWAHJghx6sQc+Utn/nPs/mj3X6rU3hPv1F6D5Vi8O2r6DvWTDj796psm17HJyxte5t2LcRpv9VB9EBuozVbq9dy9z7JS3EJX7M/jaQez/aCMDwLq158NPNbAgZjbMin3EBO1hyz2R+f2pPvtm0j8lPLuX5xdspq9Q9snZlF7MhNY+5I+I4d1hHvtios35s/rJgC/sKyggP8uOK11axPtXdZWd/QRnn/Wc5c19ewZb0Am2hFqSR0/tCAF74ZDEL1qcTGuDk8jFd+PzW8XqQ358ICHSdhKOymH/P6cnj5w9ielyl9b6+q/Nf8bupPYmVXByWcN8e/B3T+7V375C7C/L36H1j1qEUXBBoxZeqymBvgvcJXS5Y8TyO7d/y/KkBXD+xK19vyuCN5bvoERNG+0hLZJSClS9A257EjjibXhMvgIAwWPpPHdwddCEEhMPKF4kM8efO6b1Z//B05o5wdzFqGxbIv+YOISbcW7hEhNum9GD55G3cOyaEQzGqaxsuGd2ZAD8H0/pZrqAVL+rX9Q+BTfMJ9hN9T4JOmrAp8+iMVJLtfeKyfAiKhCgrLTnLo9X/ihesB0qLFNA/NpIruxdT7ReCX5t4t1g01Donexs4/HQczpOeM/Tv7XX/z5sDIxxHkuJsPUh2GqMDxDt/PtpX1Dxs+gi+uUf7W32hqkzPkhx+2jJIXQ2hMTDqeu2K2bdRx4E+vs479bYgHcLbg9MfJt4Jfc+GOa8eHCuqSzg2fgjtBkL/Oe5tQZHQYTAVyT/y0tIdPPb1b6Su+oxfq3pR7gznjatH8s3vJ/LmNaPo0z6cy5aEUqmcPNQzlY6tgrljWi8W3nEKE3pE8eR3SZz13DL2F5Tx+fp0ROCswbFcMTaeiioXHyboRtHfbt7Hx2vTuHVKDz66eRytQwO4/L+/8q/vk0jaX8hl//2VnKIKIoL8+euXW1BJCwG4YuNAAC7sqVjxwFTevX4MfzyzHz3bhev3kpmo4zat4wE4vauTuSM6Ifbgk7Yaij1mzBbtI4O4qb/OAlpR3Y+x1Qk4cjw+O1tUu02hT+FK5l/ei56Z30HXU7To1r6H9yyvuQ8Ct3zEQ2f046d7p3DrlO7c4xlATvwU0tfBuNu1mycgRLsgdy+DoAiY+Q8YdkXT00v3byZ25V8I+OXJend79Oz+LLpzkna15e/VLtBhV2ird8tnOu5WnAXhsd7CUeohHMW10p9t4QiKhPAObpdSzk7taRh5nf47zS26fR1pONv1059FpFXXkZ9KvWRtgzbd9ffBk5i+ENnpkJOF5sAIx5Ek+QdAwbS/6Apn23d9vGOb8L6mq1aV6QB463gtHGmrtaup53T9fNJC+OY+7QbwFIDCDP1FBD3gXPi2dnPVpm137dayfc0uF2RshM5jUFCTpQNA/AQc6Qk88+1GEpYvolNFChXdp/HNHyYyuXcMIkJYoB+vXzWSmOho9oQPoVuO20LpGhXKK1eM4I2rR5KeV8r5/1nO/DWpjO7ahg6RwfRqF87orm349w/bmfLkUv7wwToGdozk9qk9iG0VzHs3jGF0tzY8t3g705/+id05Jfz3ypHcOa0XK1IOkLvhS1L8epBSFUVVSDvGtC6q0/XC/i06aB9mzZyLrFqF/DRLTJR1/x3M7I66Wvq5wOv0IPTrS+4nd/6sEwVO/RNSXcHILX9HijP14Nd+kLe1Blqg/UO1sGz+GFwuYsKDuGdGH2b0tyyZimL47o866WDoZe5jB1+kf0/9o3Y5jr6x6emlluCyZYE7GaMO/JwOOrWxrJJVr+jXG30jDLxA38/f3q8FcuB5erJnZyuV5Wu3J+gJoSe2cIAOXNttRH59WU+WJt6tJ05pljVdVaHvz/YD9N+B4Tru15DFkbWt7gQQEd1FIWVJve/9cDDCcSRJ+lZ/seNG6UweT7eML/z0JPz8VEtdnW+U5MAbZ+r1L2zsoKGvwlFZBn5Bera0d43OgOo0EsJiIHYYLH9Ozzxbd3XXZFRX6cHQF5+tnQ2112qJkbsTKgqhw2C+3byPU5/6saaIrTxuAn6qkjt75/Bxl08hNIZJF919UKVuTEQQP9w5ie7j5kDWb97vH5jcO4Z3rh9DQWkVabml7mpg4MHT+zKjfzsGdozk7MGxvHDJsJrq5o6tgvnvlSNZfNdkbpzUjdeuHMHY7m25ZHRnRkeVE5m9ji/KBvLYnIH4tY3Xn8dBn2ep/gxj+unPEKBov7638tOg9xnaojvERCU4fweVAa245/JzkYFzYf27ejBUSmd0xU/Q/5c23SHxEwiM0O6QrhP14Ge7XKvKYcvnerY+7EpteXimTdsse1o/N+sJb2ux+xS4eTmMsIK6rbtoK2TFC/DMIHh2mBYmm/R1+l60B2ZPtn+nXV/l+TrZoSEqinVAuc+Z+nW7T4WQtvo14kZCW8vtZN/rZXnuDL5DWRwA0X10uvgzg3RW3oA5ENFB3+9pq/VnnPyDvs4+Z7rP0aqTWziWPKaF1pOqcn1fe8Y3POk5Q8cNdy+r+/nDxAjHkaK6EpIXQ89p2hztNUOboplbfD/Hbwsg4Y0Wu0SfyNyiBxPP2ENJI4WjqlQLR9sebveWHdzuNRPKC3R67Sn36Blg3m49EKK0q6ohOg4HxB07SV+nf3cYzP9+1QPvf37UlswPxV2pUg4uz/uP9tef9mftKqkDEdHXB+4ZrQdDOrXiwxvHctW4eM4a7Ba4wZ1a8cxFQ3n24qE8fv5gOrc92O/eNSqUB7psZ2Kknr36Ox38O3oBVTipHHChFqJWnQ8SLEAPnMqlm+J5WhwlOfqzbtVZW3M7FmkBruN4//Z9GdqlDUz4g85uW/QXPTAV7NXCIQKD5ur9+56ts9DiJ+p97c85+Qc9oA6aC71nactj04fer3VgB/zyLAycq2NMtWnX3ztederDet/OY3Vs6tObYPsP2u3zzgX6XvzqLu8JWPEBfX+OuVnH0jZ+ePDr1Gb9u/rax1p9Vp3+0P9c/bjXDC0i4GFd57kz+EpquQA9hWP4ldqS6jxWx20m36+3x410Lzy2ab4+f7fJ7nNEdoK8VB3X+/kpWP68fs+en6NyaWGqi64TwS+4zvu0OTDC4SvltZqfVVdB9nad2ZCT0rDlsGelnlXYgSvbLbPxA30Ou0ahPsoKdKCypO6WDjV4+l+bm3LLzeOVYdJY4SjXA4+daSJOd8+qAXO0C+D0p9z56QeS3am44T5YHEGREN0HV+oq3l6xi6Jda8AZwG5nJ35JPkB82xB+3ZnDhtQ8PtiUzzZHd0Lyk/SXefDF9Z87qoceMA7xhezdPpw/n93fu+upr3x2s55BH9gBqatov/NTiofdyB1zLbFq1VnPQmsP/vutHkftBui+WuLQwmH7yCPjoNd0PaCl/nrw62Ztc3/WUT11ltrat/UAD9rtBPqziewEI67Wf3ceq/93dpxj03zt1uo2WQ/yfc6AxM/cVfylufD+JXrSMO1R3z6TqJ5wzgsw52W4+mstjh9eAW+fA64qGHubFo8tn7mPsV3CvWfpuqCkhW63ZV24XLDyJT3h6DTavX34VRARB/3O0a4z0Pe9y6XPFxqlXUr1WRzt+sM5L+rrP+cFt9jEWcsIpSyBbd/o2JtnrCIyTv+vV/9Xv0+HU7u6bLItK6suVxVoF27XU/R7P84Wcjpx2PMrPBanb9icnfqf8dJYeH4EvDASnh0KG96v/xzbvgGHvzbHQZurHYbAL//W53i6X8O9m8qtxV3spn51cWAHPNG95QJjtoB6+nVLLMFolKsquEY4XDH93ZlOUT3hzi3alLeF5UCyjlmA/tx8oCp2OCUpK/nT55tJ3vgLrpj+vL92P06H8N8rRxIe5Mdj3/zGsu1ZFMWOB0TXhTh8+Er0nKGz4iqKfXu/vlBeBBVFOkPnnfPhqzshvANtZj5YU7xGqy46v7+wVrB4f6Ll+uumB5iQKG2h2a6OyDjoNkX712vHOYqztfB71gJMuk/P1Ne8rl1ctqi07gJ3bIa4EfrvoAjdUHH7d/DN/TqeMMBjABx0oZ7Fv3uBdhu+f6meZF30js//Ry8Cw+HSj/QgXpABF7+vBajdQFj4R/f/Y/tCfd0dhupYRXW5Loo7FNu/066+Mbd4WzvtB8Kdifo+rLE4crTbE6VFIzTa+7vgqtbf02Dvup2DiO6ts7d+ekJbhQMv8H4+Mk5PNFe9ogWw/xydGWgLYFYSIG4XWl30mqGt9Xqq15uKEQ5fsGsCtn6Nem6YLuBxVcOZz8B5r+ng4/p3Dn28q1r7hntO0ze/zQVv6OPPfk6bnVu/qnlqf0EZy3d43JBKuVcFy1h/6Nfa9bOeoSR+6tNb25ldzNBHvztkY7qDsMXLy+JopHBUlYJfINmBOrVydXWPuvfzrMmosTgaHnCKy6uYtyeaMFchN/VXxFcks66yM/MT0pjaJ4YeMWFcMrozK1NycClod8aDcMMSd2ykIXrN0IPRzp9823/bt/DKZHh5Erw2/eCAMkCxFcwefrXOItq3Cab91buKvpVuoneQu2rfRj2zteMFYTG1LI5OepCP6XfwvWPHBzx95UEROoED3G6qQxE/QZ9z1csw7HKY8pD7uR6n6jhG+np4dYqu0TjnJXfNTFMIbw/XL4Ybf9LFbg6nbglTkKb7eRXu1+LYc7qeBHQcrmNlmz469DlXvqCt3H6zD72Pp3DYFn1wK211eAqHPbDbFsehcDghbrgW+FaddZW5J3ZKblm+FrSxt+iJxVprVcHsbToOEnCw27OGPmfq8aUFajmMcPiCPSDetIyPA87mv2E3knn5Um2yDzxfm/C7lh06bXDXMijMoLzveby9cjcXv7JS5+i36aqPH3YFxPT3yrv+84JELn9tFbuyrVlUVZmuHIb6LY5Uy9+8/TvvRoC1yUuFXb/wwepUcksq+WTtIVJpU5Z6d1utsF1VHl+WxriqXC7tF/cPZtFeP16sOpu/7RtNdlG5127JmUXMfn4Zu+iAOrBDf7YOPz2bboBnF23n4/1aYO6P20wrKebjjCiyi8q5ZJQefK8e1xV/pzC8S2viY9s1rr17l/E68FqXu6qq3MomstY9V8oqxsrQ8YeCDIu0i7QAACAASURBVHjvYrd7ycYefPqcARe/p+M7A8/33qcu4XC5vHt1gSUclsXhH+J2s3QYrAdxT9fFoVwegy6C8b/XcYL6GH6V/rlpGZz1b++ZtgiMvgF+tw7G/0FPkGq/p6YQFgMxHr79LuO0lZT4Kfx7kB5se013X0P3qbrori6XTepqPQEYdcPBaa2eBFnvq+SAu4YjyBKOkiYIB7jjegMvOFicI616lfaDrOSEofq+W/mSThZIXXXowLhNeDv9eXtOVpsJIxy+UJYH4qAooht358/l/7IncfZLq9mYZt1AAy8AlB4w6mLTfKr8Qjnls0D+9NlmVu48wAtLalU295oBu5dDaR45xRX88Nt+ql2K5+39PNcgrk840lZrN1BJtlWheggW/QX19rl8v1YPHD/8tv/ghWMK98Fb5+g1kG1sV5UdZ6ksc1d9+yIcdtW4XxCLtmbxSsDlbKruxBu/7KrZ5fP1ezn7+WVszyxiTVEbCvduhcIMVFh7Evbk1btgT0lFFe+t2kPP/sN19o/VDj2/VT86tQnmFKtFSPvIIF68dDh/nT2g4WuujV+AdjnWlRW3/l3dPHG19ZntWKyzsE57BC79EK75Rrvl/ne+d7qlnT4bGq0Huql/rGMwiQMEcj0yq3J3WskEQ9zbwtppv3t+qj7GPk+HwVrkPV83K0mLS0StCmWHQ7uBas+Ea9OmmxaM+vp3hbTRFsywgyvXm40pD+qMrC7jIay9ds3ZtO3hbm9jU5qnW868PktbtQ11cXD6aaEo9bA4giL1RMYzxtEY4eg5Q7ur6oqrRfXU3QJOucf9/5twh7asFj6o/7ddxjX8Gi2EEQ5fKM2FoEh25+i0w9un9sDpEOa+vIIfk7K0DzR2mA4O1qayDLYs4EfnaIJCw5h/01ium9CVhYn72F/gHgArup+m/dc7FvPZur1UVitO6RXNp+v2svtAsdtFFN1X+4lrBft+yyjgq1+36BnkyGt1gNSaEe8+UMwfP9tEeVW1+4DUVUh1OcNKlnFa33bsLyhn417vc2at/QJQ/LJxGze9vYanv09C2QJmz5A9+/f4EpS3hKPKEcCy5GzOGNiBmf3b89aKXeSXVPKXLxL5/fvr6R8bweK7JtMqri8RlVnsStrIlqJQzv/PCsY8tojZzy/j4zpWkft03V4Kyqq4anw37aYo2gcOPx6/5SI+uXm8O16A7jzaL7buDKoG6TlDZxzZa0Tb2FbIkv/Tn9HKF7W/fcB5entknPbTVxTBl3e6j7MHH7uivi78ArXbwdPiqFnEqg6LIy/Vu2WFbVV5Tjyyt+lBypfYzrFOTF+4/BO4a6t3ZpydNuvZhubduXrmPvhCuGWFHqQbIqSNt8URbMU4SnLcFmZjhKPTSHgg1V1h7klwa7h3J/Q7272t5zR4MB3u3wP3p+oi2KPECXC3HAFK8yC4NbsP6Jn1zAHt+fy28XSLCuO6N1fzzaYMbXVkbICsJJRSblFI/h7K83mzcCTXTujKyPg2XDamC9VK8e6vegB4bdlOhszLozKgFWz/jvlr0rguegsvtf8CP4do68QesG3/cMZG7ab49gHUjqXc//FG5i/4XD/Xc5rODrHy9t/5dQ//W7mH5daqbRRl1tQDzPFfwd/PHYDTIXyXqOMIyZmFnP/Sctb88AEABXkH2LqvgH8v2s72VCv7y5692ZaHOOq2OMry4fPbPCwULb4786spqahmap8YbprUnYKyKqY9/SOv/7KLq8fH8971Y2gfGcSUcTpls2PJVvL9onjqgsHcO7M3FdWKu+ZvYKF1zaBbgr/xyy4GdIxgeJfW7tlydF9CQ8OIDg+k2fAsVrSpLNVNGrtN0YHaj6/T/vZR1+tB36b9AB3wzPzNvc0X4YCDU3IzNuikC7svEmiLo7pCt7rwFI52/XUWlKdwZCU17PI43qhtqdUkWVjJJ5Wl2jKfcIdeM9yXFG/QcY6SHA9xsFxVKPf93RjhqOtaG3ouINSqSm/ihKeZaFHhEJGZIrJNRJJF5P46nu8iIotEZKOILBWROI/nvhWRPBH5stYxXUXkV+ucH4hIQEu+B0DPMIJaseuAjjd0aRtKVFgg790whkFxrbj13bW8kjsEJQ7K1r7PdW8mMPrvi/h+y37YNJ8iv9ascQ6qKQrr0jaUSb2ieXfVHn5MyuJvX22hvFr4rmIg5VsX0nr/ch4o+gehCS9y8chOfLJ2L6/9oGsR1jl16wkyNujWyStfpOTL+9iQlscQSUIh2vrpOV0HTQvSa4rdFlu/7bz75WoAo0kkRvIY3bUN323ZT2lFNTf9by2pWXlMDdB++Fk9Qlhy92Sm92tHyl5roLbTEm2LIzIOV0kOd364nk/XeVgCSd/pbBA7DdSyODbtqyDQz8G47lEM7tSKCT2iyCup5PHzB/HIWf3xswrkHFF6tugv1YwbMoDzhsfpVtS3jGNwXCR3fbiB5Ewdd1m+4wDbM4u4alxXXXNh+5A9Z+PNRbgVF/HsB7RrmXbbjb0NRt2oUy39gnRL9tpExOrsKDsOVZSpByK/Bm7nuoQjpq+3MNm1HJUlbl856BTN6N7uAHl5oXZ9HCql80ShVWd3exvQgqpcjb8vQtrq+752cBzccY7GCsdxSosJh4g4gReAWUA/4GIR6VdrtyeBt5RSg4BHgcc8nnsCuLyOU/8TeFop1QPIBVqud7BNaa62OLJLiAoLrMnRjwz25+1rR3HmoFj+/lMuaxyDyFvxJsu3Z9CxVTCPf/QTaus3fFo5hpkDO3mtY3zF2C5kFZZz7Rur6RYdxue3jucnhhJYnsMb/o/jUNXgquKWcTF0jQolOVUH3h/8sZiykPY6e+qHP0NgBKG5W5kWtI2hksyB0O56NmIVquWs/5LtmUX4O4XFWzP1Ajepq6gWP/5acSkOXLD5E6b3a0dyZhG3vLOG5Mwi/ju5nIDqEj2bLctHRHji/MG08bOC2Kqap75YxS+bdKpfaXg85YUH+GTtXu74YAP3fbSRsspqqvboQsEvEpKY+PhiHpqv+/Os21fGuO5ta9phv3DpMBbdNcmriR3gznsHrxTOIH8nL102nEA/B9e+uZp7P9rAnz7bTNvQAM4cZO0XN1LHOew6hOam5wwdpLR7QCUt1PGC+Akw+T6dqTPsCvfg4klER539ZlsaxVkNWxugB8GCNF1QqpQWgdgh3vt4nqd2d9UOg90Wh11/0W5gw697POP015mPdg8ue4nddo2MbwW3sSyOPG25BYS5P2v7/+gZOD+BaUmLYxSQrJRKUUpVAO8DtfPd+gGLrcdLPJ9XSi0CvKruRPdTngrYuXVvAuc0/6XXojQPgrXFEV+r6jckwI9nLx7KG1ePZL7zdNqTzbfT85l31UjOrvwGcVXwesWpXDLae0Cc1CuGzm1CCPZ38srlwxnQMZIzz72cKuWgxK8VMuVBAGKcxXx/5yQeO70LANHRMawojtNuqOIs0k5/k2wVwQOtFzPcuYNEsWaPVqOzwkTtSrl6fFf25pWStL8IlbaK7Y5uVEb3R3UYDJs+ZJrVR2jJtixuPKUbA4t/1bPl+Ak18ZXIEH8GtHXfMt8nJOq4CvB5ajDBlPPBtUO5bUoPPlyTSr+HvyVxla4bWL5lD53bhJC0V1s9aYWKqX3di9NEBvu7ewZ5EhjmTsGtVfwX2yqYFy8dRlW14uft2fg5hXtm9CbI30pLDW4Fdye5K56bm14zAAVr5lntzxfq4jf/ID3jvC1BN+urC7vPkV05X5zlbhdSH63j9Ww5c4sOkJbmHjxzDnN/rgcLxxAd/yjcp+MvER112uyJTpvublfV/kSdQFK7q2xDhLRxB8eDIrUryc7yK/awOMShReUEpiXX4+gIeLZ3TANG19pnAzAH+DdwLhAuIm2VUge38dS0BfKUUnbpbJr1OgchIjcANwB07ty5SW+ghtJcCGrF7gMljO9Rdzro5N4xTLj3LtSLH9Al6Q0YdwGdgxazqGwofjG9GFZrzQWnQ3jrmlFUK0W3aH2TTRzci+0lb9GuS18o3G69dg7QtWbw/sclE/ji5YVMcSWwLvosnlkTxkg1jdtydUbX4uJ4JroUDodA+4E4UrbSpW0I14zvyis/pbBkSzo90tayvHwyN53eHSmfC989RMeqNIZ3aU2VS3HXtF7w4re6pURotFdQMYRS/aWoKOKb6/uTsSED1kBZeBcohNHtHYzu2ZtxPdqyYlsaA1bvBgUPntaJ8Clj2LO2EBZApQQwtY8PAyXo4GZhRp2+6NHd2vLL/VMPfWxdTRCbi9ihOvd/8d90cDR/j3fAsr4cezu3viBd148UZeqq6IboNVOv9fD9I24XWIdaFoenANVlcYB2H+76GU77S/1pqCcKbXvotFuXS3cSju7d+BU4Q9pq91/hPnfacY3F4SEcgREnRrJBPRztd3c3MElE1gGTgL1Adf2H+IZS6hWl1Ail1IjoaB9cAIfC5YIyHbjeV1B2kMXhiZ+fHzLmZt3z6Jt7Ca7MZV+/q/nTmf1qFp/xJD4qlO7R3jOTnmPPIiK2hzaLwR1wLisAhNjoaCbPvpp1/sO4OeMMfkzKomjgVeDUvvFlZV1JsWo/qsI7ElGxnym9Y2gfGUT/2AhSEn/FWV3G7uB+zB4Sq/O8HX6QMI//XTuaD24YQ0B+ik7z7DVDu708U4HLC2tad0vJAWIDSsEvmKtmjPW63nHdo7irfylOpf+d4aJdXJ0j9OfwjwtH+bb6HbjdVS20KE2TEYFzX9aJCEstL2uvGb4dW5fFEeqDkIZG6dTTlCW6h5E4D06FDW6tXYyer2PTfiAg8OMT2q12siwm1ra7LjwtzHB3Em4sdj1M7k63KyqkDSDeMY4TPL4BLSscewFP/0ycta0GpVS6UmqOUmoo8JC1rb6czgNAKxGxLaWDztnsVBSCcpGrtGB0iQqtf//BF+sbZ+2b0G4Al150hdeqZz5T0xvHEo7yAl3I43DQe/BYhj60hGV/mcuiuyZxx7njYcglVIW2J0V1YO1ufczuqjZESjGndtML3UztE0PAPl3bMWTcdN2hNby9bmew9i2CXUXazZPwOmC1Zg6K1K9tB3E9hEMHCnOtCm/LovLMrLLbRovDXThotXmOi/Yh/dEmdqjOdz/WhAO0RXPxe7otR6cxvl9jaJQW+4K9updTWZ5vrirQLc3tKvDoPgdbVSLaXRXWzjtoDtr1F9VTV74PudS3NNQTAc8V+IozmygcVvV4TopbHBxOff/XxDiMcBwuq4GeVhZUAHARsMBzBxGJEhH7Gh4A5tV3QqWUQsdC7PLTK4HPm/Wqa2NlUGRW6sG3PosD0F/M4Vfpx2Nurj/drj5qLA47za9Am8Ae+DkddI8O0y3AZz2B4+blRIYEssYSjrV5WuRGtdZpxFP6xDDMsZ1sWjFr/Ej3iTzbGWRtc7ePaNXZek2lxUMpb+EoztbBwmAP4fBswJi2Wu8b3NrdR6imALARLqRhV8DvN7j7WR1rhLTRxWeX1dPWojYiWmQK0j1ScRuuigd0Mdosa4GrDoPq3ie8nXdGlScdBgPScFX4iYRdy2H3rIrxwS1YG1s4qsq8K+RDorxdVSeBcLRYjEMpVSUitwELAScwTymVKCKPAglKqQXAZOAxEVHAT8Ct9vEi8jPQBwgTkTTgWqXUQuA+4H0R+T9gHfAaLYmVJbG3XAtHlzY+DF4T7tC+z4GHEZS1bz57IC4vqD932y8AR1hbhnVuzZo9uSzZmsn8ZLjADwKLM4CBDI5rRYeAFMqjRxAV4PGvjx0Kncfp7pvJ3+sB+tRHvK+jvEC7tFB6ZuwXbFkcORDS+mCLQyndzqHrRJ15VFs4/L2X+6wXh9N7TedjEad/42MFER1rCYePFgfoTLFzrI6udTHtUW3p1cXEu3RGmD0LPxkIj9X37HZrbY6mWBz2ZA68s6Y8Gx2W5XtnAp6gtGRwHKXU18DXtbY97PH4I9wZUrWPrbMTmlIqBZ2xdWSwBsI9JQG0DvEnMsSHwSG4tV6h7nBw+ulBu9SjsCiw4aKf4V1as3hrJje+vYYxMV10wrLV7M5ZUUiH6nToV0cG89hb4IPLdIB31uPu2a8tHGX5br95YLjVo+eAFrZ2/d2uNVs48tN01XbcKO1TrnFVWYv++DVCOE5UImKtdF4fi/9qM+SSQz8XP+HQz8X09S4YPBlwOLRQ7t+sLQRf3YKehHhMXjwtjrbdYfMn+t4uy2+4M+4JwNEOjh/7WK6qHUX+dGl7hF0ldt44NGxxWIzoomf+PduF8ex1M/Vgb/cmyrYytepa/KX36XqmFNPfvQIbuF+zrMDdpyowwt1+oTRHPw4I0xaJLRx2fKPTSG3B2MJRZdWBGOHQFkdhhrVIFRB2GEkchoaxLYGmWBvgHQ/ydEcNmKNjoUkLrVTdE184WtTiOCGwBsKteU66djvCwmHnjYMeuOvrvW8xqmsbnr9kKBN7RGvrKCLWQzjsTqh1tJhwOOFqa80Qp8dt4Wlx2O6lgDB3c7fSXC1wIvqLVSMcCdo10G5ALeGwLI6WTJM9XojoqFuD2K1HGuOqMjQeO87RVOGwvQBl+d7iED9RJyJseA8qi0+KGIexOBrCinEkFTjp0lBgvLkJbuMeiH20OESEMwfFul1qkR5rF2dt08LQ+hCFT+HtD44l2O6xck+Lw3JV5aToYjR7JuYpHOnrdOqn098SDivGUVmmfe8OM2epycDK2KBF9lgN/p8o2MLRlMC4je2u8nRHOZzulQbBCIcBKM1DOQIoUYHEH2lXVYjlqrIXcfIhxnEQkXHuBX2yk7Q/1tmIQdueWZXlewtHSFt3Xx47vmELh8ulmzDarTACwz0sDmv1v6Zmm51IeApHWLT5TFqazmP0Kor1xX8awg6Q13ZH2UsrgBEOA1CaS0VABCB1t8RoSWyLw17EqSk3ZGScztyprtIWR11uqvqoiXHku9cbt4XD8zrBLRw5O7TJblc0B4S6j60qa1xG1YmMXdVdXtD4wLih8bTtDn/Y2PhWI57Y933t72LsUN3WpK7nTkCMcDREWR6V/nrwDA1sZIuCwyW4tR5U7BbmTWmlHBmn1/nI260rXhvbQtvpryuM67I4bLwsjjy9yhy421vUdlWZwLgmJMqdqWbiG8cHdbmqQFuLdk+0k0A4jKO5IUpzqfDXN4Kf4wi7EmpaHFgrvgU2xeKwisBSlup4RGMtDnAHBO3FpOwYh01tiyNjPTgD3a8VEKYrlasrrfXGjXAAOkU0ooNuk24yqo4PQg7hqgLdO6w4q3HLEB+nGIujIUrzqLAsDueRblxmB51zd+nfTbU4QC9hCro1RmMJjNCiUVGk22T4BR7a4qgohL1rdOaKXRBnB30rinU6rsmocmMv2WpcVccHHYZol1RdVkVYDJzx1ElxfxvhaIiyPMr99IB99CyOXfp3U4PjACk/AlL3MpUNUWNxFLrbRdvtpBH3l8gWurTV3mtE2MdUFOsiKWNxuLED5MZVdXww6AL43drGd9Y9wTDC0RCleZTXWBxHWDiCawlHUyyOwDC3JdCqc9NmQ3aH3PJC7aYC78py+0tkC4erynuNiBqLo8jKqjLCUUONcPjYp8pgOAYwwlEf1VVQXnD0LY48O8bRxHWGbaujKfEN8LY47GsIaqXrMexrBO/KWi/hsC2OIpNVVRu77XlTWmAYDEcJIxz1YdUplPnpWfaRtziaIcYB7gB5U+IbUEs4LBFwOKyuuHUIh8PPu8jKM8Zhsqq86TBYJxLYxWkGw3GAyaqqD6tqvNRpWxxHWGcDwnS6ZtF+QPSaFE3hcC0OOzheXuC9LGlYjHdQ1xaOmL7e60B4BcdNjMOLLmPhgdSD180wGI5hjHDUh9XgsNSyOI74apAi2hVUtL9mEacmYQtHY2s4bIIidU+l4gPeM+Ozn/NeW9kWjtpLmdpxEdviMK4qb4xoGI4zjHDUh9V3qeRoWRygXUFF+5se3wDoMQ12rzj0oj8NYbvICjPcIgAQN6LWfpG69cKgC7232xZHeaG75YjBYDhuMcJRH7aryhEOVBz5GAd4FBwdhnC06weXvN/04+1iJ1XtbWHURgTO++/B271cVWVmhm0wHOeY4Hh9WBZHsUMPlkc8qwrc7p+j2cbA87WbYvn4e6TjVpaeFAVSBsOJjBGO+rBiHCXOcETAcTSF43BcVYeL52sHNiFA7/TT7qnSPECZ4LjBcJxjhKM+yvLAP4QK/I6OtQHN46o6XLwsjnpcVfUREAol1rrMRjgMhuOaFhUOEZkpIttEJFlE7q/j+S4iskhENorIUhGJ83juShHZbv1c6bF9qXXO9dZPy1VOleZCcGuqXOroxDfAXSdxNC2OoMO0OEALh722tsmqMhiOa1osOC4iTuAFYBqQBqwWkQVKqS0euz0JvKWUelNEpgKPAZeLSBvgEWAEenWUNdax1vJyXKqUSmipa6/BWj+4ulodnYwqOAYtjqYKR5i7PbzJqjIYjmtacjQcBSQrpVKUUhXA+8DsWvv0A6y2rSzxeH4G8L1SKscSi++BmS14rXUTPx76zabKpThaBscxYXH4h7iXem1qEWJAqK4DAWNxGAzHOS0pHB2BVI+/06xtnmwA5liPzwXCRaStD8e+brmp/iRS93qbInKDiCSISEJWVlbT3sHYW2HyfVS7FH7Ok9jiEHELV1MtjsAwE+MwGE4QjnZw/G5gkoisAyYBe4HqBo65VCk1EJho/Vxe105KqVeUUiOUUiOiow9vrYOjGuMI76B/h7U/Oq9vY7urDifGUV2hHxvhMBiOa1pSOPYCnTz+jrO21aCUSldKzVFKDQUesrbl1XesUsr+XQi8i3aJtSjVLtfRy6pq0xWuXwK9Zx2d17cJOkyLw7Nw0NRxGAzHNS0pHKuBniLSVUQCgIuABZ47iEiUiNjX8AAwz3q8EJguIq1FpDUwHVgoIn4iEmUd6w+cCWxuwfcAQLXrKHTG9aTjsKO/cIxtcdRXOV4fdvU4mMpxg+E4p8WEQylVBdyGFoHfgA+VUoki8qiInG3tNhnYJiJJQDvgb9axOcBf0eKzGnjU2haIFpCNwHq0FfJqS70Hm6NqcRwrBEXqILmziYl4XsJhLA6D4XimRXtVKaW+Br6ute1hj8cfAR8d4th5uC0Qe1sxMLz5r7R+jmqM41ghNNp7nfHG4pmNZbKqDIbjGtPk0AeqXUexjuNY4ZR7YfjVTT/ey+IwwmEwHM8Y4fCBKpc6On2qjiUiOuifpmKEw2A4YTjJp9G+oS2Ok1w4DhdP4TBZVQbDcY0RDh8wMY5mwDMby2myqgyG4xkjHD5gsqqaAburrjPwKKzBazAYmhPzDfaBqmpjcRw2tqvKZFQZDMc9Rjh8wKUUfk4jHIeF7aoyNRwGw3GPEQ4f0DEO81EdFrbFYarGDYbjHjMa+oDJqmoGalxVxuIwGI53jHD4QFW1wlF393aDr9S4qkyMw2A43jHC4QPG4mgGHE4d3zDCYTAc9xjh8IEqlwunCY4fPgGhJqvKYDgBMMLhA8biaCYCQk1WlcFwAmB6VfmAqRxvJrqeAm26He2rMBgMh4kRDh9wGYujeZj9/NG+AoPB0AwYV5UPmDoOg8FgcNPgaCgiZ3ks73pSYmIcBoPB4MYXQbgQ2C4ij4tIn5a+oGMRE+MwGAwGNw0Kh1LqMmAosAN4Q0RWiMgNIhLewKEnDNVGOAwGg6EGn1xQSqkC9Nrg7wMdgHOBtSJye33HichMEdkmIskicn8dz3cRkUUislFElopInMdzV4rIduvnSo/tw0Vkk3XOZ0VavqS7yrRVNxgMhhp8iXGcLSKfAksBf2CUUmoWMBi4q57jnMALwCygH3CxiPSrtduTwFtKqUHAo8Bj1rFtgEeA0cAo4BERaW0d8xJwPdDT+pnp0zs9DIzFYTAYDG58sTjOA55WSg1USj2hlMoEUEqVANfWc9woIFkplaKUqkBbK7Nr7dMPWGw9XuLx/Azge6VUjlIqF/gemCkiHYAIpdRKpZQC3gLO8eE9HBZVJjhuMBgMNfgiHH8GVtl/iEiwiMQDKKUW1XNcRyDV4+80a5snG4A51uNzgXARaVvPsR2tx/Wd077OG0QkQUQSsrKy6rnM+nG5FEph0nENBoPBwpfRcD7g8vi72trWHNwNTBKRdcAkYK91/sNGKfWKUmqEUmpEdHR0k89TrRSAWcjJYDAYLHypHPezXE0AKKUqRCTAh+P2Ap08/o6zttWglErHsjhEJAw4TymVJyJ7gcm1jl1qHR9Xa7vXOZubapcWDhPjMBgMBo0vFkeWiJxt/yEis4FsH45bDfQUka6W0FwELPDcQUSiPIoLHwDmWY8XAtNFpLUVFJ8OLFRKZQAFIjLGyqa6Avjch2tpMlW2cJj1OAwGgwHwzeK4CXhHRJ4HBB17uKKhg5RSVSJyG1oEnMA8pVSiiDwKJCilFqCtisdERAE/Abdax+aIyF/R4gPwqFIqx3p8C/AGEAx8Y/20GNXVxuIwGAwGTxoUDqXUDmCM5UpCKVXk68mVUl8DX9fa9rDH44/Q9SF1HTsPtwXiuT0BGODrNRwuVS4d3jExDoPBYND41B1XRM4A+gNBdr2dUurRFryuYwYT4zAYDAZvfCkA/A+6X9XtaFfVBUCXFr6uYwY7xmHqOAwGg0HjS3B8nFLqCiBXKfUXYCzQq2Uv69jBbXGYOg6DwWAA34SjzPpdIiKxQCW6X9VJQbWxOAwGg8ELX2IcX4hIK+AJYC2ggFdb9KqOIapMjMNgMBi8qFc4rBqLRUqpPOBjEfkSCFJK5R+RqzsGMMFxg8Fg8KZeV5VSyoXucGv/XX4yiQa403GNcBgMBoPGlxjHIhE570ise3EsYmIcBoPB4I0vwnEjuqlhuYgUiEihiBS08HUdM5gYh8FgMHjjS+X4SbNEbF24LQ6TjmswGAzgg3CIyCl1bVdK/dT8l3PsUWV6nW+8owAAGvNJREFUVRkMBoMXvqTj3uPxOAi9st8aYGqLXNExhsusx2EwGAxe+OKqOsvzbxHpBDzTYld0jGFiHAaDweBNUxz3aUDf5r6QY5VqOx335EwqMxgMhoPwJcbxHLpaHLTQDEFXkJ8UmBiHwWAweONLjCPB43EV8J5S6pcWup5jjpqsKhPjMBgMBsA34fgIKFNKVQOIiFNEQpRSJS17accGpq26wWAweONT5Th6mVabYOCHlrmcYw/TVt1gMBi88WU0DPJcLtZ6HNJyl3RsYSwOg8Fg8MYX4SgWkWH2HyIyHCj15eQiMlNEtolIsojcX8fznUVkiYisE5GNInK6tT1ARF4XkU0iskFEJnscs9Q653rrJ8aXa2kqLpOOazAYDF74EuP4AzBfRNLRS8e2Ry8lWy8i4kR31p2GTuFdLSILlFJbPHb7I/ChUuolEekHfA3EA9cDKKUGWsLwjYiMtLr1AlyqlPIM2rcYxuIwGAwGb3wpAFwtIn2A3tambUqpSh/OPQpIVkqlAIjI+8BswFM4FBBhPY4E0q3H/YDF1utnikgeMAJY5cPrNit2HYfDCIfBYDAAPriqRORWIFQptVkptRkIE5FbfDh3RyDV4+80a5snfwYuE5E0tLVxu7V9A3C2iPiJSFdgONDJ47jXLTfVnw7V7l1EbhCRBBFJyMrK8uFy68ZYHAaDweCNLzGO660VAAFQSuViuZKagYuBN5RSccDpwNvWqoPz0EKTgG5vshyoto65VCk1EJho/Vxe14mVUq8opUYopUZER0c3+QLNCoAGg8HgjS/C4fSc1VuxiwAfjtuLt5UQZ23z5FrgQwCl1Ap0E8UopVSVUuoOpdQQpdRsoBWQZO231/pdCLyLdom1GFWmrbrBYDB44cto+C3wgYicKiKnAu8B3/hw3Gqgp4h0FZEA4CJgQa199gCnAohIX7RwZIlIiIiEWtunAVVKqS2W6yrK2u4PnAls9uFamoyxOAwGg8EbX7Kq7gNuAG6y/t6IzqyqF6VUlYjcBiwEnMA8pVSiiDwKJCilFgB3Aa+KyB3oQPlVSillZVItFBEX2kqx3VGB1nZ/65w/AK/6+F6bhN2rysQ4DAaDQeNLVpVLRH4FugNzgSjgY19OrpT6Gh309tz2sMfjLcD4Oo7bhTuLy3N7MTpQfsSodrkQMVlVBoPBYHNI4RCRXujg9cVANvABgFJqypG5tGODaqWMtWEwGAwe1GdxbAV+Bs5USiUDWC6lk4oql8Jh1uIwGAyGGuoLjs8BMoAlIvKqFRg/6UbQ6mpjcRgMBoMnhxQOpdRnSqmLgD7AEnTrkRgReUlEph+pCzzaVLmUyagyGAwGDxpMx1VKFSul3rXWHo8D1qEzrU4Kql0KP6ep4TAYDAabRo2ISqlcqyL71Ja6oGMNY3EYDAaDN2Yq3QDVLpeJcRgMBoMHRjgawFgcBoPB4I0RjgZwuUxWlcFgMHhihKMBqlzKVI0bDAaDB0Y4GqDaWBwGg8HghRGOBtAxDvMxGQwGg40ZERvAWBwGg8HgjRGOBjBZVQaDweCNEY4GMHUcBoPB4I0RjgaoqjYWh8FgMHhihKMBXErh5zTCYTAYDDZGOBrArMdhMBgM3hjhaACTVWUwGAzetKhwiMhMEdkmIskicn8dz3cWkSUisk5ENorI6db2ABF5XUQ2icgGEZnsccxwa3uyiDwr0rLmgI5xGH01GAwGmxYbEUXECbwAzAL6AReLSL9au/0R+FApNRS4CHjR2n49gFJqIDANeEpE7Gt9yXq+p/Uzs6XeAxiLw2AwGGrTklPpUUCyUipFKVUBvA/MrrWPAiKsx5FAuvW4H7AYQCmVCeQBI0SkAxChlFqplFLAW8A5LfgeqHK5cJrguMFgMNTQksLREUj1+DvN2ubJn4HLRCQN+Bq43dq+AThbRPxEpCswHOhkHZ/WwDkBEJEbRCRBRBKysrKa/CaMxWEwGAzeHG3n/cXAG0qpOOB04G3LJTUPLQoJwDPAcqC6MSe2ViocoZQaER0d3eQLNJXjBoPB4I1fC557L9pKsImztnlyLVaMQim1QkSCgCjLPXWHvZOILAeSgFzrPPWds1kxFofBYDB405IWx2qgp4h0FZEAdPB7Qa199gCnAohIXyAIyBKREBEJtbZPA6qUUluUUhlAgYiMsbKprgA+b8H3QLWxOAwGg8GLFrM4lFJVInIbsBBwAvOUUoki8iiQoJRaANwFvCoid6AD5VcppZSIxAALRcSFtigu9zj1LcD/t3fn0VWV5x7Hv48BmZUYHIE21LIKCjIuhhtoweHeoAjWBcbihLdellxQ8Hb1im1lcNkrt1IrdOFcECtCAY1iF1bEizIIlIRJJgGBQgAlUlBQEJI894+9E0+Gc8gJOZxEf5+1sjjn3e/ePHufnP3k3e/e7/si0AB4K/xJGCUOEZHSEnmpCndfQNDpHVk2NuL1ZiCjgvV2Az+Kss0coF21BhpDQZFTR89xiIiU0BnxNNTiEBEpTYnjNAo0rLqISClKHKehFoeISGlKHKdRoNtxRURKUeKIoajIcUeDHIqIRNAZMYZCdwBSdJRERErolBhDYVFx4tBhEhEppjNiDAVh4lAfh4jIN5Q4YigsLG5xKHGIiBRT4oihoKgIgDqaj0NEpIQSRwzf9HEocYiIFFPiiEF9HCIi5SlxxKC7qkREytMZMQa1OEREylPiiKG4xXGOEoeISAkljhgK1eIQESlHiSOG4ttxdVeViMg3lDhiUItDRKQ8JY4YCvQch4hIOQlNHGaWaWYfmdkOMxtTwfLvmdliM1trZhvM7PqwvK6ZzTCzD81si5k9FLHO7rB8nZnlJDL+b1ocyq8iIsXqJGrDZpYCTAWuA/KA1WY23903R1T7DTDH3Z82syuABUA6MBio5+7tzawhsNnMZrn77nC9vu7+WaJiL1agsapERMpJ5J/S3YAd7r7T3U8Cs4GBZeo4cF74+nxgf0R5IzOrAzQATgJfJDDWCpW0ODRWlYhIiUQmjubA3oj3eWFZpPHA7WaWR9DauC8snwd8CRwA9gCT3P2f4TIHFppZrpkNi/afm9kwM8sxs5z8/Pwq7UDxRE7nmBKHiEixhF2qqqSfAS+6++/NrCfwZzNrR9BaKQQuA1KBpWa2yN13Ar3cfZ+ZXQS8Y2Zb3X1J2Q27+3PAcwBdu3b1qgRXWDw6ri5ViVTKqVOnyMvL48SJE8kOReJQv359WrRoQd26dStVP5GJYx/QMuJ9i7As0s+BTAB3X2Fm9YFmwBDgb+5+CjhoZsuBrsBOd98X1j9oZtkESaZc4qgO6uMQiU9eXh5NmjQhPT0dU0u9VnB3Dh06RF5eHq1atarUOom8VLUaaG1mrczsXOBWYH6ZOnuAawDMrC1QH8gPy68OyxsBPYCtZtbIzJpElP8rsDFRO6A+DpH4nDhxgrS0NCWNWsTMSEtLi6uVmLAWh7sXmNlI4G0gBZjm7pvM7BEgx93nA78AnjezBwj6Loa6u5vZVGC6mW0CDJju7hvM7AdAdvhLWQd4xd3/lqh90CCHIvFT0qh94v3MEtrH4e4LCDq9I8vGRrzeDGRUsN4xgltyy5bvBDpUf6QV07DqIiLl6YwYg1ocIrXLkSNHeOqpp6q07vXXX8+RI0di1hk7diyLFi2q0vZjefHFFxk5cmTMOu+99x4ffPBBtf/fVaHEEUOhBjkUqVViJY6CgoKY6y5YsICmTZvGrPPII49w7bXXVjm+M1GTEkeyb8et0QqDvKHEIVIFE97cxOb91fvc7hWXnce4G6+MunzMmDF8/PHHdOzYkeuuu44bbriBhx9+mNTUVLZu3cq2bdu46aab2Lt3LydOnGDUqFEMGxY8Dpaenk5OTg7Hjh2jX79+9OrViw8++IDmzZvzxhtv0KBBA4YOHUr//v0ZNGgQ6enp3HXXXbz55pucOnWKuXPn0qZNG/Lz8xkyZAj79++nZ8+evPPOO+Tm5tKsWbNSsU6fPp3HHnuMpk2b0qFDB+rVqwfAm2++yaOPPsrJkydJS0tj5syZHD9+nGeeeYaUlBRefvll/vjHP3LkyJFy9S6++OJqPd7RqMURg1ocIrXLxIkTufzyy1m3bh2PP/44AGvWrGHy5Mls27YNgGnTppGbm0tOTg5Tpkzh0KFD5bazfft2RowYwaZNm2jatCmvvvpqhf9fs2bNWLNmDcOHD2fSpEkATJgwgauvvppNmzYxaNAg9uzZU269AwcOMG7cOJYvX86yZcvYvPmbkZh69erFypUrWbt2Lbfeeiu/+93vSE9P59577+WBBx5g3bp19O7du8J6Z4taHDGoj0Ok6mK1DM6mbt26lXo+YcqUKWRnZwOwd+9etm/fTlpaWql1WrVqRceOHQHo0qULu3fvrnDbN998c0md1157DYBly5aVbD8zM5PU1NRy661atYo+ffpw4YUXApCVlVWS2PLy8sjKyuLAgQOcPHky6rMVla2XCGpxxFCoYdVFar1GjRqVvH7vvfdYtGgRK1asYP369XTq1KnC5xeKLxsBpKSkRO0fKa4Xq0687rvvPkaOHMmHH37Is88+G/X5isrWSwQljhgKNKy6SK3SpEkTjh49GnX5559/TmpqKg0bNmTr1q2sXLmy2mPIyMhgzpw5ACxcuJDDhw+Xq9O9e3fef/99Dh06VNI/Ehlj8+bBsH4zZswoKS+7b9HqnQ06I8agFodI7ZKWlkZGRgbt2rXjl7/8ZbnlmZmZFBQU0LZtW8aMGUOPHj2qPYZx48axcOFC2rVrx9y5c7nkkkto0qRJqTqXXnop48ePp2fPnmRkZNC2bduSZePHj2fw4MF06dKlVIf6jTfeSHZ2Nh07dmTp0qVR650N5l6l8f9qla5du3pOTvxzPk1etJ0/LNrGzv+5nnOUPEROa8uWLaVOgt9FX3/9NSkpKdSpU4cVK1YwfPhw1q1bl+ywTquiz87Mct29a9m66hyPobCoCDOUNESk0vbs2cMtt9xCUVER5557Ls8//3yyQ6p2ShwxFBQ5KRp3R0Ti0Lp1a9auXZvsMBJKfRwxFLqrf0NEpAwljhgKC13PcIiIlKHEEUNBkVocIiJlKXHEUFjk1EnRIRIRiaSzYgxqcYh8+zVu3BiA/fv3M2jQoArr9OnTh9Pd0v/kk0/y1VdflbyvzDDtVVEcbzRnMrR8ZSlxxFBYVKQ+DpHviMsuu4x58+ZVef2yiaMyw7QnwtlIHLodNwa1OETOwFtj4JMPq3ebl7SHfhOjLh4zZgwtW7ZkxIgRQPAUduPGjbn33nsZOHAghw8f5tSpUzz66KMMHDiw1Lq7d++mf//+bNy4kePHj3P33Xezfv162rRpw/Hjx0vqDR8+nNWrV3P8+HEGDRrEhAkTmDJlCvv376dv3740a9aMxYsXlwzT3qxZM5544gmmTZsGwD333MPo0aPZvXt31OHbI+3atYshQ4Zw7NixUjEXvy+7T2WHlh83btxp9z1eShwxFCpxiNQqWVlZjB49uiRxzJkzh7fffpv69euTnZ3Neeedx2effUaPHj0YMGBA1Lm2n376aRo2bMiWLVvYsGEDnTt3Lln229/+lgsuuIDCwkKuueYaNmzYwP33388TTzzB4sWLyw3/kZuby/Tp01m1ahXuTvfu3fnJT35Camoq27dvZ9asWTz//PPccsstvPrqq9x+++2l1h81ahTDhw/nzjvvZOrUqSXl0fZp4sSJbNy4seRp9YKCgrj2vTISmjjMLBOYDKQAL7j7xDLLvwfMAJqGdca4+wIzqwu8AHQOY3zJ3R+rzDarkxKHyBmI0TJIlE6dOnHw4EH2799Pfn4+qamptGzZklOnTvGrX/2KJUuWcM4557Bv3z4+/fRTLrnkkgq3s2TJEu6//34ArrrqKq666qqSZXPmzOG5556joKCAAwcOsHnz5lLLy1q2bBk//elPS0bpvfnmm1m6dCkDBgyo1PDty5cvL5kP5I477uDBBx8EwN0r3KeyotWLtu+VkbDEYWYpwFTgOiAPWG1m8919c0S13wBz3P1pM7sCWACkA4OBeu7e3swaApvNbBawtxLbrDaFRXqOQ6S2GTx4MPPmzeOTTz4hKysLgJkzZ5Kfn09ubi5169YlPT29SsOQ79q1i0mTJrF69WpSU1MZOnToGQ1nXnb49shLYpEqah1Udp+qa98jJbJzvBuww913uvtJYDZQ9sKaA+eFr88H9keUNzKzOkAD4CTwRSW3WW2CPg7dPyBSm2RlZTF79mzmzZvH4MGDgWAI8osuuoi6deuyePFi/vGPf8Tcxo9//GNeeeUVADZu3MiGDRsA+OKLL2jUqBHnn38+n376KW+99VbJOtGGdO/duzevv/46X331FV9++SXZ2dn07t270vuTkZHB7NmzgSAJFIu2TxUNvx7PvldGIs+KzQlaCMXywrJI44HbzSyPoLVxX1g+D/gSOADsASa5+z8ruU0AzGyYmeWYWU5+fn6VdkAtDpHa58orr+To0aM0b96cSy+9FIDbbruNnJwc2rdvz0svvUSbNm1ibmP48OEcO3aMtm3bMnbsWLp06QJAhw4d6NSpE23atGHIkCFkZGSUrDNs2DAyMzPp27dvqW117tyZoUOH0q1bN7p3784999xDp06dKr0/kydPZurUqbRv3559+/aVlEfbp7JDy8e775WRsGHVzWwQkOnu94Tv7wC6u/vIiDr/FcbwezPrCfwJaAf0BP4TGAqkAkuBfgR9HjG3WZGqDqs+dfEOjn1dwIOZZ36gRb4LNKx67VVThlXfB7SMeN8iLIv0cyATwN1XmFl9oBkwBPibu58CDprZcqArQWvjdNusNiP6/jBRmxYRqbUSealqNdDazFqZ2bnArcD8MnX2ANcAmFlboD6QH5ZfHZY3AnoAWyu5TRERSaCEJQ53LwBGAm8DWwjuntpkZo+Y2YCw2i+A/zCz9cAsYKgH186mAo3NbBNBspju7huibTNR+yAi8fsuzCr6bRPvZ6apY0Wk2uzatYsmTZqQlpZ2Rg+Yydnj7hw6dIijR4/SqlWrUss0dayIJFyLFi3Iy8ujqncySnLUr1+fFi1aVLq+EoeIVJu6deuW+6tVvn30dJuIiMRFiUNEROKixCEiInH5TtxVZWb5QFUHaGkGfFaN4Zxtij+5FH9yKf4z8313v7Bs4XcicZwJM8up6Ha02kLxJ5fiTy7Fnxi6VCUiInFR4hARkbgocZzec8kO4Awp/uRS/Mml+BNAfRwiIhIXtThERCQuShwiIhIXJY4YzCzTzD4ysx1mNibZ8cRiZi3NbLGZbTazTWY2Kiy/wMzeMbPt4b+pyY41FjNLMbO1ZvbX8H0rM1sVfgZ/CedhqZHMrKmZzTOzrWa2xcx61qbjb2YPhL87G81slpnVr8nH38ymmdlBM9sYUVbh8bbAlHA/NphZ5+RFXhJrRfE/Hv7+bDCzbDNrGrHsoTD+j8zs35ITdUCJIwozSyGYF6QfcAXwMzO7IrlRxVQA/MLdryCY+GpEGO8Y4F13bw28G76vyUYRzLVS7H+BP7j7D4HDBLNG1lSTCWaubAN0INiPWnH8zaw5cD/Q1d3bASkEE6XV5OP/IuEMohGiHe9+QOvwZxjw9FmKMZYXKR//O0A7d78K2AY8BBB+l28FrgzXeSo8RyWFEkd03YAd7r7T3U8Cs4GBSY4pKnc/4O5rwtdHCU5azQlinhFWmwHclJwIT8/MWgA3AC+E741gJsh5YZUaG7+ZnQ/8GPgTgLufdPcj1KLjTzBadgMzqwM0BA5Qg4+/uy8B/lmmONrxHgi85IGVQFMzu/TsRFqxiuJ394XhhHUAKwmmx4Yg/tnu/rW77wJ2EJyjkkKJI7rmBHOcF8sLy2o8M0sHOgGrgIvd/UC46BPg4iSFVRlPAv8NFIXv04AjEV+kmvwZtCKY9nh6eKnthXDa41px/N19HzCJYNrmA8DnQC615/gXi3a8a+P3+d+Bt8LXNSp+JY5vGTNrDLwKjHb3LyKXhdPy1sj7r82sP3DQ3XOTHUsV1QE6A0+7eyfgS8pclqrhxz+V4K/aVsBlQCPKX0apVWry8T4dM/s1weXnmcmOpSJKHNHtA1pGvG8RltVYZlaXIGnMdPfXwuJPi5vk4b8HkxXfaWQAA8xsN8FlwasJ+gyahpdOoGZ/BnlAnruvCt/PI0gkteX4Xwvscvd8dz8FvEbwmdSW418s2vGuNd9nMxsK9Adu828etKtR8StxRLcaaB3eVXIuQcfU/CTHFFXYH/AnYIu7PxGxaD5wV/j6LuCNsx1bZbj7Q+7ewt3TCY71/7n7bcBiYFBYrSbH/wmw18x+FBZdA2ymlhx/gktUPcysYfi7VBx/rTj+EaId7/nAneHdVT2AzyMuadUYZpZJcLl2gLt/FbFoPnCrmdUzs1YEnfx/T0aMQDBRuX4q/gGuJ7iz4WPg18mO5zSx9iJolm8A1oU/1xP0E7wLbAcWARckO9ZK7Esf4K/h6x8QfEF2AHOBesmOL0bcHYGc8DN4HUitTccfmABsBTYCfwbq1eTjD8wi6I85RdDi+3m04w0YwV2SHwMfEtw9VhPj30HQl1H8HX4mov6vw/g/AvolM3YNOSIiInHRpSoREYmLEoeIiMRFiUNEROKixCEiInFR4hARkbgocYjUcGbWp3i0YJGaQIlDRETiosQhUk3M7HYz+7uZrTOzZ8O5RY6Z2R/CeS7eNbMLw7odzWxlxLwLxfNG/NDMFpnZejNbY2aXh5tvHDHXx8zw6W6RpFDiEKkGZtYWyAIy3L0jUAjcRjBYYI67Xwm8D4wLV3kJeNCDeRc+jCifCUx19w7AvxA8WQzBaMejCeaG+QHBOFIiSVHn9FVEpBKuAboAq8PGQAOCAfaKgL+EdV4GXgvn7mjq7u+H5TOAuWbWBGju7tkA7n4CINze3909L3y/DkgHliV+t0TKU+IQqR4GzHD3h0oVmj1cpl5Vx/j5OuJ1IfruShLpUpVI9XgXGGRmF0HJ3NffJ/iOFY8uOwRY5u6fA4fNrHdYfgfwvgczN+aZ2U3hNuqZWcOzuhcilaC/WkSqgbtvNrPfAAvN7ByCEU9HEEzo1C1cdpCgHwSCIb+fCRPDTuDusPwO4FkzeyTcxuCzuBsilaLRcUUSyMyOuXvjZMchUp10qUpEROKiFoeIiMRFLQ4REYmLEoeIiMRFiUNEROKixCEiInFR4hARkbj8PyMYvDRkAqF0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eHn8XVRGcG9F",
        "outputId": "3dffe4a9-85ce-43b7-feec-25793ac734fc"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim = 20,activation='elu'))\n",
        "model.add(Dense(4,activation='elu'))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3576 - accuracy: 0.8772 - val_loss: 0.2707 - val_accuracy: 0.8973\n",
            "Epoch 2/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2679 - accuracy: 0.8979 - val_loss: 0.2282 - val_accuracy: 0.9096\n",
            "Epoch 3/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2283 - accuracy: 0.9070 - val_loss: 0.2124 - val_accuracy: 0.9133\n",
            "Epoch 4/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2167 - accuracy: 0.9077 - val_loss: 0.2078 - val_accuracy: 0.9136\n",
            "Epoch 5/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2112 - accuracy: 0.9099 - val_loss: 0.2055 - val_accuracy: 0.9131\n",
            "Epoch 6/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2072 - accuracy: 0.9114 - val_loss: 0.2067 - val_accuracy: 0.9137\n",
            "Epoch 7/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2033 - accuracy: 0.9131 - val_loss: 0.2036 - val_accuracy: 0.9134\n",
            "Epoch 8/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2017 - accuracy: 0.9126 - val_loss: 0.1997 - val_accuracy: 0.9139\n",
            "Epoch 9/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2070 - accuracy: 0.9099 - val_loss: 0.2005 - val_accuracy: 0.9122\n",
            "Epoch 10/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2034 - accuracy: 0.9114 - val_loss: 0.1969 - val_accuracy: 0.9139\n",
            "Epoch 11/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1983 - accuracy: 0.9122 - val_loss: 0.1958 - val_accuracy: 0.9136\n",
            "Epoch 12/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1961 - accuracy: 0.9113 - val_loss: 0.1991 - val_accuracy: 0.9144\n",
            "Epoch 13/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2043 - accuracy: 0.9094 - val_loss: 0.2050 - val_accuracy: 0.9114\n",
            "Epoch 14/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1971 - accuracy: 0.9120 - val_loss: 0.1935 - val_accuracy: 0.9131\n",
            "Epoch 15/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1938 - accuracy: 0.9142 - val_loss: 0.2002 - val_accuracy: 0.9144\n",
            "Epoch 16/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9148 - val_loss: 0.1928 - val_accuracy: 0.9118\n",
            "Epoch 17/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1960 - accuracy: 0.9110 - val_loss: 0.1923 - val_accuracy: 0.9135\n",
            "Epoch 18/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1950 - accuracy: 0.9116 - val_loss: 0.1922 - val_accuracy: 0.9156\n",
            "Epoch 19/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9153 - val_loss: 0.1945 - val_accuracy: 0.9153\n",
            "Epoch 20/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1983 - accuracy: 0.9075 - val_loss: 0.1937 - val_accuracy: 0.9150\n",
            "Epoch 21/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1950 - accuracy: 0.9122 - val_loss: 0.1922 - val_accuracy: 0.9160\n",
            "Epoch 22/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1973 - accuracy: 0.9126 - val_loss: 0.1923 - val_accuracy: 0.9148\n",
            "Epoch 23/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1974 - accuracy: 0.9098 - val_loss: 0.1910 - val_accuracy: 0.9156\n",
            "Epoch 24/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1984 - accuracy: 0.9099 - val_loss: 0.1908 - val_accuracy: 0.9152\n",
            "Epoch 25/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1956 - accuracy: 0.9106 - val_loss: 0.1936 - val_accuracy: 0.9124\n",
            "Epoch 26/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1939 - accuracy: 0.9135 - val_loss: 0.1907 - val_accuracy: 0.9132\n",
            "Epoch 27/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1936 - accuracy: 0.9104 - val_loss: 0.1902 - val_accuracy: 0.9125\n",
            "Epoch 28/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9162 - val_loss: 0.1912 - val_accuracy: 0.9148\n",
            "Epoch 29/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9125 - val_loss: 0.1907 - val_accuracy: 0.9142\n",
            "Epoch 30/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9144 - val_loss: 0.1901 - val_accuracy: 0.9131\n",
            "Epoch 31/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9098 - val_loss: 0.1897 - val_accuracy: 0.9143\n",
            "Epoch 32/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1962 - accuracy: 0.9105 - val_loss: 0.1923 - val_accuracy: 0.9139\n",
            "Epoch 33/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1944 - accuracy: 0.9131 - val_loss: 0.1895 - val_accuracy: 0.9135\n",
            "Epoch 34/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1988 - accuracy: 0.9100 - val_loss: 0.1896 - val_accuracy: 0.9120\n",
            "Epoch 35/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9140 - val_loss: 0.1892 - val_accuracy: 0.9158\n",
            "Epoch 36/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9113 - val_loss: 0.1921 - val_accuracy: 0.9141\n",
            "Epoch 37/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9112 - val_loss: 0.1930 - val_accuracy: 0.9123\n",
            "Epoch 38/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1923 - accuracy: 0.9124 - val_loss: 0.1887 - val_accuracy: 0.9140\n",
            "Epoch 39/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9094 - val_loss: 0.1885 - val_accuracy: 0.9126\n",
            "Epoch 40/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1976 - accuracy: 0.9077 - val_loss: 0.1884 - val_accuracy: 0.9134\n",
            "Epoch 41/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9135 - val_loss: 0.1886 - val_accuracy: 0.9135\n",
            "Epoch 42/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1941 - accuracy: 0.9104 - val_loss: 0.1892 - val_accuracy: 0.9116\n",
            "Epoch 43/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1920 - accuracy: 0.9106 - val_loss: 0.1886 - val_accuracy: 0.9133\n",
            "Epoch 44/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9110 - val_loss: 0.1888 - val_accuracy: 0.9128\n",
            "Epoch 45/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1890 - accuracy: 0.9139 - val_loss: 0.1873 - val_accuracy: 0.9127\n",
            "Epoch 46/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1896 - accuracy: 0.9134 - val_loss: 0.1875 - val_accuracy: 0.9141\n",
            "Epoch 47/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9087 - val_loss: 0.1874 - val_accuracy: 0.9138\n",
            "Epoch 48/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9132 - val_loss: 0.1875 - val_accuracy: 0.9139\n",
            "Epoch 49/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1921 - accuracy: 0.9123 - val_loss: 0.1877 - val_accuracy: 0.9127\n",
            "Epoch 50/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9138 - val_loss: 0.1869 - val_accuracy: 0.9122\n",
            "Epoch 51/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1960 - accuracy: 0.9098 - val_loss: 0.1877 - val_accuracy: 0.9133\n",
            "Epoch 52/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1927 - accuracy: 0.9099 - val_loss: 0.1877 - val_accuracy: 0.9129\n",
            "Epoch 53/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9144 - val_loss: 0.1864 - val_accuracy: 0.9127\n",
            "Epoch 54/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9138 - val_loss: 0.1863 - val_accuracy: 0.9127\n",
            "Epoch 55/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9132 - val_loss: 0.1856 - val_accuracy: 0.9142\n",
            "Epoch 56/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9149 - val_loss: 0.1858 - val_accuracy: 0.9123\n",
            "Epoch 57/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9111 - val_loss: 0.1858 - val_accuracy: 0.9133\n",
            "Epoch 58/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9139 - val_loss: 0.1869 - val_accuracy: 0.9122\n",
            "Epoch 59/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9096 - val_loss: 0.1854 - val_accuracy: 0.9135\n",
            "Epoch 60/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9104 - val_loss: 0.1864 - val_accuracy: 0.9124\n",
            "Epoch 61/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1867 - accuracy: 0.9132 - val_loss: 0.1848 - val_accuracy: 0.9135\n",
            "Epoch 62/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9103 - val_loss: 0.1845 - val_accuracy: 0.9128\n",
            "Epoch 63/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1901 - accuracy: 0.9126 - val_loss: 0.1843 - val_accuracy: 0.9131\n",
            "Epoch 64/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1870 - accuracy: 0.9148 - val_loss: 0.1850 - val_accuracy: 0.9124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I9OqVLPARtwn",
        "outputId": "7726876e-c629-45df-d31b-e1de9c0ab648"
      },
      "source": [
        "model_r1 = Sequential()\n",
        "model_r1.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_r1.add(Dense(4,activation='relu'))\n",
        "model_r1.add(Dense(1,activation='sigmoid'))\n",
        "model_r1.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "model_r1.summary()\n",
        "history = model_r1.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=512)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_47 (Dense)             (None, 8)                 168       \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 209\n",
            "Trainable params: 209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4137 - accuracy: 0.8118 - val_loss: 0.2741 - val_accuracy: 0.9007\n",
            "Epoch 2/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2831 - accuracy: 0.8943 - val_loss: 0.2522 - val_accuracy: 0.9054\n",
            "Epoch 3/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2517 - accuracy: 0.9004 - val_loss: 0.2270 - val_accuracy: 0.9088\n",
            "Epoch 4/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2355 - accuracy: 0.9047 - val_loss: 0.2147 - val_accuracy: 0.9105\n",
            "Epoch 5/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2131 - accuracy: 0.9125 - val_loss: 0.2076 - val_accuracy: 0.9127\n",
            "Epoch 6/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2171 - accuracy: 0.9078 - val_loss: 0.2051 - val_accuracy: 0.9147\n",
            "Epoch 7/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2113 - accuracy: 0.9094 - val_loss: 0.2080 - val_accuracy: 0.9143\n",
            "Epoch 8/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2151 - accuracy: 0.9074 - val_loss: 0.2009 - val_accuracy: 0.9147\n",
            "Epoch 9/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2106 - accuracy: 0.9087 - val_loss: 0.1993 - val_accuracy: 0.9169\n",
            "Epoch 10/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2082 - accuracy: 0.9086 - val_loss: 0.1976 - val_accuracy: 0.9156\n",
            "Epoch 11/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2097 - accuracy: 0.9074 - val_loss: 0.1969 - val_accuracy: 0.9148\n",
            "Epoch 12/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9116 - val_loss: 0.1949 - val_accuracy: 0.9160\n",
            "Epoch 13/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2062 - accuracy: 0.9099 - val_loss: 0.1985 - val_accuracy: 0.9131\n",
            "Epoch 14/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2051 - accuracy: 0.9095 - val_loss: 0.1944 - val_accuracy: 0.9142\n",
            "Epoch 15/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2059 - accuracy: 0.9082 - val_loss: 0.1928 - val_accuracy: 0.9160\n",
            "Epoch 16/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9114 - val_loss: 0.1938 - val_accuracy: 0.9146\n",
            "Epoch 17/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9145 - val_loss: 0.1932 - val_accuracy: 0.9148\n",
            "Epoch 18/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9107 - val_loss: 0.1916 - val_accuracy: 0.9156\n",
            "Epoch 19/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2008 - accuracy: 0.9130 - val_loss: 0.1916 - val_accuracy: 0.9160\n",
            "Epoch 20/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1978 - accuracy: 0.9122 - val_loss: 0.1912 - val_accuracy: 0.9143\n",
            "Epoch 21/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9129 - val_loss: 0.1937 - val_accuracy: 0.9134\n",
            "Epoch 22/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9128 - val_loss: 0.1910 - val_accuracy: 0.9157\n",
            "Epoch 23/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9117 - val_loss: 0.1902 - val_accuracy: 0.9141\n",
            "Epoch 24/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9119 - val_loss: 0.1891 - val_accuracy: 0.9160\n",
            "Epoch 25/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9127 - val_loss: 0.1894 - val_accuracy: 0.9160\n",
            "Epoch 26/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9125 - val_loss: 0.1891 - val_accuracy: 0.9159\n",
            "Epoch 27/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9105 - val_loss: 0.1886 - val_accuracy: 0.9152\n",
            "Epoch 28/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9078 - val_loss: 0.1896 - val_accuracy: 0.9156\n",
            "Epoch 29/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9128 - val_loss: 0.1890 - val_accuracy: 0.9148\n",
            "Epoch 30/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9123 - val_loss: 0.1889 - val_accuracy: 0.9152\n",
            "Epoch 31/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9112 - val_loss: 0.1920 - val_accuracy: 0.9137\n",
            "Epoch 32/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9095 - val_loss: 0.1868 - val_accuracy: 0.9170\n",
            "Epoch 33/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9138 - val_loss: 0.1929 - val_accuracy: 0.9143\n",
            "Epoch 34/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9116 - val_loss: 0.1864 - val_accuracy: 0.9142\n",
            "Epoch 35/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9117 - val_loss: 0.1866 - val_accuracy: 0.9143\n",
            "Epoch 36/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9128 - val_loss: 0.1896 - val_accuracy: 0.9137\n",
            "Epoch 37/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9132 - val_loss: 0.1855 - val_accuracy: 0.9141\n",
            "Epoch 38/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9113 - val_loss: 0.1893 - val_accuracy: 0.9124\n",
            "Epoch 39/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1968 - accuracy: 0.9072 - val_loss: 0.1874 - val_accuracy: 0.9131\n",
            "Epoch 40/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9090 - val_loss: 0.1857 - val_accuracy: 0.9144\n",
            "Epoch 41/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1921 - accuracy: 0.9121 - val_loss: 0.1867 - val_accuracy: 0.9129\n",
            "Epoch 42/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9104 - val_loss: 0.1854 - val_accuracy: 0.9152\n",
            "Epoch 43/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9139 - val_loss: 0.1871 - val_accuracy: 0.9125\n",
            "Epoch 44/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9137 - val_loss: 0.1885 - val_accuracy: 0.9123\n",
            "Epoch 45/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9142 - val_loss: 0.1906 - val_accuracy: 0.9115\n",
            "Epoch 46/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9099 - val_loss: 0.1844 - val_accuracy: 0.9150\n",
            "Epoch 47/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9099 - val_loss: 0.1854 - val_accuracy: 0.9145\n",
            "Epoch 48/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9130 - val_loss: 0.1884 - val_accuracy: 0.9131\n",
            "Epoch 49/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9128 - val_loss: 0.1857 - val_accuracy: 0.9134\n",
            "Epoch 50/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9107 - val_loss: 0.1856 - val_accuracy: 0.9145\n",
            "Epoch 51/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9118 - val_loss: 0.1875 - val_accuracy: 0.9114\n",
            "Epoch 52/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9124 - val_loss: 0.1840 - val_accuracy: 0.9148\n",
            "Epoch 53/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9115 - val_loss: 0.1824 - val_accuracy: 0.9136\n",
            "Epoch 54/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9141 - val_loss: 0.1828 - val_accuracy: 0.9147\n",
            "Epoch 55/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9156 - val_loss: 0.1820 - val_accuracy: 0.9158\n",
            "Epoch 56/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9156 - val_loss: 0.1824 - val_accuracy: 0.9140\n",
            "Epoch 57/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9126 - val_loss: 0.1837 - val_accuracy: 0.9135\n",
            "Epoch 58/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9104 - val_loss: 0.1824 - val_accuracy: 0.9150\n",
            "Epoch 59/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1824 - accuracy: 0.9154 - val_loss: 0.1816 - val_accuracy: 0.9139\n",
            "Epoch 60/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9116 - val_loss: 0.1811 - val_accuracy: 0.9156\n",
            "Epoch 61/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9118 - val_loss: 0.1821 - val_accuracy: 0.9150\n",
            "Epoch 62/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9146 - val_loss: 0.1812 - val_accuracy: 0.9155\n",
            "Epoch 63/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9158 - val_loss: 0.1858 - val_accuracy: 0.9129\n",
            "Epoch 64/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9146 - val_loss: 0.1808 - val_accuracy: 0.9144\n",
            "Epoch 65/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9119 - val_loss: 0.1809 - val_accuracy: 0.9148\n",
            "Epoch 66/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9118 - val_loss: 0.1835 - val_accuracy: 0.9145\n",
            "Epoch 67/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9117 - val_loss: 0.1849 - val_accuracy: 0.9139\n",
            "Epoch 68/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9110 - val_loss: 0.1801 - val_accuracy: 0.9155\n",
            "Epoch 69/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9145 - val_loss: 0.1807 - val_accuracy: 0.9137\n",
            "Epoch 70/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9130 - val_loss: 0.1808 - val_accuracy: 0.9138\n",
            "Epoch 71/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9118 - val_loss: 0.1813 - val_accuracy: 0.9147\n",
            "Epoch 72/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9128 - val_loss: 0.1870 - val_accuracy: 0.9115\n",
            "Epoch 73/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9143 - val_loss: 0.1825 - val_accuracy: 0.9142\n",
            "Epoch 74/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9127 - val_loss: 0.1837 - val_accuracy: 0.9148\n",
            "Epoch 75/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9153 - val_loss: 0.1837 - val_accuracy: 0.9132\n",
            "Epoch 76/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1835 - accuracy: 0.9151 - val_loss: 0.1800 - val_accuracy: 0.9159\n",
            "Epoch 77/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9130 - val_loss: 0.1798 - val_accuracy: 0.9159\n",
            "Epoch 78/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9129 - val_loss: 0.1841 - val_accuracy: 0.9131\n",
            "Epoch 79/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9115 - val_loss: 0.1797 - val_accuracy: 0.9150\n",
            "Epoch 80/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9106 - val_loss: 0.1802 - val_accuracy: 0.9158\n",
            "Epoch 81/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9136 - val_loss: 0.1796 - val_accuracy: 0.9146\n",
            "Epoch 82/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9134 - val_loss: 0.1803 - val_accuracy: 0.9135\n",
            "Epoch 83/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9121 - val_loss: 0.1795 - val_accuracy: 0.9152\n",
            "Epoch 84/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9147 - val_loss: 0.1820 - val_accuracy: 0.9141\n",
            "Epoch 85/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9141 - val_loss: 0.1806 - val_accuracy: 0.9146\n",
            "Epoch 86/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9158 - val_loss: 0.1806 - val_accuracy: 0.9144\n",
            "Epoch 87/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9130 - val_loss: 0.1800 - val_accuracy: 0.9138\n",
            "Epoch 88/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9131 - val_loss: 0.1808 - val_accuracy: 0.9142\n",
            "Epoch 89/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9108 - val_loss: 0.1804 - val_accuracy: 0.9138\n",
            "Epoch 90/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9159 - val_loss: 0.1804 - val_accuracy: 0.9156\n",
            "Epoch 91/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9166 - val_loss: 0.1807 - val_accuracy: 0.9158\n",
            "Epoch 92/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9123 - val_loss: 0.1799 - val_accuracy: 0.9163\n",
            "Epoch 93/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9134 - val_loss: 0.1784 - val_accuracy: 0.9148\n",
            "Epoch 94/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9106 - val_loss: 0.1796 - val_accuracy: 0.9146\n",
            "Epoch 95/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9177 - val_loss: 0.1803 - val_accuracy: 0.9143\n",
            "Epoch 96/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1840 - accuracy: 0.9134 - val_loss: 0.1801 - val_accuracy: 0.9152\n",
            "Epoch 97/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9154 - val_loss: 0.1816 - val_accuracy: 0.9131\n",
            "Epoch 98/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9120 - val_loss: 0.1788 - val_accuracy: 0.9155\n",
            "Epoch 99/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9148 - val_loss: 0.1787 - val_accuracy: 0.9146\n",
            "Epoch 100/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9117 - val_loss: 0.1823 - val_accuracy: 0.9136\n",
            "Epoch 101/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9156 - val_loss: 0.1794 - val_accuracy: 0.9149\n",
            "Epoch 102/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9132 - val_loss: 0.1793 - val_accuracy: 0.9142\n",
            "Epoch 103/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9148 - val_loss: 0.1801 - val_accuracy: 0.9131\n",
            "Epoch 104/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9118 - val_loss: 0.1786 - val_accuracy: 0.9152\n",
            "Epoch 105/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9160 - val_loss: 0.1794 - val_accuracy: 0.9157\n",
            "Epoch 106/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9144 - val_loss: 0.1787 - val_accuracy: 0.9148\n",
            "Epoch 107/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1837 - accuracy: 0.9108 - val_loss: 0.1792 - val_accuracy: 0.9148\n",
            "Epoch 108/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9123 - val_loss: 0.1790 - val_accuracy: 0.9148\n",
            "Epoch 109/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9145 - val_loss: 0.1807 - val_accuracy: 0.9143\n",
            "Epoch 110/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9143 - val_loss: 0.1806 - val_accuracy: 0.9158\n",
            "Epoch 111/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9099 - val_loss: 0.1831 - val_accuracy: 0.9132\n",
            "Epoch 112/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1869 - accuracy: 0.9109 - val_loss: 0.1790 - val_accuracy: 0.9148\n",
            "Epoch 113/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9151 - val_loss: 0.1803 - val_accuracy: 0.9140\n",
            "Epoch 114/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9109 - val_loss: 0.1789 - val_accuracy: 0.9153\n",
            "Epoch 115/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9136 - val_loss: 0.1804 - val_accuracy: 0.9135\n",
            "Epoch 116/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9152 - val_loss: 0.1790 - val_accuracy: 0.9152\n",
            "Epoch 117/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9149 - val_loss: 0.1797 - val_accuracy: 0.9144\n",
            "Epoch 118/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9128 - val_loss: 0.1791 - val_accuracy: 0.9148\n",
            "Epoch 119/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9118 - val_loss: 0.1793 - val_accuracy: 0.9144\n",
            "Epoch 120/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9108 - val_loss: 0.1792 - val_accuracy: 0.9140\n",
            "Epoch 121/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9150 - val_loss: 0.1805 - val_accuracy: 0.9156\n",
            "Epoch 122/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9098 - val_loss: 0.1835 - val_accuracy: 0.9142\n",
            "Epoch 123/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9156 - val_loss: 0.1805 - val_accuracy: 0.9143\n",
            "Epoch 124/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9140 - val_loss: 0.1790 - val_accuracy: 0.9145\n",
            "Epoch 125/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9141 - val_loss: 0.1790 - val_accuracy: 0.9153\n",
            "Epoch 126/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9138 - val_loss: 0.1792 - val_accuracy: 0.9139\n",
            "Epoch 127/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9164 - val_loss: 0.1783 - val_accuracy: 0.9152\n",
            "Epoch 128/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9142 - val_loss: 0.1798 - val_accuracy: 0.9141\n",
            "Epoch 129/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9129 - val_loss: 0.1793 - val_accuracy: 0.9144\n",
            "Epoch 130/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9157 - val_loss: 0.1800 - val_accuracy: 0.9152\n",
            "Epoch 131/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9140 - val_loss: 0.1836 - val_accuracy: 0.9126\n",
            "Epoch 132/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9131 - val_loss: 0.1794 - val_accuracy: 0.9138\n",
            "Epoch 133/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9155 - val_loss: 0.1782 - val_accuracy: 0.9147\n",
            "Epoch 134/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9114 - val_loss: 0.1791 - val_accuracy: 0.9153\n",
            "Epoch 135/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9134 - val_loss: 0.1823 - val_accuracy: 0.9144\n",
            "Epoch 136/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9153 - val_loss: 0.1817 - val_accuracy: 0.9146\n",
            "Epoch 137/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9118 - val_loss: 0.1785 - val_accuracy: 0.9150\n",
            "Epoch 138/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9124 - val_loss: 0.1815 - val_accuracy: 0.9144\n",
            "Epoch 139/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9149 - val_loss: 0.1797 - val_accuracy: 0.9151\n",
            "Epoch 140/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9143 - val_loss: 0.1803 - val_accuracy: 0.9146\n",
            "Epoch 141/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9128 - val_loss: 0.1789 - val_accuracy: 0.9152\n",
            "Epoch 142/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9131 - val_loss: 0.1783 - val_accuracy: 0.9161\n",
            "Epoch 143/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9133 - val_loss: 0.1814 - val_accuracy: 0.9155\n",
            "Epoch 144/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9139 - val_loss: 0.1806 - val_accuracy: 0.9142\n",
            "Epoch 145/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9129 - val_loss: 0.1802 - val_accuracy: 0.9150\n",
            "Epoch 146/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9140 - val_loss: 0.1782 - val_accuracy: 0.9154\n",
            "Epoch 147/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9150 - val_loss: 0.1831 - val_accuracy: 0.9150\n",
            "Epoch 148/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9134 - val_loss: 0.1793 - val_accuracy: 0.9148\n",
            "Epoch 149/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9149 - val_loss: 0.1808 - val_accuracy: 0.9159\n",
            "Epoch 150/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9131 - val_loss: 0.1783 - val_accuracy: 0.9156\n",
            "Epoch 151/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9149 - val_loss: 0.1783 - val_accuracy: 0.9154\n",
            "Epoch 152/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9160 - val_loss: 0.1829 - val_accuracy: 0.9135\n",
            "Epoch 153/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9137 - val_loss: 0.1787 - val_accuracy: 0.9165\n",
            "Epoch 154/512\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9132 - val_loss: 0.1787 - val_accuracy: 0.9156\n",
            "Epoch 155/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9169 - val_loss: 0.1798 - val_accuracy: 0.9145\n",
            "Epoch 156/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9160 - val_loss: 0.1789 - val_accuracy: 0.9153\n",
            "Epoch 157/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9154 - val_loss: 0.1788 - val_accuracy: 0.9157\n",
            "Epoch 158/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9149 - val_loss: 0.1788 - val_accuracy: 0.9161\n",
            "Epoch 159/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9125 - val_loss: 0.1795 - val_accuracy: 0.9161\n",
            "Epoch 160/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9124 - val_loss: 0.1789 - val_accuracy: 0.9156\n",
            "Epoch 161/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9141 - val_loss: 0.1788 - val_accuracy: 0.9159\n",
            "Epoch 162/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9123 - val_loss: 0.1785 - val_accuracy: 0.9156\n",
            "Epoch 163/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9157 - val_loss: 0.1809 - val_accuracy: 0.9150\n",
            "Epoch 164/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9160 - val_loss: 0.1809 - val_accuracy: 0.9148\n",
            "Epoch 165/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9126 - val_loss: 0.1806 - val_accuracy: 0.9146\n",
            "Epoch 166/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9121 - val_loss: 0.1784 - val_accuracy: 0.9152\n",
            "Epoch 167/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9103 - val_loss: 0.1797 - val_accuracy: 0.9140\n",
            "Epoch 168/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9138 - val_loss: 0.1795 - val_accuracy: 0.9167\n",
            "Epoch 169/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9137 - val_loss: 0.1797 - val_accuracy: 0.9145\n",
            "Epoch 170/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9120 - val_loss: 0.1854 - val_accuracy: 0.9122\n",
            "Epoch 171/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9136 - val_loss: 0.1784 - val_accuracy: 0.9149\n",
            "Epoch 172/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9142 - val_loss: 0.1862 - val_accuracy: 0.9109\n",
            "Epoch 173/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9154 - val_loss: 0.1955 - val_accuracy: 0.9081\n",
            "Epoch 174/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9145 - val_loss: 0.1794 - val_accuracy: 0.9160\n",
            "Epoch 175/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9156 - val_loss: 0.1788 - val_accuracy: 0.9156\n",
            "Epoch 176/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9128 - val_loss: 0.1787 - val_accuracy: 0.9160\n",
            "Epoch 177/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9148 - val_loss: 0.1785 - val_accuracy: 0.9156\n",
            "Epoch 178/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9142 - val_loss: 0.1782 - val_accuracy: 0.9160\n",
            "Epoch 179/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9143 - val_loss: 0.1801 - val_accuracy: 0.9149\n",
            "Epoch 180/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9167 - val_loss: 0.1782 - val_accuracy: 0.9152\n",
            "Epoch 181/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9144 - val_loss: 0.1790 - val_accuracy: 0.9148\n",
            "Epoch 182/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9168 - val_loss: 0.1789 - val_accuracy: 0.9148\n",
            "Epoch 183/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9145 - val_loss: 0.1788 - val_accuracy: 0.9161\n",
            "Epoch 184/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9138 - val_loss: 0.1802 - val_accuracy: 0.9143\n",
            "Epoch 185/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9133 - val_loss: 0.1855 - val_accuracy: 0.9125\n",
            "Epoch 186/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9141 - val_loss: 0.1817 - val_accuracy: 0.9145\n",
            "Epoch 187/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1759 - accuracy: 0.9177 - val_loss: 0.1790 - val_accuracy: 0.9149\n",
            "Epoch 188/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9156 - val_loss: 0.1854 - val_accuracy: 0.9125\n",
            "Epoch 189/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9158 - val_loss: 0.1909 - val_accuracy: 0.9092\n",
            "Epoch 190/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9120 - val_loss: 0.1791 - val_accuracy: 0.9138\n",
            "Epoch 191/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9149 - val_loss: 0.1785 - val_accuracy: 0.9148\n",
            "Epoch 192/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9149 - val_loss: 0.1789 - val_accuracy: 0.9152\n",
            "Epoch 193/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9126 - val_loss: 0.1795 - val_accuracy: 0.9148\n",
            "Epoch 194/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9145 - val_loss: 0.1833 - val_accuracy: 0.9132\n",
            "Epoch 195/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9120 - val_loss: 0.1785 - val_accuracy: 0.9161\n",
            "Epoch 196/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9128 - val_loss: 0.1789 - val_accuracy: 0.9155\n",
            "Epoch 197/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9157 - val_loss: 0.1787 - val_accuracy: 0.9149\n",
            "Epoch 198/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9149 - val_loss: 0.1788 - val_accuracy: 0.9154\n",
            "Epoch 199/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9135 - val_loss: 0.1799 - val_accuracy: 0.9156\n",
            "Epoch 200/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9128 - val_loss: 0.1788 - val_accuracy: 0.9159\n",
            "Epoch 201/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9139 - val_loss: 0.1819 - val_accuracy: 0.9139\n",
            "Epoch 202/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9168 - val_loss: 0.1812 - val_accuracy: 0.9143\n",
            "Epoch 203/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9154 - val_loss: 0.1811 - val_accuracy: 0.9153\n",
            "Epoch 204/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9130 - val_loss: 0.1797 - val_accuracy: 0.9145\n",
            "Epoch 205/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9155 - val_loss: 0.1797 - val_accuracy: 0.9149\n",
            "Epoch 206/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9122 - val_loss: 0.1795 - val_accuracy: 0.9158\n",
            "Epoch 207/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1782 - accuracy: 0.9173 - val_loss: 0.1789 - val_accuracy: 0.9154\n",
            "Epoch 208/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9121 - val_loss: 0.1815 - val_accuracy: 0.9133\n",
            "Epoch 209/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9122 - val_loss: 0.1783 - val_accuracy: 0.9148\n",
            "Epoch 210/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9125 - val_loss: 0.1790 - val_accuracy: 0.9162\n",
            "Epoch 211/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9159 - val_loss: 0.1787 - val_accuracy: 0.9157\n",
            "Epoch 212/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9137 - val_loss: 0.1786 - val_accuracy: 0.9165\n",
            "Epoch 213/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9120 - val_loss: 0.1796 - val_accuracy: 0.9141\n",
            "Epoch 214/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9151 - val_loss: 0.1784 - val_accuracy: 0.9151\n",
            "Epoch 215/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9167 - val_loss: 0.1813 - val_accuracy: 0.9152\n",
            "Epoch 216/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9148 - val_loss: 0.1788 - val_accuracy: 0.9158\n",
            "Epoch 217/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9183 - val_loss: 0.1786 - val_accuracy: 0.9146\n",
            "Epoch 218/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9111 - val_loss: 0.1805 - val_accuracy: 0.9133\n",
            "Epoch 219/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9147 - val_loss: 0.1782 - val_accuracy: 0.9152\n",
            "Epoch 220/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9123 - val_loss: 0.1786 - val_accuracy: 0.9152\n",
            "Epoch 221/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9124 - val_loss: 0.1792 - val_accuracy: 0.9152\n",
            "Epoch 222/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9161 - val_loss: 0.1805 - val_accuracy: 0.9137\n",
            "Epoch 223/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1773 - accuracy: 0.9156 - val_loss: 0.1793 - val_accuracy: 0.9143\n",
            "Epoch 224/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9131 - val_loss: 0.1807 - val_accuracy: 0.9153\n",
            "Epoch 225/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9157 - val_loss: 0.1798 - val_accuracy: 0.9144\n",
            "Epoch 226/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9143 - val_loss: 0.1787 - val_accuracy: 0.9158\n",
            "Epoch 227/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9149 - val_loss: 0.1802 - val_accuracy: 0.9160\n",
            "Epoch 228/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9148 - val_loss: 0.1812 - val_accuracy: 0.9141\n",
            "Epoch 229/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9139 - val_loss: 0.1835 - val_accuracy: 0.9129\n",
            "Epoch 230/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9149 - val_loss: 0.1799 - val_accuracy: 0.9149\n",
            "Epoch 231/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9133 - val_loss: 0.1783 - val_accuracy: 0.9155\n",
            "Epoch 232/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9125 - val_loss: 0.1782 - val_accuracy: 0.9165\n",
            "Epoch 233/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9142 - val_loss: 0.1780 - val_accuracy: 0.9152\n",
            "Epoch 234/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9125 - val_loss: 0.1819 - val_accuracy: 0.9136\n",
            "Epoch 235/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9110 - val_loss: 0.1841 - val_accuracy: 0.9132\n",
            "Epoch 236/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9147 - val_loss: 0.1797 - val_accuracy: 0.9148\n",
            "Epoch 237/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9152 - val_loss: 0.1793 - val_accuracy: 0.9150\n",
            "Epoch 238/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9153 - val_loss: 0.1786 - val_accuracy: 0.9155\n",
            "Epoch 239/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9143 - val_loss: 0.1792 - val_accuracy: 0.9157\n",
            "Epoch 240/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9136 - val_loss: 0.1794 - val_accuracy: 0.9152\n",
            "Epoch 241/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9162 - val_loss: 0.1815 - val_accuracy: 0.9152\n",
            "Epoch 242/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9142 - val_loss: 0.1843 - val_accuracy: 0.9125\n",
            "Epoch 243/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9148 - val_loss: 0.1821 - val_accuracy: 0.9146\n",
            "Epoch 244/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9123 - val_loss: 0.1781 - val_accuracy: 0.9160\n",
            "Epoch 245/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9135 - val_loss: 0.1836 - val_accuracy: 0.9124\n",
            "Epoch 246/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9156 - val_loss: 0.1810 - val_accuracy: 0.9153\n",
            "Epoch 247/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9134 - val_loss: 0.1838 - val_accuracy: 0.9133\n",
            "Epoch 248/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9135 - val_loss: 0.1838 - val_accuracy: 0.9135\n",
            "Epoch 249/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9148 - val_loss: 0.1861 - val_accuracy: 0.9128\n",
            "Epoch 250/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9149 - val_loss: 0.1833 - val_accuracy: 0.9148\n",
            "Epoch 251/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9183 - val_loss: 0.1806 - val_accuracy: 0.9154\n",
            "Epoch 252/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9154 - val_loss: 0.1814 - val_accuracy: 0.9143\n",
            "Epoch 253/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9138 - val_loss: 0.1793 - val_accuracy: 0.9139\n",
            "Epoch 254/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9111 - val_loss: 0.1787 - val_accuracy: 0.9154\n",
            "Epoch 255/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9123 - val_loss: 0.1782 - val_accuracy: 0.9157\n",
            "Epoch 256/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9118 - val_loss: 0.1784 - val_accuracy: 0.9156\n",
            "Epoch 257/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9167 - val_loss: 0.1806 - val_accuracy: 0.9138\n",
            "Epoch 258/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9169 - val_loss: 0.1791 - val_accuracy: 0.9150\n",
            "Epoch 259/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9156 - val_loss: 0.1787 - val_accuracy: 0.9149\n",
            "Epoch 260/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9144 - val_loss: 0.1840 - val_accuracy: 0.9121\n",
            "Epoch 261/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9158 - val_loss: 0.1793 - val_accuracy: 0.9135\n",
            "Epoch 262/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9166 - val_loss: 0.1796 - val_accuracy: 0.9153\n",
            "Epoch 263/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9155 - val_loss: 0.1781 - val_accuracy: 0.9155\n",
            "Epoch 264/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9135 - val_loss: 0.1782 - val_accuracy: 0.9161\n",
            "Epoch 265/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9143 - val_loss: 0.1795 - val_accuracy: 0.9151\n",
            "Epoch 266/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9127 - val_loss: 0.1780 - val_accuracy: 0.9166\n",
            "Epoch 267/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9161 - val_loss: 0.1781 - val_accuracy: 0.9173\n",
            "Epoch 268/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9170 - val_loss: 0.1788 - val_accuracy: 0.9153\n",
            "Epoch 269/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9151 - val_loss: 0.1786 - val_accuracy: 0.9170\n",
            "Epoch 270/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9161 - val_loss: 0.1781 - val_accuracy: 0.9170\n",
            "Epoch 271/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9153 - val_loss: 0.1779 - val_accuracy: 0.9173\n",
            "Epoch 272/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9150 - val_loss: 0.1781 - val_accuracy: 0.9173\n",
            "Epoch 273/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9140 - val_loss: 0.1815 - val_accuracy: 0.9141\n",
            "Epoch 274/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9147 - val_loss: 0.1785 - val_accuracy: 0.9157\n",
            "Epoch 275/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9169 - val_loss: 0.1779 - val_accuracy: 0.9165\n",
            "Epoch 276/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9133 - val_loss: 0.1786 - val_accuracy: 0.9157\n",
            "Epoch 277/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9134 - val_loss: 0.1783 - val_accuracy: 0.9162\n",
            "Epoch 278/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9146 - val_loss: 0.1794 - val_accuracy: 0.9156\n",
            "Epoch 279/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9153 - val_loss: 0.1780 - val_accuracy: 0.9153\n",
            "Epoch 280/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9134 - val_loss: 0.1786 - val_accuracy: 0.9167\n",
            "Epoch 281/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9166 - val_loss: 0.1802 - val_accuracy: 0.9153\n",
            "Epoch 282/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9121 - val_loss: 0.1783 - val_accuracy: 0.9165\n",
            "Epoch 283/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9106 - val_loss: 0.1790 - val_accuracy: 0.9154\n",
            "Epoch 284/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9150 - val_loss: 0.1788 - val_accuracy: 0.9155\n",
            "Epoch 285/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9113 - val_loss: 0.1784 - val_accuracy: 0.9146\n",
            "Epoch 286/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9142 - val_loss: 0.1785 - val_accuracy: 0.9160\n",
            "Epoch 287/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9130 - val_loss: 0.1849 - val_accuracy: 0.9152\n",
            "Epoch 288/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9160 - val_loss: 0.1779 - val_accuracy: 0.9161\n",
            "Epoch 289/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9131 - val_loss: 0.1782 - val_accuracy: 0.9156\n",
            "Epoch 290/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9152 - val_loss: 0.1778 - val_accuracy: 0.9173\n",
            "Epoch 291/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1795 - accuracy: 0.9156 - val_loss: 0.1776 - val_accuracy: 0.9156\n",
            "Epoch 292/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1780 - accuracy: 0.9164 - val_loss: 0.1809 - val_accuracy: 0.9145\n",
            "Epoch 293/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9160 - val_loss: 0.1806 - val_accuracy: 0.9166\n",
            "Epoch 294/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9144 - val_loss: 0.1786 - val_accuracy: 0.9167\n",
            "Epoch 295/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9120 - val_loss: 0.1776 - val_accuracy: 0.9155\n",
            "Epoch 296/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9134 - val_loss: 0.1774 - val_accuracy: 0.9155\n",
            "Epoch 297/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9164 - val_loss: 0.1837 - val_accuracy: 0.9150\n",
            "Epoch 298/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9147 - val_loss: 0.1776 - val_accuracy: 0.9156\n",
            "Epoch 299/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9157 - val_loss: 0.1789 - val_accuracy: 0.9173\n",
            "Epoch 300/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9145 - val_loss: 0.1796 - val_accuracy: 0.9161\n",
            "Epoch 301/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9136 - val_loss: 0.1774 - val_accuracy: 0.9160\n",
            "Epoch 302/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9137 - val_loss: 0.1796 - val_accuracy: 0.9171\n",
            "Epoch 303/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9173 - val_loss: 0.1825 - val_accuracy: 0.9163\n",
            "Epoch 304/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9156 - val_loss: 0.1779 - val_accuracy: 0.9148\n",
            "Epoch 305/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9145 - val_loss: 0.1829 - val_accuracy: 0.9152\n",
            "Epoch 306/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9131 - val_loss: 0.1795 - val_accuracy: 0.9142\n",
            "Epoch 307/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9152 - val_loss: 0.1777 - val_accuracy: 0.9161\n",
            "Epoch 308/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9149 - val_loss: 0.1779 - val_accuracy: 0.9161\n",
            "Epoch 309/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9142 - val_loss: 0.1782 - val_accuracy: 0.9164\n",
            "Epoch 310/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9146 - val_loss: 0.1805 - val_accuracy: 0.9133\n",
            "Epoch 311/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9140 - val_loss: 0.1779 - val_accuracy: 0.9154\n",
            "Epoch 312/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9138 - val_loss: 0.1774 - val_accuracy: 0.9154\n",
            "Epoch 313/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9116 - val_loss: 0.1774 - val_accuracy: 0.9173\n",
            "Epoch 314/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9139 - val_loss: 0.1807 - val_accuracy: 0.9147\n",
            "Epoch 315/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9133 - val_loss: 0.1783 - val_accuracy: 0.9143\n",
            "Epoch 316/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9143 - val_loss: 0.1774 - val_accuracy: 0.9175\n",
            "Epoch 317/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9150 - val_loss: 0.1823 - val_accuracy: 0.9143\n",
            "Epoch 318/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9149 - val_loss: 0.1784 - val_accuracy: 0.9169\n",
            "Epoch 319/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9122 - val_loss: 0.1775 - val_accuracy: 0.9164\n",
            "Epoch 320/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9163 - val_loss: 0.1832 - val_accuracy: 0.9131\n",
            "Epoch 321/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9115 - val_loss: 0.1785 - val_accuracy: 0.9163\n",
            "Epoch 322/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9159 - val_loss: 0.1795 - val_accuracy: 0.9170\n",
            "Epoch 323/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9138 - val_loss: 0.1799 - val_accuracy: 0.9132\n",
            "Epoch 324/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9146 - val_loss: 0.1785 - val_accuracy: 0.9178\n",
            "Epoch 325/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9138 - val_loss: 0.1801 - val_accuracy: 0.9166\n",
            "Epoch 326/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9153 - val_loss: 0.1796 - val_accuracy: 0.9168\n",
            "Epoch 327/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9162 - val_loss: 0.1779 - val_accuracy: 0.9159\n",
            "Epoch 328/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9149 - val_loss: 0.1776 - val_accuracy: 0.9165\n",
            "Epoch 329/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9162 - val_loss: 0.1850 - val_accuracy: 0.9143\n",
            "Epoch 330/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9155 - val_loss: 0.1822 - val_accuracy: 0.9142\n",
            "Epoch 331/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9129 - val_loss: 0.1775 - val_accuracy: 0.9164\n",
            "Epoch 332/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9177 - val_loss: 0.1781 - val_accuracy: 0.9169\n",
            "Epoch 333/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9128 - val_loss: 0.1794 - val_accuracy: 0.9168\n",
            "Epoch 334/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9127 - val_loss: 0.1794 - val_accuracy: 0.9162\n",
            "Epoch 335/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9145 - val_loss: 0.1787 - val_accuracy: 0.9169\n",
            "Epoch 336/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9158 - val_loss: 0.1779 - val_accuracy: 0.9169\n",
            "Epoch 337/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9157 - val_loss: 0.1831 - val_accuracy: 0.9140\n",
            "Epoch 338/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9130 - val_loss: 0.1816 - val_accuracy: 0.9128\n",
            "Epoch 339/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9148 - val_loss: 0.1778 - val_accuracy: 0.9163\n",
            "Epoch 340/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9147 - val_loss: 0.1818 - val_accuracy: 0.9143\n",
            "Epoch 341/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9155 - val_loss: 0.1775 - val_accuracy: 0.9158\n",
            "Epoch 342/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9148 - val_loss: 0.1798 - val_accuracy: 0.9166\n",
            "Epoch 343/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9163 - val_loss: 0.1832 - val_accuracy: 0.9140\n",
            "Epoch 344/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9120 - val_loss: 0.1778 - val_accuracy: 0.9165\n",
            "Epoch 345/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9150 - val_loss: 0.1775 - val_accuracy: 0.9169\n",
            "Epoch 346/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9163 - val_loss: 0.1786 - val_accuracy: 0.9157\n",
            "Epoch 347/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9135 - val_loss: 0.1773 - val_accuracy: 0.9160\n",
            "Epoch 348/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9140 - val_loss: 0.1801 - val_accuracy: 0.9148\n",
            "Epoch 349/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9148 - val_loss: 0.1780 - val_accuracy: 0.9174\n",
            "Epoch 350/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9141 - val_loss: 0.1782 - val_accuracy: 0.9165\n",
            "Epoch 351/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9134 - val_loss: 0.1793 - val_accuracy: 0.9154\n",
            "Epoch 352/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9130 - val_loss: 0.1778 - val_accuracy: 0.9174\n",
            "Epoch 353/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9139 - val_loss: 0.1826 - val_accuracy: 0.9147\n",
            "Epoch 354/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9142 - val_loss: 0.1791 - val_accuracy: 0.9156\n",
            "Epoch 355/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9121 - val_loss: 0.1793 - val_accuracy: 0.9162\n",
            "Epoch 356/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9133 - val_loss: 0.1778 - val_accuracy: 0.9157\n",
            "Epoch 357/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9147 - val_loss: 0.1780 - val_accuracy: 0.9156\n",
            "Epoch 358/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9139 - val_loss: 0.1775 - val_accuracy: 0.9151\n",
            "Epoch 359/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9137 - val_loss: 0.1802 - val_accuracy: 0.9164\n",
            "Epoch 360/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9158 - val_loss: 0.1776 - val_accuracy: 0.9156\n",
            "Epoch 361/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9129 - val_loss: 0.1791 - val_accuracy: 0.9167\n",
            "Epoch 362/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9137 - val_loss: 0.1772 - val_accuracy: 0.9159\n",
            "Epoch 363/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9148 - val_loss: 0.1777 - val_accuracy: 0.9161\n",
            "Epoch 364/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9180 - val_loss: 0.1800 - val_accuracy: 0.9161\n",
            "Epoch 365/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9124 - val_loss: 0.1782 - val_accuracy: 0.9161\n",
            "Epoch 366/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9125 - val_loss: 0.1809 - val_accuracy: 0.9138\n",
            "Epoch 367/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9131 - val_loss: 0.1772 - val_accuracy: 0.9160\n",
            "Epoch 368/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9158 - val_loss: 0.1792 - val_accuracy: 0.9161\n",
            "Epoch 369/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9112 - val_loss: 0.1799 - val_accuracy: 0.9150\n",
            "Epoch 370/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9116 - val_loss: 0.1786 - val_accuracy: 0.9163\n",
            "Epoch 371/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9141 - val_loss: 0.1796 - val_accuracy: 0.9167\n",
            "Epoch 372/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9147 - val_loss: 0.1779 - val_accuracy: 0.9164\n",
            "Epoch 373/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9127 - val_loss: 0.1783 - val_accuracy: 0.9171\n",
            "Epoch 374/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9134 - val_loss: 0.1786 - val_accuracy: 0.9156\n",
            "Epoch 375/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9154 - val_loss: 0.1802 - val_accuracy: 0.9148\n",
            "Epoch 376/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9133 - val_loss: 0.1778 - val_accuracy: 0.9165\n",
            "Epoch 377/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9137 - val_loss: 0.1778 - val_accuracy: 0.9170\n",
            "Epoch 378/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9164 - val_loss: 0.1793 - val_accuracy: 0.9178\n",
            "Epoch 379/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9171 - val_loss: 0.1779 - val_accuracy: 0.9156\n",
            "Epoch 380/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9127 - val_loss: 0.1787 - val_accuracy: 0.9156\n",
            "Epoch 381/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9132 - val_loss: 0.1833 - val_accuracy: 0.9122\n",
            "Epoch 382/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9162 - val_loss: 0.1791 - val_accuracy: 0.9146\n",
            "Epoch 383/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9138 - val_loss: 0.1780 - val_accuracy: 0.9162\n",
            "Epoch 384/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9158 - val_loss: 0.1779 - val_accuracy: 0.9156\n",
            "Epoch 385/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9117 - val_loss: 0.1779 - val_accuracy: 0.9168\n",
            "Epoch 386/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9129 - val_loss: 0.1776 - val_accuracy: 0.9169\n",
            "Epoch 387/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9166 - val_loss: 0.1782 - val_accuracy: 0.9165\n",
            "Epoch 388/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9137 - val_loss: 0.1794 - val_accuracy: 0.9164\n",
            "Epoch 389/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9141 - val_loss: 0.1848 - val_accuracy: 0.9126\n",
            "Epoch 390/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9131 - val_loss: 0.1788 - val_accuracy: 0.9168\n",
            "Epoch 391/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9132 - val_loss: 0.1778 - val_accuracy: 0.9170\n",
            "Epoch 392/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9140 - val_loss: 0.1808 - val_accuracy: 0.9165\n",
            "Epoch 393/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9142 - val_loss: 0.1780 - val_accuracy: 0.9160\n",
            "Epoch 394/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9094 - val_loss: 0.1789 - val_accuracy: 0.9156\n",
            "Epoch 395/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9132 - val_loss: 0.1795 - val_accuracy: 0.9150\n",
            "Epoch 396/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9156 - val_loss: 0.1845 - val_accuracy: 0.9142\n",
            "Epoch 397/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9113 - val_loss: 0.1797 - val_accuracy: 0.9160\n",
            "Epoch 398/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9154 - val_loss: 0.1784 - val_accuracy: 0.9143\n",
            "Epoch 399/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9130 - val_loss: 0.1789 - val_accuracy: 0.9159\n",
            "Epoch 400/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9149 - val_loss: 0.1779 - val_accuracy: 0.9156\n",
            "Epoch 401/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9136 - val_loss: 0.1795 - val_accuracy: 0.9160\n",
            "Epoch 402/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9187 - val_loss: 0.1806 - val_accuracy: 0.9149\n",
            "Epoch 403/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9109 - val_loss: 0.1795 - val_accuracy: 0.9175\n",
            "Epoch 404/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9123 - val_loss: 0.1792 - val_accuracy: 0.9167\n",
            "Epoch 405/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9138 - val_loss: 0.1798 - val_accuracy: 0.9155\n",
            "Epoch 406/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9125 - val_loss: 0.1823 - val_accuracy: 0.9139\n",
            "Epoch 407/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9147 - val_loss: 0.1782 - val_accuracy: 0.9163\n",
            "Epoch 408/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9112 - val_loss: 0.1782 - val_accuracy: 0.9169\n",
            "Epoch 409/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9133 - val_loss: 0.1828 - val_accuracy: 0.9141\n",
            "Epoch 410/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1767 - accuracy: 0.9172 - val_loss: 0.1794 - val_accuracy: 0.9156\n",
            "Epoch 411/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9140 - val_loss: 0.1786 - val_accuracy: 0.9131\n",
            "Epoch 412/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9164 - val_loss: 0.1790 - val_accuracy: 0.9148\n",
            "Epoch 413/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9128 - val_loss: 0.1786 - val_accuracy: 0.9165\n",
            "Epoch 414/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9119 - val_loss: 0.1979 - val_accuracy: 0.9068\n",
            "Epoch 415/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9149 - val_loss: 0.1794 - val_accuracy: 0.9145\n",
            "Epoch 416/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9128 - val_loss: 0.1803 - val_accuracy: 0.9152\n",
            "Epoch 417/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9160 - val_loss: 0.1790 - val_accuracy: 0.9157\n",
            "Epoch 418/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9142 - val_loss: 0.1796 - val_accuracy: 0.9172\n",
            "Epoch 419/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9126 - val_loss: 0.1786 - val_accuracy: 0.9165\n",
            "Epoch 420/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9133 - val_loss: 0.1818 - val_accuracy: 0.9149\n",
            "Epoch 421/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9134 - val_loss: 0.1814 - val_accuracy: 0.9165\n",
            "Epoch 422/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9161 - val_loss: 0.1794 - val_accuracy: 0.9156\n",
            "Epoch 423/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9105 - val_loss: 0.1804 - val_accuracy: 0.9160\n",
            "Epoch 424/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9140 - val_loss: 0.1784 - val_accuracy: 0.9168\n",
            "Epoch 425/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9161 - val_loss: 0.1794 - val_accuracy: 0.9144\n",
            "Epoch 426/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9153 - val_loss: 0.1911 - val_accuracy: 0.9108\n",
            "Epoch 427/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9145 - val_loss: 0.1808 - val_accuracy: 0.9152\n",
            "Epoch 428/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9134 - val_loss: 0.1799 - val_accuracy: 0.9146\n",
            "Epoch 429/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9146 - val_loss: 0.1788 - val_accuracy: 0.9152\n",
            "Epoch 430/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9157 - val_loss: 0.1792 - val_accuracy: 0.9142\n",
            "Epoch 431/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1762 - accuracy: 0.9169 - val_loss: 0.1794 - val_accuracy: 0.9164\n",
            "Epoch 432/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1769 - accuracy: 0.9173 - val_loss: 0.1796 - val_accuracy: 0.9150\n",
            "Epoch 433/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9175 - val_loss: 0.1820 - val_accuracy: 0.9153\n",
            "Epoch 434/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9139 - val_loss: 0.1802 - val_accuracy: 0.9159\n",
            "Epoch 435/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9144 - val_loss: 0.1813 - val_accuracy: 0.9157\n",
            "Epoch 436/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9145 - val_loss: 0.1792 - val_accuracy: 0.9170\n",
            "Epoch 437/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9154 - val_loss: 0.1827 - val_accuracy: 0.9148\n",
            "Epoch 438/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9152 - val_loss: 0.1785 - val_accuracy: 0.9150\n",
            "Epoch 439/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9123 - val_loss: 0.1790 - val_accuracy: 0.9166\n",
            "Epoch 440/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9134 - val_loss: 0.1820 - val_accuracy: 0.9133\n",
            "Epoch 441/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1784 - accuracy: 0.9177 - val_loss: 0.1825 - val_accuracy: 0.9134\n",
            "Epoch 442/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9124 - val_loss: 0.1797 - val_accuracy: 0.9173\n",
            "Epoch 443/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9132 - val_loss: 0.1835 - val_accuracy: 0.9145\n",
            "Epoch 444/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9137 - val_loss: 0.1824 - val_accuracy: 0.9138\n",
            "Epoch 445/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9141 - val_loss: 0.1839 - val_accuracy: 0.9132\n",
            "Epoch 446/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9118 - val_loss: 0.1791 - val_accuracy: 0.9154\n",
            "Epoch 447/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9145 - val_loss: 0.1874 - val_accuracy: 0.9118\n",
            "Epoch 448/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9179 - val_loss: 0.1787 - val_accuracy: 0.9158\n",
            "Epoch 449/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9104 - val_loss: 0.1788 - val_accuracy: 0.9164\n",
            "Epoch 450/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9138 - val_loss: 0.1803 - val_accuracy: 0.9152\n",
            "Epoch 451/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9140 - val_loss: 0.1791 - val_accuracy: 0.9162\n",
            "Epoch 452/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9149 - val_loss: 0.1824 - val_accuracy: 0.9143\n",
            "Epoch 453/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9125 - val_loss: 0.1813 - val_accuracy: 0.9152\n",
            "Epoch 454/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9113 - val_loss: 0.1796 - val_accuracy: 0.9159\n",
            "Epoch 455/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9139 - val_loss: 0.1786 - val_accuracy: 0.9166\n",
            "Epoch 456/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9133 - val_loss: 0.1814 - val_accuracy: 0.9150\n",
            "Epoch 457/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9152 - val_loss: 0.1811 - val_accuracy: 0.9160\n",
            "Epoch 458/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9155 - val_loss: 0.1792 - val_accuracy: 0.9153\n",
            "Epoch 459/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9152 - val_loss: 0.1843 - val_accuracy: 0.9124\n",
            "Epoch 460/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9143 - val_loss: 0.1885 - val_accuracy: 0.9101\n",
            "Epoch 461/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9154 - val_loss: 0.1792 - val_accuracy: 0.9163\n",
            "Epoch 462/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9113 - val_loss: 0.1799 - val_accuracy: 0.9140\n",
            "Epoch 463/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9125 - val_loss: 0.1841 - val_accuracy: 0.9143\n",
            "Epoch 464/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9125 - val_loss: 0.1789 - val_accuracy: 0.9159\n",
            "Epoch 465/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9140 - val_loss: 0.1783 - val_accuracy: 0.9165\n",
            "Epoch 466/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9122 - val_loss: 0.1806 - val_accuracy: 0.9149\n",
            "Epoch 467/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9149 - val_loss: 0.1793 - val_accuracy: 0.9157\n",
            "Epoch 468/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1770 - accuracy: 0.9166 - val_loss: 0.1792 - val_accuracy: 0.9162\n",
            "Epoch 469/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9133 - val_loss: 0.1971 - val_accuracy: 0.9069\n",
            "Epoch 470/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9144 - val_loss: 0.1790 - val_accuracy: 0.9155\n",
            "Epoch 471/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9124 - val_loss: 0.1790 - val_accuracy: 0.9166\n",
            "Epoch 472/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9121 - val_loss: 0.1835 - val_accuracy: 0.9145\n",
            "Epoch 473/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9140 - val_loss: 0.1796 - val_accuracy: 0.9152\n",
            "Epoch 474/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9143 - val_loss: 0.1802 - val_accuracy: 0.9144\n",
            "Epoch 475/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9144 - val_loss: 0.1785 - val_accuracy: 0.9163\n",
            "Epoch 476/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9144 - val_loss: 0.1817 - val_accuracy: 0.9148\n",
            "Epoch 477/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9153 - val_loss: 0.1814 - val_accuracy: 0.9147\n",
            "Epoch 478/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9132 - val_loss: 0.1804 - val_accuracy: 0.9152\n",
            "Epoch 479/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9163 - val_loss: 0.1790 - val_accuracy: 0.9171\n",
            "Epoch 480/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9149 - val_loss: 0.1814 - val_accuracy: 0.9164\n",
            "Epoch 481/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9144 - val_loss: 0.1790 - val_accuracy: 0.9165\n",
            "Epoch 482/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9141 - val_loss: 0.1814 - val_accuracy: 0.9160\n",
            "Epoch 483/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9148 - val_loss: 0.1788 - val_accuracy: 0.9143\n",
            "Epoch 484/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9156 - val_loss: 0.1849 - val_accuracy: 0.9155\n",
            "Epoch 485/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9153 - val_loss: 0.1796 - val_accuracy: 0.9172\n",
            "Epoch 486/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1800 - accuracy: 0.9167 - val_loss: 0.1798 - val_accuracy: 0.9162\n",
            "Epoch 487/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9144 - val_loss: 0.1789 - val_accuracy: 0.9157\n",
            "Epoch 488/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9152 - val_loss: 0.1810 - val_accuracy: 0.9157\n",
            "Epoch 489/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9137 - val_loss: 0.1790 - val_accuracy: 0.9158\n",
            "Epoch 490/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9108 - val_loss: 0.1788 - val_accuracy: 0.9157\n",
            "Epoch 491/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9140 - val_loss: 0.1802 - val_accuracy: 0.9185\n",
            "Epoch 492/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1798 - accuracy: 0.9142 - val_loss: 0.1794 - val_accuracy: 0.9165\n",
            "Epoch 493/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9117 - val_loss: 0.1796 - val_accuracy: 0.9159\n",
            "Epoch 494/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9155 - val_loss: 0.1789 - val_accuracy: 0.9163\n",
            "Epoch 495/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9135 - val_loss: 0.1796 - val_accuracy: 0.9152\n",
            "Epoch 496/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9153 - val_loss: 0.1820 - val_accuracy: 0.9138\n",
            "Epoch 497/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9143 - val_loss: 0.1787 - val_accuracy: 0.9164\n",
            "Epoch 498/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9122 - val_loss: 0.1806 - val_accuracy: 0.9160\n",
            "Epoch 499/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9131 - val_loss: 0.1837 - val_accuracy: 0.9156\n",
            "Epoch 500/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9118 - val_loss: 0.1801 - val_accuracy: 0.9160\n",
            "Epoch 501/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9173 - val_loss: 0.1811 - val_accuracy: 0.9160\n",
            "Epoch 502/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9161 - val_loss: 0.1794 - val_accuracy: 0.9163\n",
            "Epoch 503/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9142 - val_loss: 0.1820 - val_accuracy: 0.9169\n",
            "Epoch 504/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9145 - val_loss: 0.1796 - val_accuracy: 0.9169\n",
            "Epoch 505/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9156 - val_loss: 0.1794 - val_accuracy: 0.9156\n",
            "Epoch 506/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9142 - val_loss: 0.1789 - val_accuracy: 0.9153\n",
            "Epoch 507/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9135 - val_loss: 0.1804 - val_accuracy: 0.9160\n",
            "Epoch 508/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9155 - val_loss: 0.1826 - val_accuracy: 0.9162\n",
            "Epoch 509/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9161 - val_loss: 0.1791 - val_accuracy: 0.9165\n",
            "Epoch 510/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9150 - val_loss: 0.1831 - val_accuracy: 0.9147\n",
            "Epoch 511/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1772 - accuracy: 0.9189 - val_loss: 0.1788 - val_accuracy: 0.9160\n",
            "Epoch 512/512\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9119 - val_loss: 0.1789 - val_accuracy: 0.9155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CxCqnloWS9sk",
        "outputId": "c19d561e-058c-45ef-feb8-9011801955f8"
      },
      "source": [
        "model_r2 = Sequential()\n",
        "model_r2.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_r2.add(Dense(4,activation='relu'))\n",
        "model_r2.add(Dense(1,activation='sigmoid'))\n",
        "model_r2.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "model_r2.summary()\n",
        "history = model_r2.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_52 (Dense)             (None, 8)                 168       \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 209\n",
            "Trainable params: 209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/256\n",
            "901/901 [==============================] - 3s 2ms/step - loss: 0.4086 - accuracy: 0.8832 - val_loss: 0.2872 - val_accuracy: 0.8928\n",
            "Epoch 2/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2997 - accuracy: 0.8849 - val_loss: 0.2661 - val_accuracy: 0.9020\n",
            "Epoch 3/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2774 - accuracy: 0.8959 - val_loss: 0.2429 - val_accuracy: 0.9067\n",
            "Epoch 4/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2510 - accuracy: 0.9014 - val_loss: 0.2248 - val_accuracy: 0.9101\n",
            "Epoch 5/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2274 - accuracy: 0.9081 - val_loss: 0.2096 - val_accuracy: 0.9136\n",
            "Epoch 6/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2182 - accuracy: 0.9078 - val_loss: 0.2043 - val_accuracy: 0.9139\n",
            "Epoch 7/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2136 - accuracy: 0.9110 - val_loss: 0.1999 - val_accuracy: 0.9146\n",
            "Epoch 8/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2096 - accuracy: 0.9100 - val_loss: 0.1985 - val_accuracy: 0.9170\n",
            "Epoch 9/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2110 - accuracy: 0.9082 - val_loss: 0.1960 - val_accuracy: 0.9169\n",
            "Epoch 10/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2036 - accuracy: 0.9126 - val_loss: 0.1943 - val_accuracy: 0.9149\n",
            "Epoch 11/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2113 - accuracy: 0.9079 - val_loss: 0.1938 - val_accuracy: 0.9150\n",
            "Epoch 12/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2037 - accuracy: 0.9100 - val_loss: 0.1932 - val_accuracy: 0.9152\n",
            "Epoch 13/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2070 - accuracy: 0.9098 - val_loss: 0.1925 - val_accuracy: 0.9153\n",
            "Epoch 14/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2045 - accuracy: 0.9088 - val_loss: 0.1919 - val_accuracy: 0.9158\n",
            "Epoch 15/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2036 - accuracy: 0.9104 - val_loss: 0.1921 - val_accuracy: 0.9136\n",
            "Epoch 16/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9098 - val_loss: 0.1917 - val_accuracy: 0.9133\n",
            "Epoch 17/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2052 - accuracy: 0.9077 - val_loss: 0.1946 - val_accuracy: 0.9105\n",
            "Epoch 18/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9102 - val_loss: 0.1906 - val_accuracy: 0.9143\n",
            "Epoch 19/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1995 - accuracy: 0.9089 - val_loss: 0.1909 - val_accuracy: 0.9135\n",
            "Epoch 20/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2000 - accuracy: 0.9111 - val_loss: 0.1905 - val_accuracy: 0.9148\n",
            "Epoch 21/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9106 - val_loss: 0.1897 - val_accuracy: 0.9129\n",
            "Epoch 22/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9119 - val_loss: 0.1922 - val_accuracy: 0.9109\n",
            "Epoch 23/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9105 - val_loss: 0.1898 - val_accuracy: 0.9129\n",
            "Epoch 24/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2000 - accuracy: 0.9117 - val_loss: 0.1894 - val_accuracy: 0.9133\n",
            "Epoch 25/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9142 - val_loss: 0.1900 - val_accuracy: 0.9120\n",
            "Epoch 26/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1930 - accuracy: 0.9134 - val_loss: 0.1920 - val_accuracy: 0.9118\n",
            "Epoch 27/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2025 - accuracy: 0.9059 - val_loss: 0.1894 - val_accuracy: 0.9145\n",
            "Epoch 28/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2034 - accuracy: 0.9091 - val_loss: 0.1891 - val_accuracy: 0.9146\n",
            "Epoch 29/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9091 - val_loss: 0.1880 - val_accuracy: 0.9146\n",
            "Epoch 30/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9119 - val_loss: 0.1880 - val_accuracy: 0.9154\n",
            "Epoch 31/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9114 - val_loss: 0.1880 - val_accuracy: 0.9156\n",
            "Epoch 32/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9116 - val_loss: 0.1880 - val_accuracy: 0.9152\n",
            "Epoch 33/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9090 - val_loss: 0.1879 - val_accuracy: 0.9158\n",
            "Epoch 34/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9108 - val_loss: 0.1891 - val_accuracy: 0.9136\n",
            "Epoch 35/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9106 - val_loss: 0.1935 - val_accuracy: 0.9102\n",
            "Epoch 36/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9119 - val_loss: 0.1877 - val_accuracy: 0.9140\n",
            "Epoch 37/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9153 - val_loss: 0.1884 - val_accuracy: 0.9136\n",
            "Epoch 38/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9085 - val_loss: 0.1871 - val_accuracy: 0.9147\n",
            "Epoch 39/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9074 - val_loss: 0.1920 - val_accuracy: 0.9157\n",
            "Epoch 40/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9074 - val_loss: 0.1867 - val_accuracy: 0.9159\n",
            "Epoch 41/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9093 - val_loss: 0.1872 - val_accuracy: 0.9152\n",
            "Epoch 42/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9137 - val_loss: 0.1874 - val_accuracy: 0.9144\n",
            "Epoch 43/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9106 - val_loss: 0.1860 - val_accuracy: 0.9151\n",
            "Epoch 44/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2032 - accuracy: 0.9082 - val_loss: 0.1864 - val_accuracy: 0.9150\n",
            "Epoch 45/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9126 - val_loss: 0.1867 - val_accuracy: 0.9146\n",
            "Epoch 46/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9094 - val_loss: 0.1867 - val_accuracy: 0.9146\n",
            "Epoch 47/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9110 - val_loss: 0.1858 - val_accuracy: 0.9158\n",
            "Epoch 48/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9086 - val_loss: 0.1875 - val_accuracy: 0.9125\n",
            "Epoch 49/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9118 - val_loss: 0.1858 - val_accuracy: 0.9160\n",
            "Epoch 50/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9112 - val_loss: 0.1917 - val_accuracy: 0.9109\n",
            "Epoch 51/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9135 - val_loss: 0.1862 - val_accuracy: 0.9145\n",
            "Epoch 52/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9106 - val_loss: 0.1861 - val_accuracy: 0.9142\n",
            "Epoch 53/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9110 - val_loss: 0.1855 - val_accuracy: 0.9163\n",
            "Epoch 54/256\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1939 - accuracy: 0.9084 - val_loss: 0.1858 - val_accuracy: 0.9156\n",
            "Epoch 55/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9081 - val_loss: 0.1858 - val_accuracy: 0.9166\n",
            "Epoch 56/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9100 - val_loss: 0.1860 - val_accuracy: 0.9150\n",
            "Epoch 57/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9111 - val_loss: 0.1857 - val_accuracy: 0.9154\n",
            "Epoch 58/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9123 - val_loss: 0.1855 - val_accuracy: 0.9156\n",
            "Epoch 59/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9111 - val_loss: 0.1867 - val_accuracy: 0.9143\n",
            "Epoch 60/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1970 - accuracy: 0.9095 - val_loss: 0.1854 - val_accuracy: 0.9167\n",
            "Epoch 61/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9101 - val_loss: 0.1846 - val_accuracy: 0.9166\n",
            "Epoch 62/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9094 - val_loss: 0.1853 - val_accuracy: 0.9152\n",
            "Epoch 63/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9112 - val_loss: 0.1853 - val_accuracy: 0.9145\n",
            "Epoch 64/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9089 - val_loss: 0.1848 - val_accuracy: 0.9169\n",
            "Epoch 65/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9078 - val_loss: 0.1845 - val_accuracy: 0.9169\n",
            "Epoch 66/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9114 - val_loss: 0.1854 - val_accuracy: 0.9160\n",
            "Epoch 67/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2004 - accuracy: 0.9067 - val_loss: 0.1839 - val_accuracy: 0.9162\n",
            "Epoch 68/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9087 - val_loss: 0.1845 - val_accuracy: 0.9160\n",
            "Epoch 69/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9117 - val_loss: 0.1841 - val_accuracy: 0.9158\n",
            "Epoch 70/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9095 - val_loss: 0.1841 - val_accuracy: 0.9151\n",
            "Epoch 71/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9103 - val_loss: 0.1841 - val_accuracy: 0.9152\n",
            "Epoch 72/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9121 - val_loss: 0.1867 - val_accuracy: 0.9159\n",
            "Epoch 73/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9113 - val_loss: 0.1836 - val_accuracy: 0.9149\n",
            "Epoch 74/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9108 - val_loss: 0.1837 - val_accuracy: 0.9163\n",
            "Epoch 75/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9116 - val_loss: 0.1845 - val_accuracy: 0.9148\n",
            "Epoch 76/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9120 - val_loss: 0.1842 - val_accuracy: 0.9162\n",
            "Epoch 77/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9102 - val_loss: 0.1858 - val_accuracy: 0.9133\n",
            "Epoch 78/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9112 - val_loss: 0.1867 - val_accuracy: 0.9130\n",
            "Epoch 79/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9135 - val_loss: 0.1830 - val_accuracy: 0.9158\n",
            "Epoch 80/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9121 - val_loss: 0.1840 - val_accuracy: 0.9142\n",
            "Epoch 81/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9127 - val_loss: 0.1882 - val_accuracy: 0.9114\n",
            "Epoch 82/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9098 - val_loss: 0.1850 - val_accuracy: 0.9165\n",
            "Epoch 83/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9114 - val_loss: 0.1880 - val_accuracy: 0.9126\n",
            "Epoch 84/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9124 - val_loss: 0.1842 - val_accuracy: 0.9139\n",
            "Epoch 85/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9138 - val_loss: 0.1873 - val_accuracy: 0.9124\n",
            "Epoch 86/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9109 - val_loss: 0.1846 - val_accuracy: 0.9132\n",
            "Epoch 87/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9128 - val_loss: 0.1826 - val_accuracy: 0.9163\n",
            "Epoch 88/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9117 - val_loss: 0.1828 - val_accuracy: 0.9167\n",
            "Epoch 89/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9103 - val_loss: 0.1819 - val_accuracy: 0.9164\n",
            "Epoch 90/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9093 - val_loss: 0.1829 - val_accuracy: 0.9147\n",
            "Epoch 91/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9122 - val_loss: 0.1825 - val_accuracy: 0.9160\n",
            "Epoch 92/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9086 - val_loss: 0.1822 - val_accuracy: 0.9170\n",
            "Epoch 93/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9134 - val_loss: 0.1885 - val_accuracy: 0.9110\n",
            "Epoch 94/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9124 - val_loss: 0.1821 - val_accuracy: 0.9161\n",
            "Epoch 95/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9096 - val_loss: 0.1832 - val_accuracy: 0.9164\n",
            "Epoch 96/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9136 - val_loss: 0.1833 - val_accuracy: 0.9139\n",
            "Epoch 97/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9134 - val_loss: 0.1829 - val_accuracy: 0.9155\n",
            "Epoch 98/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9138 - val_loss: 0.1852 - val_accuracy: 0.9170\n",
            "Epoch 99/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9114 - val_loss: 0.1825 - val_accuracy: 0.9148\n",
            "Epoch 100/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9124 - val_loss: 0.1830 - val_accuracy: 0.9151\n",
            "Epoch 101/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9132 - val_loss: 0.1819 - val_accuracy: 0.9155\n",
            "Epoch 102/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9141 - val_loss: 0.1838 - val_accuracy: 0.9148\n",
            "Epoch 103/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9095 - val_loss: 0.1828 - val_accuracy: 0.9152\n",
            "Epoch 104/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9136 - val_loss: 0.1813 - val_accuracy: 0.9166\n",
            "Epoch 105/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9099 - val_loss: 0.1849 - val_accuracy: 0.9133\n",
            "Epoch 106/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9104 - val_loss: 0.1826 - val_accuracy: 0.9165\n",
            "Epoch 107/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9136 - val_loss: 0.1820 - val_accuracy: 0.9151\n",
            "Epoch 108/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9124 - val_loss: 0.1824 - val_accuracy: 0.9155\n",
            "Epoch 109/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9138 - val_loss: 0.1821 - val_accuracy: 0.9159\n",
            "Epoch 110/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9083 - val_loss: 0.1879 - val_accuracy: 0.9101\n",
            "Epoch 111/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9112 - val_loss: 0.1826 - val_accuracy: 0.9135\n",
            "Epoch 112/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9130 - val_loss: 0.1823 - val_accuracy: 0.9169\n",
            "Epoch 113/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9126 - val_loss: 0.1839 - val_accuracy: 0.9153\n",
            "Epoch 114/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9110 - val_loss: 0.1817 - val_accuracy: 0.9150\n",
            "Epoch 115/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9095 - val_loss: 0.1894 - val_accuracy: 0.9143\n",
            "Epoch 116/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9110 - val_loss: 0.1839 - val_accuracy: 0.9126\n",
            "Epoch 117/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9113 - val_loss: 0.1825 - val_accuracy: 0.9145\n",
            "Epoch 118/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9094 - val_loss: 0.1820 - val_accuracy: 0.9169\n",
            "Epoch 119/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9148 - val_loss: 0.1820 - val_accuracy: 0.9153\n",
            "Epoch 120/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9125 - val_loss: 0.1817 - val_accuracy: 0.9168\n",
            "Epoch 121/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9115 - val_loss: 0.1830 - val_accuracy: 0.9142\n",
            "Epoch 122/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9093 - val_loss: 0.1829 - val_accuracy: 0.9135\n",
            "Epoch 123/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9126 - val_loss: 0.1882 - val_accuracy: 0.9105\n",
            "Epoch 124/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9124 - val_loss: 0.1816 - val_accuracy: 0.9160\n",
            "Epoch 125/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9102 - val_loss: 0.1819 - val_accuracy: 0.9158\n",
            "Epoch 126/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9153 - val_loss: 0.1817 - val_accuracy: 0.9154\n",
            "Epoch 127/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9079 - val_loss: 0.1818 - val_accuracy: 0.9162\n",
            "Epoch 128/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9132 - val_loss: 0.1820 - val_accuracy: 0.9147\n",
            "Epoch 129/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9089 - val_loss: 0.1819 - val_accuracy: 0.9166\n",
            "Epoch 130/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9107 - val_loss: 0.1820 - val_accuracy: 0.9156\n",
            "Epoch 131/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9086 - val_loss: 0.1824 - val_accuracy: 0.9156\n",
            "Epoch 132/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9139 - val_loss: 0.1839 - val_accuracy: 0.9152\n",
            "Epoch 133/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9128 - val_loss: 0.1825 - val_accuracy: 0.9148\n",
            "Epoch 134/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9095 - val_loss: 0.1826 - val_accuracy: 0.9166\n",
            "Epoch 135/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9122 - val_loss: 0.1832 - val_accuracy: 0.9144\n",
            "Epoch 136/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9081 - val_loss: 0.1845 - val_accuracy: 0.9143\n",
            "Epoch 137/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9098 - val_loss: 0.1814 - val_accuracy: 0.9153\n",
            "Epoch 138/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9088 - val_loss: 0.1842 - val_accuracy: 0.9156\n",
            "Epoch 139/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9117 - val_loss: 0.1834 - val_accuracy: 0.9161\n",
            "Epoch 140/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9108 - val_loss: 0.1821 - val_accuracy: 0.9160\n",
            "Epoch 141/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9125 - val_loss: 0.1842 - val_accuracy: 0.9140\n",
            "Epoch 142/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9105 - val_loss: 0.1819 - val_accuracy: 0.9172\n",
            "Epoch 143/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9114 - val_loss: 0.1820 - val_accuracy: 0.9165\n",
            "Epoch 144/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9120 - val_loss: 0.1817 - val_accuracy: 0.9157\n",
            "Epoch 145/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9113 - val_loss: 0.1834 - val_accuracy: 0.9137\n",
            "Epoch 146/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9113 - val_loss: 0.1819 - val_accuracy: 0.9159\n",
            "Epoch 147/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9130 - val_loss: 0.1815 - val_accuracy: 0.9151\n",
            "Epoch 148/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9101 - val_loss: 0.1827 - val_accuracy: 0.9168\n",
            "Epoch 149/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9144 - val_loss: 0.1835 - val_accuracy: 0.9135\n",
            "Epoch 150/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9144 - val_loss: 0.1828 - val_accuracy: 0.9176\n",
            "Epoch 151/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9117 - val_loss: 0.1822 - val_accuracy: 0.9162\n",
            "Epoch 152/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9122 - val_loss: 0.1818 - val_accuracy: 0.9143\n",
            "Epoch 153/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9113 - val_loss: 0.1837 - val_accuracy: 0.9161\n",
            "Epoch 154/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9133 - val_loss: 0.1820 - val_accuracy: 0.9142\n",
            "Epoch 155/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9121 - val_loss: 0.1835 - val_accuracy: 0.9156\n",
            "Epoch 156/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9136 - val_loss: 0.1815 - val_accuracy: 0.9165\n",
            "Epoch 157/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9115 - val_loss: 0.1826 - val_accuracy: 0.9158\n",
            "Epoch 158/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9111 - val_loss: 0.1824 - val_accuracy: 0.9158\n",
            "Epoch 159/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9113 - val_loss: 0.1848 - val_accuracy: 0.9131\n",
            "Epoch 160/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9122 - val_loss: 0.1859 - val_accuracy: 0.9155\n",
            "Epoch 161/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9105 - val_loss: 0.1832 - val_accuracy: 0.9162\n",
            "Epoch 162/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9122 - val_loss: 0.1855 - val_accuracy: 0.9128\n",
            "Epoch 163/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9112 - val_loss: 0.1820 - val_accuracy: 0.9157\n",
            "Epoch 164/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9122 - val_loss: 0.1830 - val_accuracy: 0.9155\n",
            "Epoch 165/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9126 - val_loss: 0.1820 - val_accuracy: 0.9154\n",
            "Epoch 166/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9094 - val_loss: 0.1854 - val_accuracy: 0.9158\n",
            "Epoch 167/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9109 - val_loss: 0.1822 - val_accuracy: 0.9153\n",
            "Epoch 168/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9130 - val_loss: 0.1824 - val_accuracy: 0.9159\n",
            "Epoch 169/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9130 - val_loss: 0.1842 - val_accuracy: 0.9156\n",
            "Epoch 170/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9122 - val_loss: 0.1837 - val_accuracy: 0.9135\n",
            "Epoch 171/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9126 - val_loss: 0.1831 - val_accuracy: 0.9135\n",
            "Epoch 172/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9118 - val_loss: 0.1836 - val_accuracy: 0.9135\n",
            "Epoch 173/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9137 - val_loss: 0.1826 - val_accuracy: 0.9156\n",
            "Epoch 174/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9116 - val_loss: 0.1823 - val_accuracy: 0.9156\n",
            "Epoch 175/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9100 - val_loss: 0.1845 - val_accuracy: 0.9140\n",
            "Epoch 176/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9100 - val_loss: 0.1823 - val_accuracy: 0.9165\n",
            "Epoch 177/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9129 - val_loss: 0.1833 - val_accuracy: 0.9155\n",
            "Epoch 178/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9127 - val_loss: 0.1829 - val_accuracy: 0.9152\n",
            "Epoch 179/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9118 - val_loss: 0.1836 - val_accuracy: 0.9156\n",
            "Epoch 180/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9126 - val_loss: 0.1824 - val_accuracy: 0.9152\n",
            "Epoch 181/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9123 - val_loss: 0.1846 - val_accuracy: 0.9144\n",
            "Epoch 182/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9130 - val_loss: 0.1820 - val_accuracy: 0.9163\n",
            "Epoch 183/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9109 - val_loss: 0.1853 - val_accuracy: 0.9142\n",
            "Epoch 184/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9128 - val_loss: 0.1836 - val_accuracy: 0.9158\n",
            "Epoch 185/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9129 - val_loss: 0.1828 - val_accuracy: 0.9163\n",
            "Epoch 186/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9134 - val_loss: 0.1829 - val_accuracy: 0.9146\n",
            "Epoch 187/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9137 - val_loss: 0.1843 - val_accuracy: 0.9129\n",
            "Epoch 188/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9092 - val_loss: 0.1821 - val_accuracy: 0.9165\n",
            "Epoch 189/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9101 - val_loss: 0.1852 - val_accuracy: 0.9150\n",
            "Epoch 190/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9120 - val_loss: 0.1839 - val_accuracy: 0.9163\n",
            "Epoch 191/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9131 - val_loss: 0.1832 - val_accuracy: 0.9148\n",
            "Epoch 192/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9157 - val_loss: 0.1818 - val_accuracy: 0.9156\n",
            "Epoch 193/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9132 - val_loss: 0.1852 - val_accuracy: 0.9135\n",
            "Epoch 194/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9107 - val_loss: 0.1838 - val_accuracy: 0.9135\n",
            "Epoch 195/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9113 - val_loss: 0.1875 - val_accuracy: 0.9126\n",
            "Epoch 196/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9129 - val_loss: 0.1866 - val_accuracy: 0.9127\n",
            "Epoch 197/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9155 - val_loss: 0.1842 - val_accuracy: 0.9157\n",
            "Epoch 198/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9112 - val_loss: 0.1815 - val_accuracy: 0.9173\n",
            "Epoch 199/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9120 - val_loss: 0.1828 - val_accuracy: 0.9159\n",
            "Epoch 200/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9111 - val_loss: 0.1849 - val_accuracy: 0.9148\n",
            "Epoch 201/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9132 - val_loss: 0.1824 - val_accuracy: 0.9156\n",
            "Epoch 202/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9152 - val_loss: 0.1816 - val_accuracy: 0.9164\n",
            "Epoch 203/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9132 - val_loss: 0.1833 - val_accuracy: 0.9142\n",
            "Epoch 204/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9122 - val_loss: 0.1830 - val_accuracy: 0.9159\n",
            "Epoch 205/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9119 - val_loss: 0.1832 - val_accuracy: 0.9135\n",
            "Epoch 206/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9115 - val_loss: 0.1834 - val_accuracy: 0.9145\n",
            "Epoch 207/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9113 - val_loss: 0.1824 - val_accuracy: 0.9160\n",
            "Epoch 208/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9138 - val_loss: 0.1829 - val_accuracy: 0.9156\n",
            "Epoch 209/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9116 - val_loss: 0.1829 - val_accuracy: 0.9159\n",
            "Epoch 210/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9117 - val_loss: 0.1821 - val_accuracy: 0.9132\n",
            "Epoch 211/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9091 - val_loss: 0.1821 - val_accuracy: 0.9160\n",
            "Epoch 212/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9108 - val_loss: 0.1825 - val_accuracy: 0.9152\n",
            "Epoch 213/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9099 - val_loss: 0.1828 - val_accuracy: 0.9135\n",
            "Epoch 214/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9111 - val_loss: 0.1829 - val_accuracy: 0.9138\n",
            "Epoch 215/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9124 - val_loss: 0.1817 - val_accuracy: 0.9162\n",
            "Epoch 216/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9124 - val_loss: 0.1815 - val_accuracy: 0.9152\n",
            "Epoch 217/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9120 - val_loss: 0.1820 - val_accuracy: 0.9161\n",
            "Epoch 218/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9102 - val_loss: 0.1850 - val_accuracy: 0.9127\n",
            "Epoch 219/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9125 - val_loss: 0.1825 - val_accuracy: 0.9149\n",
            "Epoch 220/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9124 - val_loss: 0.1821 - val_accuracy: 0.9161\n",
            "Epoch 221/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9124 - val_loss: 0.1850 - val_accuracy: 0.9151\n",
            "Epoch 222/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9136 - val_loss: 0.1822 - val_accuracy: 0.9150\n",
            "Epoch 223/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9114 - val_loss: 0.1823 - val_accuracy: 0.9146\n",
            "Epoch 224/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9107 - val_loss: 0.1859 - val_accuracy: 0.9120\n",
            "Epoch 225/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9108 - val_loss: 0.1842 - val_accuracy: 0.9147\n",
            "Epoch 226/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9077 - val_loss: 0.1865 - val_accuracy: 0.9113\n",
            "Epoch 227/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9148 - val_loss: 0.1846 - val_accuracy: 0.9143\n",
            "Epoch 228/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9142 - val_loss: 0.1817 - val_accuracy: 0.9148\n",
            "Epoch 229/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9094 - val_loss: 0.1816 - val_accuracy: 0.9152\n",
            "Epoch 230/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9103 - val_loss: 0.1846 - val_accuracy: 0.9140\n",
            "Epoch 231/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9104 - val_loss: 0.1817 - val_accuracy: 0.9167\n",
            "Epoch 232/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9086 - val_loss: 0.1836 - val_accuracy: 0.9160\n",
            "Epoch 233/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9147 - val_loss: 0.1834 - val_accuracy: 0.9142\n",
            "Epoch 234/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9152 - val_loss: 0.1843 - val_accuracy: 0.9120\n",
            "Epoch 235/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9112 - val_loss: 0.1829 - val_accuracy: 0.9136\n",
            "Epoch 236/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9129 - val_loss: 0.1840 - val_accuracy: 0.9161\n",
            "Epoch 237/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9113 - val_loss: 0.1840 - val_accuracy: 0.9153\n",
            "Epoch 238/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9126 - val_loss: 0.1869 - val_accuracy: 0.9126\n",
            "Epoch 239/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9107 - val_loss: 0.1829 - val_accuracy: 0.9154\n",
            "Epoch 240/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9157 - val_loss: 0.1828 - val_accuracy: 0.9143\n",
            "Epoch 241/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9135 - val_loss: 0.1829 - val_accuracy: 0.9156\n",
            "Epoch 242/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9124 - val_loss: 0.1825 - val_accuracy: 0.9145\n",
            "Epoch 243/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9126 - val_loss: 0.1831 - val_accuracy: 0.9129\n",
            "Epoch 244/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9102 - val_loss: 0.1819 - val_accuracy: 0.9156\n",
            "Epoch 245/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9138 - val_loss: 0.1817 - val_accuracy: 0.9155\n",
            "Epoch 246/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9148 - val_loss: 0.1821 - val_accuracy: 0.9151\n",
            "Epoch 247/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9106 - val_loss: 0.1829 - val_accuracy: 0.9168\n",
            "Epoch 248/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9135 - val_loss: 0.1819 - val_accuracy: 0.9149\n",
            "Epoch 249/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9118 - val_loss: 0.1844 - val_accuracy: 0.9150\n",
            "Epoch 250/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9123 - val_loss: 0.1822 - val_accuracy: 0.9163\n",
            "Epoch 251/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9124 - val_loss: 0.1838 - val_accuracy: 0.9146\n",
            "Epoch 252/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9124 - val_loss: 0.1839 - val_accuracy: 0.9118\n",
            "Epoch 253/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9106 - val_loss: 0.1821 - val_accuracy: 0.9156\n",
            "Epoch 254/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9152 - val_loss: 0.1844 - val_accuracy: 0.9151\n",
            "Epoch 255/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9097 - val_loss: 0.1820 - val_accuracy: 0.9153\n",
            "Epoch 256/256\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9141 - val_loss: 0.1819 - val_accuracy: 0.9157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8eqwqPwxBdhO",
        "outputId": "122d04a9-54fc-4527-cd56-e0988242e624"
      },
      "source": [
        "model_r5 = Sequential()\n",
        "model_r5.add(Dense(32, input_dim = 20,activation='relu'))\n",
        "model_r5.add(Dense(16,activation='relu'))\n",
        "model_r5.add(Dense(8,activation='relu'))\n",
        "model_r5.add(Dense(4,activation='relu'))\n",
        "model_r5.add(Dense(1,activation='sigmoid'))\n",
        "model_r5.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "model_r5.summary()\n",
        "history = model_r5.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_34 (Dense)             (None, 32)                672       \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 1,377\n",
            "Trainable params: 1,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.6158 - accuracy: 0.8512 - val_loss: 0.4268 - val_accuracy: 0.9050\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3960 - accuracy: 0.8994 - val_loss: 0.2986 - val_accuracy: 0.9075\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2910 - accuracy: 0.9037 - val_loss: 0.2440 - val_accuracy: 0.9101\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2447 - accuracy: 0.9091 - val_loss: 0.2246 - val_accuracy: 0.9094\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2262 - accuracy: 0.9102 - val_loss: 0.2279 - val_accuracy: 0.9096\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2203 - accuracy: 0.9082 - val_loss: 0.2086 - val_accuracy: 0.9111\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2175 - accuracy: 0.9083 - val_loss: 0.2206 - val_accuracy: 0.9070\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2127 - accuracy: 0.9077 - val_loss: 0.2004 - val_accuracy: 0.9118\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2112 - accuracy: 0.9084 - val_loss: 0.1988 - val_accuracy: 0.9109\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2083 - accuracy: 0.9085 - val_loss: 0.1994 - val_accuracy: 0.9121\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2032 - accuracy: 0.9090 - val_loss: 0.1979 - val_accuracy: 0.9102\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9112 - val_loss: 0.1931 - val_accuracy: 0.9125\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9110 - val_loss: 0.1964 - val_accuracy: 0.9107\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2048 - accuracy: 0.9076 - val_loss: 0.1896 - val_accuracy: 0.9118\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9130 - val_loss: 0.1994 - val_accuracy: 0.9116\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9095 - val_loss: 0.1935 - val_accuracy: 0.9107\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9121 - val_loss: 0.1924 - val_accuracy: 0.9117\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9116 - val_loss: 0.1913 - val_accuracy: 0.9107\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9116 - val_loss: 0.1866 - val_accuracy: 0.9133\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9127 - val_loss: 0.1878 - val_accuracy: 0.9112\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9141 - val_loss: 0.1863 - val_accuracy: 0.9119\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9137 - val_loss: 0.1918 - val_accuracy: 0.9087\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9137 - val_loss: 0.1994 - val_accuracy: 0.9092\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9112 - val_loss: 0.1871 - val_accuracy: 0.9128\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9106 - val_loss: 0.1863 - val_accuracy: 0.9126\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9107 - val_loss: 0.1849 - val_accuracy: 0.9116\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9102 - val_loss: 0.2005 - val_accuracy: 0.9117\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9108 - val_loss: 0.1898 - val_accuracy: 0.9119\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9102 - val_loss: 0.1866 - val_accuracy: 0.9126\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9168 - val_loss: 0.1831 - val_accuracy: 0.9118\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9145 - val_loss: 0.1865 - val_accuracy: 0.9130\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9145 - val_loss: 0.1846 - val_accuracy: 0.9114\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9135 - val_loss: 0.1886 - val_accuracy: 0.9105\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9107 - val_loss: 0.1958 - val_accuracy: 0.9077\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9130 - val_loss: 0.2014 - val_accuracy: 0.9121\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9121 - val_loss: 0.1903 - val_accuracy: 0.9092\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9149 - val_loss: 0.1845 - val_accuracy: 0.9128\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9119 - val_loss: 0.1846 - val_accuracy: 0.9124\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9163 - val_loss: 0.1834 - val_accuracy: 0.9114\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9133 - val_loss: 0.1853 - val_accuracy: 0.9105\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9099 - val_loss: 0.1861 - val_accuracy: 0.9126\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9104 - val_loss: 0.1863 - val_accuracy: 0.9125\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9133 - val_loss: 0.1970 - val_accuracy: 0.9144\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9118 - val_loss: 0.1848 - val_accuracy: 0.9110\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9145 - val_loss: 0.1839 - val_accuracy: 0.9134\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9091 - val_loss: 0.1912 - val_accuracy: 0.9106\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9131 - val_loss: 0.1825 - val_accuracy: 0.9114\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9133 - val_loss: 0.1889 - val_accuracy: 0.9109\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.1926 - accuracy: 0.9104 - val_loss: 0.1920 - val_accuracy: 0.9061\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9137 - val_loss: 0.1848 - val_accuracy: 0.9118\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9118 - val_loss: 0.1886 - val_accuracy: 0.9107\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9116 - val_loss: 0.1850 - val_accuracy: 0.9128\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9146 - val_loss: 0.1850 - val_accuracy: 0.9089\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9132 - val_loss: 0.1861 - val_accuracy: 0.9122\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9170 - val_loss: 0.1884 - val_accuracy: 0.9102\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9097 - val_loss: 0.1869 - val_accuracy: 0.9072\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9127 - val_loss: 0.1845 - val_accuracy: 0.9092\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9139 - val_loss: 0.1821 - val_accuracy: 0.9116\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9159 - val_loss: 0.1817 - val_accuracy: 0.9114\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9113 - val_loss: 0.1894 - val_accuracy: 0.9109\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9163 - val_loss: 0.1876 - val_accuracy: 0.9117\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9118 - val_loss: 0.1838 - val_accuracy: 0.9113\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9147 - val_loss: 0.1856 - val_accuracy: 0.9120\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9118 - val_loss: 0.1840 - val_accuracy: 0.9142\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9154 - val_loss: 0.1830 - val_accuracy: 0.9124\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9113 - val_loss: 0.1848 - val_accuracy: 0.9124\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9150 - val_loss: 0.1823 - val_accuracy: 0.9118\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9139 - val_loss: 0.1825 - val_accuracy: 0.9103\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9135 - val_loss: 0.1843 - val_accuracy: 0.9111\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9126 - val_loss: 0.1852 - val_accuracy: 0.9130\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9143 - val_loss: 0.1841 - val_accuracy: 0.9115\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9158 - val_loss: 0.1853 - val_accuracy: 0.9135\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9118 - val_loss: 0.1890 - val_accuracy: 0.9135\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9156 - val_loss: 0.1837 - val_accuracy: 0.9120\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9144 - val_loss: 0.1834 - val_accuracy: 0.9136\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1928 - accuracy: 0.9105 - val_loss: 0.1863 - val_accuracy: 0.9122\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9104 - val_loss: 0.1864 - val_accuracy: 0.9139\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9116 - val_loss: 0.1830 - val_accuracy: 0.9130\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9119 - val_loss: 0.1859 - val_accuracy: 0.9105\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9144 - val_loss: 0.1829 - val_accuracy: 0.9125\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9134 - val_loss: 0.1855 - val_accuracy: 0.9141\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9109 - val_loss: 0.1926 - val_accuracy: 0.9129\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9137 - val_loss: 0.1870 - val_accuracy: 0.9131\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9134 - val_loss: 0.1868 - val_accuracy: 0.9124\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9119 - val_loss: 0.1845 - val_accuracy: 0.9129\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9160 - val_loss: 0.1820 - val_accuracy: 0.9132\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9133 - val_loss: 0.1823 - val_accuracy: 0.9128\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9156 - val_loss: 0.1850 - val_accuracy: 0.9101\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9095 - val_loss: 0.1880 - val_accuracy: 0.9129\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9138 - val_loss: 0.1818 - val_accuracy: 0.9147\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9145 - val_loss: 0.1917 - val_accuracy: 0.9123\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9121 - val_loss: 0.1992 - val_accuracy: 0.9054\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9118 - val_loss: 0.1866 - val_accuracy: 0.9150\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9160 - val_loss: 0.1840 - val_accuracy: 0.9142\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9165 - val_loss: 0.1867 - val_accuracy: 0.9102\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9146 - val_loss: 0.1831 - val_accuracy: 0.9131\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9138 - val_loss: 0.1825 - val_accuracy: 0.9139\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9148 - val_loss: 0.1829 - val_accuracy: 0.9121\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9152 - val_loss: 0.1847 - val_accuracy: 0.9141\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9152 - val_loss: 0.1979 - val_accuracy: 0.9064\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1808 - accuracy: 0.9161 - val_loss: 0.1853 - val_accuracy: 0.9108\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9118 - val_loss: 0.1870 - val_accuracy: 0.9157\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9162 - val_loss: 0.1861 - val_accuracy: 0.9097\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9132 - val_loss: 0.1879 - val_accuracy: 0.9122\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9160 - val_loss: 0.1862 - val_accuracy: 0.9147\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9135 - val_loss: 0.1885 - val_accuracy: 0.9136\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9160 - val_loss: 0.1916 - val_accuracy: 0.9114\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9138 - val_loss: 0.1835 - val_accuracy: 0.9121\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9167 - val_loss: 0.1894 - val_accuracy: 0.9100\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9121 - val_loss: 0.1863 - val_accuracy: 0.9132\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9144 - val_loss: 0.1873 - val_accuracy: 0.9139\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9123 - val_loss: 0.1854 - val_accuracy: 0.9130\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9149 - val_loss: 0.1857 - val_accuracy: 0.9119\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9149 - val_loss: 0.1861 - val_accuracy: 0.9126\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9131 - val_loss: 0.1961 - val_accuracy: 0.9102\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9172 - val_loss: 0.1849 - val_accuracy: 0.9124\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9161 - val_loss: 0.1865 - val_accuracy: 0.9153\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9148 - val_loss: 0.1875 - val_accuracy: 0.9131\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9140 - val_loss: 0.1862 - val_accuracy: 0.9118\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9153 - val_loss: 0.1872 - val_accuracy: 0.9122\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9162 - val_loss: 0.1926 - val_accuracy: 0.9101\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9137 - val_loss: 0.1854 - val_accuracy: 0.9131\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9159 - val_loss: 0.1858 - val_accuracy: 0.9141\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9149 - val_loss: 0.1925 - val_accuracy: 0.9083\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9138 - val_loss: 0.1845 - val_accuracy: 0.9128\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9163 - val_loss: 0.1854 - val_accuracy: 0.9114\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9125 - val_loss: 0.1919 - val_accuracy: 0.9098\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9139 - val_loss: 0.1862 - val_accuracy: 0.9123\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7pIdiSdO02eC",
        "outputId": "98974cd2-b7f0-496e-c99e-ff980d90b745"
      },
      "source": [
        "#0.3\n",
        "model_9 = Sequential()\n",
        "model_9.add(Dense(32, input_dim = 20,activation='relu'))\n",
        "model_9.add(Dense(16,activation='relu'))\n",
        "model_9.add(Dense(8,activation='relu'))\n",
        "model_9.add(Dense(4,activation='relu'))\n",
        "model_9.add(Dense(1,activation='sigmoid'))\n",
        "model_9.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_9.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=32)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3704 - accuracy: 0.8466 - val_loss: 0.2323 - val_accuracy: 0.9079\n",
            "Epoch 2/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2217 - accuracy: 0.9090 - val_loss: 0.2020 - val_accuracy: 0.9132\n",
            "Epoch 3/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2083 - accuracy: 0.9081 - val_loss: 0.2045 - val_accuracy: 0.9081\n",
            "Epoch 4/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2034 - accuracy: 0.9091 - val_loss: 0.1988 - val_accuracy: 0.9122\n",
            "Epoch 5/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9117 - val_loss: 0.1916 - val_accuracy: 0.9151\n",
            "Epoch 6/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9099 - val_loss: 0.1949 - val_accuracy: 0.9139\n",
            "Epoch 7/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9099 - val_loss: 0.1903 - val_accuracy: 0.9129\n",
            "Epoch 8/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9101 - val_loss: 0.1921 - val_accuracy: 0.9131\n",
            "Epoch 9/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9127 - val_loss: 0.1961 - val_accuracy: 0.9156\n",
            "Epoch 10/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1955 - accuracy: 0.9117 - val_loss: 0.1876 - val_accuracy: 0.9151\n",
            "Epoch 11/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9118 - val_loss: 0.1900 - val_accuracy: 0.9116\n",
            "Epoch 12/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9115 - val_loss: 0.1900 - val_accuracy: 0.9134\n",
            "Epoch 13/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9061 - val_loss: 0.1899 - val_accuracy: 0.9139\n",
            "Epoch 14/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9123 - val_loss: 0.1832 - val_accuracy: 0.9151\n",
            "Epoch 15/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9133 - val_loss: 0.1861 - val_accuracy: 0.9152\n",
            "Epoch 16/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9142 - val_loss: 0.1841 - val_accuracy: 0.9152\n",
            "Epoch 17/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1827 - accuracy: 0.9140 - val_loss: 0.1844 - val_accuracy: 0.9124\n",
            "Epoch 18/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9158 - val_loss: 0.1833 - val_accuracy: 0.9149\n",
            "Epoch 19/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9137 - val_loss: 0.1801 - val_accuracy: 0.9152\n",
            "Epoch 20/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9129 - val_loss: 0.1793 - val_accuracy: 0.9161\n",
            "Epoch 21/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9163 - val_loss: 0.1805 - val_accuracy: 0.9133\n",
            "Epoch 22/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9139 - val_loss: 0.1780 - val_accuracy: 0.9164\n",
            "Epoch 23/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1804 - accuracy: 0.9141 - val_loss: 0.1812 - val_accuracy: 0.9156\n",
            "Epoch 24/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9158 - val_loss: 0.1800 - val_accuracy: 0.9146\n",
            "Epoch 25/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1781 - accuracy: 0.9174 - val_loss: 0.1791 - val_accuracy: 0.9139\n",
            "Epoch 26/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1792 - accuracy: 0.9148 - val_loss: 0.1786 - val_accuracy: 0.9148\n",
            "Epoch 27/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9184 - val_loss: 0.1788 - val_accuracy: 0.9140\n",
            "Epoch 28/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1765 - accuracy: 0.9159 - val_loss: 0.1789 - val_accuracy: 0.9153\n",
            "Epoch 29/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1752 - accuracy: 0.9193 - val_loss: 0.1790 - val_accuracy: 0.9165\n",
            "Epoch 30/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9157 - val_loss: 0.1765 - val_accuracy: 0.9157\n",
            "Epoch 31/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9154 - val_loss: 0.1796 - val_accuracy: 0.9143\n",
            "Epoch 32/32\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1745 - accuracy: 0.9167 - val_loss: 0.1805 - val_accuracy: 0.9158\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-HkqVBFLLHi9",
        "outputId": "b22e784c-0af2-47fd-ce02-95ad4c193111"
      },
      "source": [
        "model_r4 = Sequential()\n",
        "model_r4.add(Dense(16, input_dim = 20,activation='relu'))\n",
        "model_r4.add(Dense(8,activation='relu'))\n",
        "model_r4.add(Dense(4,activation='relu'))\n",
        "model_r4.add(Dense(4,activation='relu'))\n",
        "model_r4.add(Dense(4,activation='relu'))\n",
        "model_r4.add(Dense(1,activation='sigmoid'))\n",
        "model_r4.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "model_r4.summary()\n",
        "history = model_r4.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=128)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 16)                336       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 553\n",
            "Trainable params: 553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/128\n",
            "901/901 [==============================] - 3s 2ms/step - loss: 0.3849 - accuracy: 0.8739 - val_loss: 0.2644 - val_accuracy: 0.9010\n",
            "Epoch 2/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2593 - accuracy: 0.9002 - val_loss: 0.2200 - val_accuracy: 0.9074\n",
            "Epoch 3/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2194 - accuracy: 0.9060 - val_loss: 0.2085 - val_accuracy: 0.9101\n",
            "Epoch 4/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2174 - accuracy: 0.9054 - val_loss: 0.2040 - val_accuracy: 0.9101\n",
            "Epoch 5/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2074 - accuracy: 0.9081 - val_loss: 0.2016 - val_accuracy: 0.9085\n",
            "Epoch 6/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2088 - accuracy: 0.9081 - val_loss: 0.1987 - val_accuracy: 0.9103\n",
            "Epoch 7/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2052 - accuracy: 0.9091 - val_loss: 0.1978 - val_accuracy: 0.9118\n",
            "Epoch 8/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9095 - val_loss: 0.1961 - val_accuracy: 0.9124\n",
            "Epoch 9/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2073 - accuracy: 0.9057 - val_loss: 0.1956 - val_accuracy: 0.9123\n",
            "Epoch 10/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2045 - accuracy: 0.9071 - val_loss: 0.1947 - val_accuracy: 0.9120\n",
            "Epoch 11/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2052 - accuracy: 0.9069 - val_loss: 0.2035 - val_accuracy: 0.9122\n",
            "Epoch 12/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9070 - val_loss: 0.1955 - val_accuracy: 0.9129\n",
            "Epoch 13/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9085 - val_loss: 0.1969 - val_accuracy: 0.9118\n",
            "Epoch 14/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9129 - val_loss: 0.1927 - val_accuracy: 0.9124\n",
            "Epoch 15/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9117 - val_loss: 0.2011 - val_accuracy: 0.9127\n",
            "Epoch 16/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1988 - accuracy: 0.9093 - val_loss: 0.1996 - val_accuracy: 0.9059\n",
            "Epoch 17/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9143 - val_loss: 0.1893 - val_accuracy: 0.9114\n",
            "Epoch 18/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9113 - val_loss: 0.1898 - val_accuracy: 0.9109\n",
            "Epoch 19/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9120 - val_loss: 0.1921 - val_accuracy: 0.9092\n",
            "Epoch 20/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9110 - val_loss: 0.1897 - val_accuracy: 0.9100\n",
            "Epoch 21/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9114 - val_loss: 0.1891 - val_accuracy: 0.9129\n",
            "Epoch 22/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9127 - val_loss: 0.1894 - val_accuracy: 0.9131\n",
            "Epoch 23/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9098 - val_loss: 0.1926 - val_accuracy: 0.9140\n",
            "Epoch 24/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9127 - val_loss: 0.2011 - val_accuracy: 0.9048\n",
            "Epoch 25/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9124 - val_loss: 0.1920 - val_accuracy: 0.9128\n",
            "Epoch 26/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9106 - val_loss: 0.1868 - val_accuracy: 0.9112\n",
            "Epoch 27/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9122 - val_loss: 0.1860 - val_accuracy: 0.9109\n",
            "Epoch 28/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9099 - val_loss: 0.1889 - val_accuracy: 0.9119\n",
            "Epoch 29/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9113 - val_loss: 0.1866 - val_accuracy: 0.9118\n",
            "Epoch 30/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9140 - val_loss: 0.1903 - val_accuracy: 0.9136\n",
            "Epoch 31/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9130 - val_loss: 0.1922 - val_accuracy: 0.9054\n",
            "Epoch 32/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9119 - val_loss: 0.1887 - val_accuracy: 0.9131\n",
            "Epoch 33/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9150 - val_loss: 0.1890 - val_accuracy: 0.9080\n",
            "Epoch 34/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9138 - val_loss: 0.1886 - val_accuracy: 0.9137\n",
            "Epoch 35/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9131 - val_loss: 0.1855 - val_accuracy: 0.9112\n",
            "Epoch 36/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9122 - val_loss: 0.1858 - val_accuracy: 0.9109\n",
            "Epoch 37/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9123 - val_loss: 0.1865 - val_accuracy: 0.9114\n",
            "Epoch 38/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9119 - val_loss: 0.1863 - val_accuracy: 0.9133\n",
            "Epoch 39/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9132 - val_loss: 0.1856 - val_accuracy: 0.9115\n",
            "Epoch 40/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9120 - val_loss: 0.1878 - val_accuracy: 0.9103\n",
            "Epoch 41/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9125 - val_loss: 0.1870 - val_accuracy: 0.9132\n",
            "Epoch 42/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9133 - val_loss: 0.1856 - val_accuracy: 0.9126\n",
            "Epoch 43/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9139 - val_loss: 0.1885 - val_accuracy: 0.9142\n",
            "Epoch 44/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9170 - val_loss: 0.1874 - val_accuracy: 0.9124\n",
            "Epoch 45/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9127 - val_loss: 0.1865 - val_accuracy: 0.9118\n",
            "Epoch 46/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9137 - val_loss: 0.1879 - val_accuracy: 0.9138\n",
            "Epoch 47/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9156 - val_loss: 0.1895 - val_accuracy: 0.9135\n",
            "Epoch 48/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9153 - val_loss: 0.1950 - val_accuracy: 0.9144\n",
            "Epoch 49/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9092 - val_loss: 0.1859 - val_accuracy: 0.9126\n",
            "Epoch 50/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9121 - val_loss: 0.1887 - val_accuracy: 0.9112\n",
            "Epoch 51/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9145 - val_loss: 0.1872 - val_accuracy: 0.9129\n",
            "Epoch 52/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9139 - val_loss: 0.1898 - val_accuracy: 0.9106\n",
            "Epoch 53/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9102 - val_loss: 0.1897 - val_accuracy: 0.9135\n",
            "Epoch 54/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9126 - val_loss: 0.1899 - val_accuracy: 0.9102\n",
            "Epoch 55/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9147 - val_loss: 0.1937 - val_accuracy: 0.9063\n",
            "Epoch 56/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9170 - val_loss: 0.1869 - val_accuracy: 0.9128\n",
            "Epoch 57/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9113 - val_loss: 0.1865 - val_accuracy: 0.9114\n",
            "Epoch 58/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9146 - val_loss: 0.1872 - val_accuracy: 0.9100\n",
            "Epoch 59/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9168 - val_loss: 0.1869 - val_accuracy: 0.9114\n",
            "Epoch 60/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9147 - val_loss: 0.1964 - val_accuracy: 0.9148\n",
            "Epoch 61/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9115 - val_loss: 0.1887 - val_accuracy: 0.9104\n",
            "Epoch 62/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9122 - val_loss: 0.1867 - val_accuracy: 0.9106\n",
            "Epoch 63/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9122 - val_loss: 0.1938 - val_accuracy: 0.9142\n",
            "Epoch 64/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9108 - val_loss: 0.1891 - val_accuracy: 0.9118\n",
            "Epoch 65/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9129 - val_loss: 0.1857 - val_accuracy: 0.9128\n",
            "Epoch 66/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9157 - val_loss: 0.1865 - val_accuracy: 0.9139\n",
            "Epoch 67/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9108 - val_loss: 0.1857 - val_accuracy: 0.9144\n",
            "Epoch 68/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9162 - val_loss: 0.1923 - val_accuracy: 0.9126\n",
            "Epoch 69/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9161 - val_loss: 0.1867 - val_accuracy: 0.9147\n",
            "Epoch 70/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9144 - val_loss: 0.1866 - val_accuracy: 0.9143\n",
            "Epoch 71/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9107 - val_loss: 0.1855 - val_accuracy: 0.9139\n",
            "Epoch 72/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9127 - val_loss: 0.1885 - val_accuracy: 0.9115\n",
            "Epoch 73/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9131 - val_loss: 0.1848 - val_accuracy: 0.9136\n",
            "Epoch 74/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9131 - val_loss: 0.1859 - val_accuracy: 0.9132\n",
            "Epoch 75/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9127 - val_loss: 0.1875 - val_accuracy: 0.9144\n",
            "Epoch 76/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9137 - val_loss: 0.1941 - val_accuracy: 0.9117\n",
            "Epoch 77/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1796 - accuracy: 0.9176 - val_loss: 0.1842 - val_accuracy: 0.9146\n",
            "Epoch 78/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9152 - val_loss: 0.1843 - val_accuracy: 0.9136\n",
            "Epoch 79/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9129 - val_loss: 0.1838 - val_accuracy: 0.9141\n",
            "Epoch 80/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9141 - val_loss: 0.1916 - val_accuracy: 0.9135\n",
            "Epoch 81/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9138 - val_loss: 0.1853 - val_accuracy: 0.9143\n",
            "Epoch 82/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9125 - val_loss: 0.1882 - val_accuracy: 0.9131\n",
            "Epoch 83/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9144 - val_loss: 0.1841 - val_accuracy: 0.9128\n",
            "Epoch 84/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9136 - val_loss: 0.1851 - val_accuracy: 0.9132\n",
            "Epoch 85/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9150 - val_loss: 0.1856 - val_accuracy: 0.9138\n",
            "Epoch 86/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9138 - val_loss: 0.1860 - val_accuracy: 0.9140\n",
            "Epoch 87/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9126 - val_loss: 0.1888 - val_accuracy: 0.9147\n",
            "Epoch 88/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1806 - accuracy: 0.9168 - val_loss: 0.1886 - val_accuracy: 0.9153\n",
            "Epoch 89/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9154 - val_loss: 0.2026 - val_accuracy: 0.9123\n",
            "Epoch 90/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9091 - val_loss: 0.1860 - val_accuracy: 0.9143\n",
            "Epoch 91/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9122 - val_loss: 0.1827 - val_accuracy: 0.9140\n",
            "Epoch 92/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9142 - val_loss: 0.1844 - val_accuracy: 0.9118\n",
            "Epoch 93/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9141 - val_loss: 0.1830 - val_accuracy: 0.9131\n",
            "Epoch 94/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9125 - val_loss: 0.1849 - val_accuracy: 0.9130\n",
            "Epoch 95/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9132 - val_loss: 0.1848 - val_accuracy: 0.9127\n",
            "Epoch 96/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9111 - val_loss: 0.1846 - val_accuracy: 0.9127\n",
            "Epoch 97/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9128 - val_loss: 0.1883 - val_accuracy: 0.9123\n",
            "Epoch 98/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9095 - val_loss: 0.2035 - val_accuracy: 0.9141\n",
            "Epoch 99/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9109 - val_loss: 0.1840 - val_accuracy: 0.9137\n",
            "Epoch 100/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9139 - val_loss: 0.1862 - val_accuracy: 0.9141\n",
            "Epoch 101/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9157 - val_loss: 0.1880 - val_accuracy: 0.9128\n",
            "Epoch 102/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9116 - val_loss: 0.1838 - val_accuracy: 0.9143\n",
            "Epoch 103/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9174 - val_loss: 0.1838 - val_accuracy: 0.9136\n",
            "Epoch 104/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9139 - val_loss: 0.1880 - val_accuracy: 0.9137\n",
            "Epoch 105/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9153 - val_loss: 0.1872 - val_accuracy: 0.9132\n",
            "Epoch 106/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9130 - val_loss: 0.1846 - val_accuracy: 0.9144\n",
            "Epoch 107/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9096 - val_loss: 0.1918 - val_accuracy: 0.9122\n",
            "Epoch 108/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9168 - val_loss: 0.1895 - val_accuracy: 0.9151\n",
            "Epoch 109/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9181 - val_loss: 0.1842 - val_accuracy: 0.9129\n",
            "Epoch 110/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9153 - val_loss: 0.1847 - val_accuracy: 0.9151\n",
            "Epoch 111/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9173 - val_loss: 0.1901 - val_accuracy: 0.9131\n",
            "Epoch 112/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9150 - val_loss: 0.1871 - val_accuracy: 0.9122\n",
            "Epoch 113/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9156 - val_loss: 0.1863 - val_accuracy: 0.9143\n",
            "Epoch 114/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9173 - val_loss: 0.1951 - val_accuracy: 0.9113\n",
            "Epoch 115/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9127 - val_loss: 0.1854 - val_accuracy: 0.9149\n",
            "Epoch 116/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9123 - val_loss: 0.1869 - val_accuracy: 0.9152\n",
            "Epoch 117/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9152 - val_loss: 0.1905 - val_accuracy: 0.9150\n",
            "Epoch 118/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9144 - val_loss: 0.2021 - val_accuracy: 0.9156\n",
            "Epoch 119/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9172 - val_loss: 0.1937 - val_accuracy: 0.9096\n",
            "Epoch 120/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9132 - val_loss: 0.1854 - val_accuracy: 0.9149\n",
            "Epoch 121/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1779 - accuracy: 0.9174 - val_loss: 0.1848 - val_accuracy: 0.9132\n",
            "Epoch 122/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9152 - val_loss: 0.1849 - val_accuracy: 0.9141\n",
            "Epoch 123/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9161 - val_loss: 0.1840 - val_accuracy: 0.9150\n",
            "Epoch 124/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9133 - val_loss: 0.1849 - val_accuracy: 0.9142\n",
            "Epoch 125/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9133 - val_loss: 0.1844 - val_accuracy: 0.9148\n",
            "Epoch 126/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9171 - val_loss: 0.1883 - val_accuracy: 0.9144\n",
            "Epoch 127/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9127 - val_loss: 0.1906 - val_accuracy: 0.9150\n",
            "Epoch 128/128\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9157 - val_loss: 0.1890 - val_accuracy: 0.9138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KWi_W7Nn7XtG",
        "outputId": "410166e8-e0c2-4906-b0f7-c52c916bad0d"
      },
      "source": [
        "model_l = Sequential()\n",
        "model_l.add(Dense(8, input_dim = 20,activation='relu'))\n",
        "model_l.add(Dense(4,activation='relu'))\n",
        "model_l.add(Dense(1,activation='linear'))\n",
        "model_l.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_l.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8422 - accuracy: 0.8806 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 2/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.8004 - accuracy: 0.8833 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 3/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.8051 - accuracy: 0.8830 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 4/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7434 - accuracy: 0.8870 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 5/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7805 - accuracy: 0.8846 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 6/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7979 - accuracy: 0.8834 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 7/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8250 - accuracy: 0.8817 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 8/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7604 - accuracy: 0.8859 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 9/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7422 - accuracy: 0.8871 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 10/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8003 - accuracy: 0.8833 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 11/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7608 - accuracy: 0.8858 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 12/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7579 - accuracy: 0.8860 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 13/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8028 - accuracy: 0.8831 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 14/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7516 - accuracy: 0.8864 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 15/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8227 - accuracy: 0.8818 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 16/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7397 - accuracy: 0.8872 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 17/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7850 - accuracy: 0.8843 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 18/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.8153 - accuracy: 0.8823 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 19/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7741 - accuracy: 0.8850 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 20/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7766 - accuracy: 0.8848 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 21/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.6876 - accuracy: 0.8906 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 22/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7762 - accuracy: 0.8849 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 23/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7900 - accuracy: 0.8840 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 24/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7778 - accuracy: 0.8847 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 25/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7799 - accuracy: 0.8846 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 26/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7386 - accuracy: 0.8873 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 27/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7239 - accuracy: 0.8882 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 28/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7683 - accuracy: 0.8854 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 29/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.8044 - accuracy: 0.8830 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 30/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7419 - accuracy: 0.8871 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 31/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7485 - accuracy: 0.8866 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 32/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8166 - accuracy: 0.8822 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 33/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7471 - accuracy: 0.8867 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 34/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7310 - accuracy: 0.8878 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 35/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7814 - accuracy: 0.8845 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 36/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7237 - accuracy: 0.8883 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 37/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8185 - accuracy: 0.8821 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 38/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7846 - accuracy: 0.8843 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 39/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7507 - accuracy: 0.8865 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 40/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8084 - accuracy: 0.8828 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 41/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8287 - accuracy: 0.8814 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 42/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7696 - accuracy: 0.8853 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 43/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7732 - accuracy: 0.8850 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 44/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7676 - accuracy: 0.8854 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 45/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7516 - accuracy: 0.8864 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 46/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7298 - accuracy: 0.8879 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 47/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7488 - accuracy: 0.8866 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 48/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8026 - accuracy: 0.8831 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 49/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7577 - accuracy: 0.8860 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 50/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7559 - accuracy: 0.8862 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 51/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7401 - accuracy: 0.8872 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 52/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7299 - accuracy: 0.8878 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 53/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7575 - accuracy: 0.8861 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 54/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.8001 - accuracy: 0.8833 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 55/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7411 - accuracy: 0.8871 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 56/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7178 - accuracy: 0.8886 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 57/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7351 - accuracy: 0.8875 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 58/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 1.7480 - accuracy: 0.8867 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 59/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7961 - accuracy: 0.8836 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 60/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7726 - accuracy: 0.8851 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 61/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7808 - accuracy: 0.8846 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 62/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7563 - accuracy: 0.8861 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 63/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7775 - accuracy: 0.8848 - val_loss: 1.6546 - val_accuracy: 0.8927\n",
            "Epoch 64/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 1.7776 - accuracy: 0.8848 - val_loss: 1.6546 - val_accuracy: 0.8927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEHCAYAAAC5u6FsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZxXZZ3/8ddbbhzAGwgwlcEgw4BEQb4/1NDWJApJoVwQcbFsTdMVU37++kWtJbK1uWWt2pJ5s4CZSYhRuItiGuYdugxyI3cqKsIAykgS4h0MfvaPc4a+DAN8D8xxZuT9fDzmwTnXuc75fi75Oh+uc51zXYoIzMzMSnVAQwdgZmZNixOHmZll4sRhZmaZOHGYmVkmThxmZpaJE4eZmWXSPM+LSxoE3Ag0A26PiOtqHT8KuANom9YZGxEzJbUEbgEKwPvAFRHxSHrOA8ARaeyPAZdFxLbdxdGhQ4fo0qVLPbbMzOzDb968ea9HRMfa5bklDknNgAnAQKASmCtpRkQsLap2NTA1Im6W1BOYCXQBLgKIiF6SDgPul/R/IuJ94JyI2CRJwDRgODBld7F06dKFioqKem6hmdmHm6RX6irP81ZVP2BFRLwUEVtIfrkPrVUngEPS7UOBtel2T+BPABGxHthI0vsgIjaldZoDLdNrmJnZByTPxNEJWF20X5mWFRsHjJJUSdLbuDwtXwgMkdRcUlegL9C55iRJs4D1wJskvY6dSLpYUoWkiqqqqnpojpmZQcMPjo8EJkdEOTAYuFPSAcBEkkRTAdwAPAlsH8eIiC+QjHMcCJxe14Uj4taIKEREoWPHnW7RmZnZXspzcHwNRb0EoDwtK3YhMAggIuZIKgM6pLenxtRUkvQk8HzxiRHxrqQ/kNz++mP9h29mZnXJs8cxF+gmqWv6lNS5wIxadVYBAwAk9QDKgCpJrSW1ScsHAtURsVTSQZKOSMubA18ElufYBjMzqyW3HkdEVEsaDcwiedR2YkQskTQeqIiIGcBVwG2SxpAMcl8QEZE+STVL0vskvZTz08u2AWZIOpAk6c0GfplXG8zMbGfaH6ZVLxQK4cdxzcyykTQvIgq1y3N9AbDJu38svPpsQ0dhZrZ3Du8FZ1y353oZNfRTVWZm1sS4x7E7OWRqM7Omzj0OMzPLxInDzMwyceIwM7NMnDjMzCwTJw4zM8vEicPMzDJx4jAzs0ycOMzMLBMnDjMzy8SJw8zMMnHiMDOzTJw4zMwsEycOMzPLxInDzMwyyTVxSBok6TlJKySNreP4UZJmS5ovaZGkwWl5S0mTJD0raaGk09Ly1pL+W9JySUsked5zM7MPWG6JQ1IzYAJwBtATGCmpZ61qVwNTI6IPcC7wi7T8IoCI6AUMBH4qqSbW6yOiO9AH6C/pjLzaYGZmO8uzx9EPWBERL0XEFmAKMLRWnQAOSbcPBdam2z2BPwFExHpgI1CIiLcjYnZavgV4BijPsQ1mZlZLnomjE7C6aL8yLSs2DhglqRKYCVyeli8EhkhqLqkr0BfoXHyipLbAWcDDdX24pIslVUiqqKqq2te2mJlZqqEHx0cCkyOiHBgM3JnekppIkmgqgBuAJ4FtNSdJag7cDdwUES/VdeGIuDUiChFR6NixY87NMDPbf+S55vgaduwllKdlxS4EBgFExBxJZUCH9PbUmJpKkp4Eni8671bghYi4IY/Azcxs1/LsccwFuknqKqklyeD3jFp1VgEDACT1AMqAqvTpqTZp+UCgOiKWpvs/IBkPuTLH2M3MbBdy63FERLWk0cAsoBkwMSKWSBoPVETEDOAq4DZJY0gGyi+IiJB0GDBL0vskvZTzASSVA/8MLAeekQTwHxFxe17tMDOzHSkiGjqG3BUKhaioqGjoMMzMmhRJ8yKiULu8oQfHzcysiXHiMDOzTJw4zMwsEycOMzPLxInDzMwyceIwM7NMnDjMzCwTJw4zM8vEicPMzDJx4jAzs0ycOMzMLBMnDjMzy8SJw8zMMnHiMDOzTJw4zMwsEycOMzPLxInDzMwyyTVxSBok6TlJKySNreP4UZJmS5ovaZGkwWl5S0mTJD0raaGk04rO+aGk1ZI25xm7mZnVLbfEIakZMAE4A+gJjJTUs1a1q4GpEdEHOBf4RVp+EUBE9AIGAj+VVBPrfUC/vOI2M7Pdy7PH0Q9YEREvRcQWYAowtFadAA5Jtw8F1qbbPYE/AUTEemAjUEj3n4qIdTnGbWZmu5Fn4ugErC7ar0zLio0DRkmqBGYCl6flC4EhkppL6gr0BTpn+XBJF0uqkFRRVVW1N/GbmVkdGnpwfCQwOSLKgcHAnektqYkkiaYCuAF4EtiW5cIRcWtEFCKi0LFjx3oO28xs/9U8x2uvYcdeQnlaVuxCYBBARMyRVAZ0SG9PjampJOlJ4PkcYzUzsxLl2eOYC3ST1FVSS5LB7xm16qwCBgBI6gGUAVWSWktqk5YPBKojYmmOsZqZWYlySxwRUQ2MBmYBy0ienloiabykIWm1q4CLJC0E7gYuiIgADgOekbQM+DZwfs11Jf04HRNpLalS0ri82mBmZjtT8nv6w61QKERFRUVDh2Fm1qRImhcRhdrlDT04bmZmTYwTh5mZZeLEYWZmmThxmJlZJk4cZmaWiROHmZll4sRhZmaZOHGYmVkmThxmZpaJE4eZmWXixGFmZpk4cZiZWSZOHGZmlokTh5mZZeLEYWZmmThxmJlZJrkmDkmDJD0naYWksXUcP0rSbEnzJS2SNDgtbylpkqRnJS2UdFrROX3T8hWSbpKkPNtgZmY7yi1xSGoGTADOAHoCIyX1rFXtapIlZfuQrEn+i7T8IoCI6AUMBH4qqSbWm9Pj3dKfQXm1wczMdpZnj6MfsCIiXoqILcAUYGitOgEckm4fCqxNt3sCfwKIiPXARqAg6QjgkIh4Kl2b/FfAl3Jsg5mZ1ZJn4ugErC7ar0zLio0DRkmqBGYCl6flC4EhkppL6gr0BTqn51fu4ZoASLpYUoWkiqqqqn1ti5mZpRp6cHwkMDkiyoHBwJ3pLamJJEmhArgBeBLYluXCEXFrRBQiotCxY8d6DtvMbP/VPMdrryHpJdQoT8uKXUg6RhERcySVAR3S21NjaipJehJ4Hngjvc7urmlmZjnKs8cxF+gmqaukliSD3zNq1VkFDACQ1AMoA6oktZbUJi0fCFRHxNKIWAdsknRS+jTVV4A/5NgGMzOrJbceR0RUSxoNzAKaARMjYomk8UBFRMwArgJukzSGZKD8gogISYcBsyS9T9KjOL/o0v8ETAZaAfenP2Zm9gFR8nDSh1uhUIiKioqGDsPMrEmRNC8iCrXLG3pw3MzMmhgnDjMzy8SJw8zMMtlj4pB0VtF0H2Zmtp8rJSGMAF6Q9GNJ3fMOyMzMGrc9Jo6IGAX0AV4EJkuak07ncXDu0ZmZWaNT0i2oiNgETCOZqPAI4MvAM5Iu3+2JZmb2oVPKGMcQSdOBR4AWQL+IOAM4nuQFPjMz24+U8ub43wP/HhGPFhdGxNuSLswnLDMza6xKSRzjgHU1O5JaAR+NiJUR8XBegZmZWeNUyhjHPcD7Rfvb0jIzM9sPlZI4mqcr+AGQbrfMLyQzM2vMSkkcVZKG1OxIGgq8nl9IZmbWmJUyxnEJcJek/wBEshzsV3KNyszMGq09Jo6IeBE4SdJB6f7m3KMyM7NGq6SFnCR9EfgUUJYsvAcRMT7HuMzMrJEq5QXAX5LMV3U5ya2q4cDHco7LzMwaqVIGxz8dEV8B3oiIa4GTgWNKubikQZKek7RC0tg6jh8labak+ZIWSRqclreQdIekZyUtk/SdonOukLRY0hJJV5bWTDMzqy+lJI530z/flnQksJVkvqrdktQMmACcAfQERkrqWava1cDUiOgDnAv8Ii0fDhwYEb2AvsA3JHWRdCxwEdCPZMqTMyV9ooQ2mJlZPSklcdwnqS3wE+AZYCXwmxLO6wesiIiX0nc/pgBDa9UJ4JB0+1BgbVF5G0nNgVbAFmAT0AN4OiLejohq4M/A2SXEYmZm9WS3iSNdwOnhiNgYEfeSjG10j4jvl3DtTiSP7taoTMuKjQNGSaoEZpKMo0AyE+9bJFOdrAKuj4i/AIuBUyW1l9QaGAx03kXsF0uqkFRRVVVVQrhmZlaK3SaOiHif5HZTzf57EfHXevz8kcDkiCgnSQJ3psmqH8nUJkcCXYGrJH08IpYB/wY8CDwALEjr1RX7rRFRiIhCx44d6zFkM7P9Wym3qh6W9PeqeQ63dGvYsTdQnpYVuxCYChARc4AyoANwHvBARGyNiPXAE0AhrfefEdE3Ij4DvAE8nzEuMzPbB6Ukjm+QTGr4nqRNkt6UtKmE8+YC3SR1ldSSZPB7Rq06q4ABAJJ6kCSOqrT89LS8DXASsDzdPyz98yiS8Y1SxlvMzKyelPLm+F4tERsR1ZJGA7OAZsDEiFgiaTxQEREzSBaCuk3SGJIB8QsiIiRNACZJWkLy7sikiFiUXvpeSe1Jnu66LCI27k18Zma2dxQRu68gfaau8toLOzVmhUIhKioqGjoMM7MmRdK8iCjULi9lypFvFW2XkQxczyO9lWRmZvuXUm5VnVW8L6kzcENuEZmZWaNWyuB4bZUkL+KZmdl+aI89Dkk/Jxm4hiTR9CZ5g9zMzPZDpYxxFI8qVwN3R8QTOcVjZmaNXCmJYxrwbkRsg2TyQkmtI+LtfEMzM7PGqKQ3x0kmGqzRCngon3DMzKyxKyVxlBUvF5tut84vJDMza8xKSRxvSTqhZkdSX+Cd/EIyM7PGrJQxjiuBeyStJZn+43CSpWTNzGw/VMoLgHMldQc+mRY9FxFb8w3LzMwaqz3eqpJ0GdAmIhZHxGLgIEn/lH9oZmbWGJUyxnFR8Qy0EfEGybrfZma2HyolcTQrXsRJUjOgZX4hmZlZY1bK4PgDwG8l3ZLufwO4P7+QzMysMSslcXwbuBi4JN1fRPJklZmZ7Yf2eKsqIt4HngZWkqzFcTqwrJSLSxok6TlJKySNreP4UZJmS5ovaZGkwWl5C0l3SHpW0jJJ3yk6Z4ykJZIWS7pbUllpTTUzs/qwyx6HpGOAkenP68BvASLis6VcOB0LmQAMJJmKfa6kGRGxtKja1cDUiLhZUk9gJtAFGA4cGBG9JLUGlkq6m2S52G8CPSPiHUlTSdYyn1x6k83MbF/srsexnKR3cWZEnBIRPwe2Zbh2P2BFRLwUEVuAKcDQWnUCOCTdPhRYW1TeRlJzkrmxtgCb0mPNgVbpsdZF55iZ2Qdgd4njbGAdMFvSbZIGkLw5XqpOwOqi/cq0rNg4YJSkSpLexuVp+TTgrfTzVwHXR8RfImINcH1atg74a0Q8mCEmMzPbR7tMHBHx+4g4F+gOzCaZeuQwSTdL+nw9ff5IYHJElAODgTslHUDSW9kGHAl0Ba6S9HFJ7Uh6LV3TY20kjarrwpIullQhqaKqqqqewjUzs1IGx9+KiN+ka4+XA/NJnrTakzVA56L98rSs2IXA1PRz5gBlQAfgPOCBiNgaEeuBJ4AC8Dng5YioSqc9+R3w6V3EfWtEFCKi0LFjxxLCNTOzUmRaczwi3kh/IQ8oofpcoJukrpJakgxiz6hVZxUwAEBSD5LEUZWWn56WtwFOIhlzWQWcJKl1+lLiAEp8wsvMzOpHKe9x7JWIqJY0GpgFNAMmRsQSSeOBioiYAVwF3CZpDMmA+AUREZImAJMkLSEZV5kUEYsAJE0jWfO8mqT3c2tebTAzs50pIho6htwVCoWoqKjYc0UzM9tO0ryIKNQuz3SryszMzInDzMwyceIwM7NMnDjMzCwTJw4zM8vEicPMzDJx4jAzs0ycOMzMLBMnDjMzy8SJw8zMMnHiMDOzTJw4zMwsEycOMzPLxInDzMwyceIwM7NMnDjMzCwTJw4zM8sk18QhaZCk5yStkDS2juNHSZotab6kRZIGp+UtJN0h6VlJyyR9Jy3/pKQFRT+bJF2ZZxvMzGxHua05LqkZMAEYCFQCcyXNiIilRdWuBqZGxM2SegIzgS7AcODAiOglqTWwVNLdEfEc0Lvo+muA6Xm1wczMdpZnj6MfsCIiXoqILcAUYGitOgEckm4fCqwtKm8jqTnQCtgCbKp17gDgxYh4JY/gzcysbnkmjk7A6qL9yrSs2DhglKRKkt7G5Wn5NOAtYB2wCrg+Iv5S69xzgbt39eGSLpZUIamiqqpqrxthZmY7aujB8ZHA5IgoBwYDd0o6gKS3sg04EugKXCXp4zUnSWoJDAHu2dWFI+LWiChERKFjx455tsHMbL+SZ+JYA3Qu2i9Py4pdCEwFiIg5QBnQATgPeCAitkbEeuAJoFB03hnAMxHxWk6xm5nZLuSZOOYC3SR1TXsI5wIzatVZRTJWgaQeJImjKi0/PS1vA5wELC86byS7uU1lZmb5yS1xREQ1MBqYBSwjeXpqiaTxkoak1a4CLpK0kCQRXBARQfI01kGSlpAkoEkRsQi2J5KBwO/yit3MzHZNye/pD7dCoRAVFRUNHYaZWZMiaV5EFGqXN/TguJmZNTFOHGZmlokTh5mZZeLEYWZmmThxmJlZJk4cZmaWiROHmZll4sRhZmaZOHGYmVkmThxmZpaJE4eZmWXixGFmZpk4cZiZWSZOHGZmlokTh5mZZeLEYWZmmThxmJlZJs3zvLikQcCNQDPg9oi4rtbxo4A7gLZpnbERMVNSC+B24IQ0xl9FxI/Sc9qmx44FAvjHiJiTZzvMrDRbt26lsrKSd999t6FDsQzKysooLy+nRYsWJdXPLXFIakaydvhAoBKYK2lGRCwtqnY1yVrkN0vqCcwEugDDgQMjopek1sBSSXdHxEqSRPRARAyT1BJonVcbzCybyspKDj74YLp06YKkhg7HShARbNiwgcrKSrp27VrSOXnequoHrIiIlyJiCzAFGFqrTgCHpNuHAmuLyttIag60ArYAmyQdCnwG+E+AiNgSERtzbIOZZfDuu+/Svn17J40mRBLt27fP1EvMM3F0AlYX7VemZcXGAaMkVZL0Ni5Py6cBbwHrgFXA9RHxF6ArUAVMkjRf0u2S2tT14ZIullQhqaKqqqq+2mRme+Ck0fRk/Ttr6MHxkcDkiCgHBgN3SjqApLeyDTiSJFlcJenjJLfWTgBujog+JMllbF0XjohbI6IQEYWOHTt+AE0xM9s/5Jk41gCdi/bL07JiFwJTAdIB7jKgA3AeyTjG1ohYDzwBFEh6LZUR8XR6/jSSRGJmxsaNG/nFL36xV+cOHjyYjRt3f+f7+9//Pg899NBeXX93Jk+ezOjRo3db55FHHuHJJ5+s98/eG3kmjrlAN0ld00Hsc4EZteqsAgYASOpBkjiq0vLT0/I2wEnA8oh4FVgt6ZPp+QOApZiZsfvEUV1dvdtzZ86cSdu2bXdbZ/z48Xzuc5/b6/j2RWNKHLk9VRUR1ZJGA7NIHrWdGBFLJI0HKiJiBnAVcJukMSQD4hdEREiaQDKOsQQQMCkiFqWXvhy4K01GLwFfy6sNZrb3rr1vCUvXbqrXa/Y88hCuOetTuzw+duxYXnzxRXr37s3AgQP54he/yPe+9z3atWvH8uXLef755/nSl77E6tWreffdd7niiiu4+OKLAejSpQsVFRVs3ryZM844g1NOOYUnn3ySTp068Yc//IFWrVpxwQUXcOaZZzJs2DC6dOnCV7/6Ve677z62bt3KPffcQ/fu3amqquK8885j7dq1nHzyyfzxj39k3rx5dOjQYYdYJ02axI9+9CPatm3L8ccfz4EHHgjAfffdxw9+8AO2bNlC+/btueuuu3jnnXf45S9/SbNmzfj1r3/Nz3/+czZu3LhTvY9+9KP1+t97V3Id44iImRFxTEQcHRE/TMu+nyYNImJpRPSPiOMjondEPJiWb46I4RHxqYjoGRE/KbrmgnTs4riI+FJEvJFnG8ys6bjuuus4+uijWbBgAT/5SfJr45lnnuHGG2/k+eefB2DixInMmzePiooKbrrpJjZs2LDTdV544QUuu+wylixZQtu2bbn33nvr/LwOHTrwzDPPcOmll3L99dcDcO2113L66aezZMkShg0bxqpVq3Y6b926dVxzzTU88cQTPP744yxd+rcbJ6eccgpPPfUU8+fP59xzz+XHP/4xXbp04ZJLLmHMmDEsWLCAU089tc56H5RcXwA0s/3X7noGH6R+/frt8H7CTTfdxPTp0wFYvXo1L7zwAu3bt9/hnK5du9K7d28A+vbty8qVK+u89tlnn729zu9+9zsAHn/88e3XHzRoEO3atdvpvKeffprTTjuNmgd3RowYsT2xVVZWMmLECNatW8eWLVt2+W5FqfXy0NBPVZmZ5apNm789sf/II4/w0EMPMWfOHBYuXEifPn3qfH+h5rYRQLNmzXY5PlJTb3d1srr88ssZPXo0zz77LLfccssu368otV4enDjM7EPj4IMP5s0339zl8b/+9a+0a9eO1q1bs3z5cp566ql6j6F///5MnToVgAcffJA33tj5bvqJJ57In//8ZzZs2LB9fKQ4xk6dklfe7rjjju3ltdu2q3ofBCcOM/vQaN++Pf379+fYY4/lW9/61k7HBw0aRHV1NT169GDs2LGcdNJJ9R7DNddcw4MPPsixxx7LPffcw+GHH87BBx+8Q50jjjiCcePGcfLJJ9O/f3969Oix/di4ceMYPnw4ffv23WFA/ayzzmL69On07t2bxx57bJf1PgiKiA/0AxtCoVCIioqKhg7D7ENv2bJlO/wS3B+99957NGvWjObNmzNnzhwuvfRSFixY0NBh7VFdf3eS5kVEoXZdD46bmdWjVatWcc455/D+++/TsmVLbrvttoYOqd45cZiZ1aNu3boxf/78hg4jVx7jMDOzTJw4zMwsEycOMzPLxInDzMwyceIws/3aQQcdBMDatWsZNmxYnXVOO+009vRI/w033MDbb7+9fb+Uadr3Rk28u7IvU8uXyonDzAw48sgjmTZt2l6fXztxlDJNex4+iMThx3HNLB/3j4VXn63fax7eC864bpeHx44dS+fOnbnsssuA5C3sgw46iEsuuYShQ4fyxhtvsHXrVn7wgx8wdOjQHc5duXIlZ555JosXL+add97ha1/7GgsXLqR79+6888472+tdeumlzJ07l3feeYdhw4Zx7bXXctNNN7F27Vo++9nP0qFDB2bPnr19mvYOHTrws5/9jIkTJwLw9a9/nSuvvJKVK1fucvr2Yi+//DLnnXcemzdv3iHmmv3abao9tfw111yzx7Zn5cRhZh8aI0aM4Morr9yeOKZOncqsWbMoKytj+vTpHHLIIbz++uucdNJJDBkyZJdrbd988820bt2aZcuWsWjRIk444W8Ljf7whz/kIx/5CNu2bWPAgAEsWrSIb37zm/zsZz9j9uzZO03/MW/ePCZNmsTTTz9NRHDiiSfyd3/3d7Rr144XXniBu+++m9tuu41zzjmHe++9l1GjRu1w/hVXXMGll17KV77yFSZMmLC9fFdtuu6661i8ePH2t9Wrq6sztb0UThxmlo/d9Azy0qdPH9avX8/atWupqqqiXbt2dO7cma1bt/Ld736XRx99lAMOOIA1a9bw2muvcfjhh9d5nUcffZRvfvObABx33HEcd9xx249NnTqVW2+9lerqatatW8fSpUt3OF7b448/zpe//OXts/SeffbZPPbYYwwZMqSk6dufeOKJ7euBnH/++Xz7298GICLqbFNtu6q3q7aXItfEIWkQcCPJCoC3R8R1tY4fBdwBtE3rjI2ImZJaALeTrCfeHPhVRPwoPWcl8CawDaiuax4VM9t/DR8+nGnTpvHqq68yYsQIAO666y6qqqqYN28eLVq0oEuXLns1DfnLL7/M9ddfz9y5c2nXrh0XXHDBPk1nXnv69uJbYsXq6h2U2qb6anux3AbHJTUDJgBnAD2BkZJ61qp2NTA1IvqQrEleM6IzHDgwInoBfYFvSOpSdN5n0xUDnTTMbAcjRoxgypQpTJs2jeHDhwPJFOSHHXYYLVq0YPbs2bzyyiu7vcZnPvMZfvOb3wCwePFiFi1KVq7etGkTbdq04dBDD+W1117j/vvv337OrqZ0P/XUU/n973/P22+/zVtvvcX06dM59dRTS25P//79mTJlCpAkgRq7alNd069naXsp8uxx9ANWRMRLAJKmAEOBpUV1Ajgk3T4UWFtU3kZSc6AVsAWo38WLS5DHmslmH2aX9WlFy6rNDRpD2WEfY8PGv/KRww7n7eYH82LVZj79haFMvvMcPtnzU/Q6vg9HdzuGVza8xbY2m4mAF6s2U7nhLbZse58XqzbzhWHn86crLuUTx3ySo7t9kmOP70PlG2/Tq/cJfKLHsRzd7RiO6FRO78KJrH/zXV6s2szZ532VAQM/z2GHH8Fd02dS/X7w8uub+UjnYzhz2Eh6903+nXvOP3yVQ8q78cqqV7Z/HsCGze/x1ltbtu/X+L/f/1fGXPKP/Mu//ojPDfri9nh31abyNh04vnAix/ToyYDPfYF/ueafOeuss+jVqxeFQoHu3bvv83/j3KZVlzQMGBQRX0/3zwdOjIjRRXWOAB4E2gFtgM9FxLz0VtWdwACgNTAmIm5Nz3kZeIMkudxSU747ezutuhOHWTaX9WlFp66faOgwLNWqRTOObNtqzxVpWtOqjwQmR8RPJZ0M3CnpWJLeyjbgSJKk8pikh9LeyykRsUbSYcAfJS2PiEdrX1jSxcDFAEcdddReBddY1kw2ayqWLVvG0R13/4KaNX15vgC4BuhctF+elhW7EJgKEBFzgDKgA3Ae8EBEbI2I9cATQCGttyb9cz0wnSTJ7CQibo2IQkQUahaENzOzfZdn4pgLdJPUVVJLksHvGbXqrCK5HYWkHiSJoyotPz0tbwOcBCyX1EbSwUXlnwcW59gGM8tof1hV9MMm699ZbokjIqqB0cAsYBnJ01NLJI2XNCStdhVwkaSFwN3ABZG0YAJwkKQlJAloUkQsAj4KPJ7W/x/gvyPigbzaYGbZlJWVsWHDBiePJiQi2LBhA2VlZSWf4zXHzazebN26lcrKyn1+T8A+WGVlZZSXl9OiRYsdyhvr4LiZfYi0aNGCrlOtW9cAAAW9SURBVF27NnQYljPPjmtmZpk4cZiZWSZOHGZmlsl+MTguqQrY2wlaOgCv12M4H7SmHj80/TY4/obV1OOHhmvDxyJipxfh9ovEsS8kVTTlyRSbevzQ9Nvg+BtWU48fGl8bfKvKzMwyceIwM7NMnDj2bI+z7zZyTT1+aPptcPwNq6nHD42sDR7jMDOzTNzjMDOzTJw4zMwsEyeOXZA0SNJzklZIGtvQ8ZRC0kRJ6yUtLir7iKQ/Snoh/bNdQ8a4O5I6S5otaamkJZKuSMubRBsklUn6H0kL0/ivTcu7Sno6/S79Nl1moNGS1EzSfEn/le43tfhXSnpW0gJJFWlZk/gOAUhqK2mapOWSlkk6ubHF78RRB0nNSKZ2PwPoCYyU1LNhoyrJZGBQrbKxwMMR0Q14ON1vrKqBqyKiJ8kaLJel/92bShveA06PiOOB3sAgSScB/wb8e0R8gmTZ4wsbMMZSXEGyFEKNphY/wGcjonfRuw9N5TsEcCPJQnbdgeNJ/i4aV/wR4Z9aP8DJwKyi/e8A32nouEqMvQuwuGj/OeCIdPsI4LmGjjFDW/4ADGyKbQBaA88AJ5K88ds8Ld/hu9XYfkhW6nyYZCG1/wLUlOJPY1wJdKhV1iS+Q8ChwMukDy411vjd46hbJ2B10X5lWtYUfTQi1qXbr5IshtXoSeoC9AGepgm1Ib3NswBYD/wReBHYGMnCZtD4v0s3AP8feD/db0/Tih8ggAclzZN0cVrWVL5DXUlWQZ2U3i68PV3ttFHF78SxH4nknyuN/vlrSQcB9wJXRsSm4mONvQ0RsS0iepP8y70f0L2BQyqZpDOB9RExr6Fj2UenRMQJJLeaL5P0meKDjfw71Bw4Abg5IvoAb1HrtlRjiN+Jo25rgM5F++VpWVP0mqQjANI/1zdwPLslqQVJ0rgrIn6XFjepNgBExEZgNsmtnbaSahZNa8zfpf7AEEkrgSkkt6tupOnED0BErEn/XA9MJ0ngTeU7VAlURsTT6f40kkTSqOJ34qjbXKBb+jRJS+BcYEYDx7S3ZgBfTbe/SjJu0ChJEvCfwLKI+FnRoSbRBkkdJbVNt1uRjM8sI0kgw9JqjTb+iPhORJRHRBeS7/yfIuIfaCLxA0hqI+ngmm3g88Bimsh3KCJeBVZL+mRaNABYSiOL32+O74KkwST3e5sBEyPihw0c0h5Juhs4jWQK5teAa4DfA1OBo0imlj8nIv7SUDHujqRTgMeAZ/nbPfbvkoxzNPo2SDoOuIPkO3MAMDUixkv6OMm/4D8CzAdGRcR7DRfpnkk6Dfh/EXFmU4o/jXV6utsc+E1E/FBSe5rAdwhAUm/gdqAl8BLwNdLvE40kficOMzPLxLeqzMwsEycOMzPLxInDzMwyceIwM7NMnDjMzCwTJw6zRkzSaTWz1Jo1Fk4cZmaWiROHWT2QNCpdi2OBpFvSyQ43S/r3dG2OhyV1TOv2lvSUpEWSptesrSDpE5IeStfzeEbS0enlDypan+Gu9A17swbjxGG2jyT1AEYA/dMJDrcB/wC0ASoi4lPAn0ne5Af4FfDtiDiO5C35mvK7gAmRrOfxaaBmNtQ+wJUka8N8nGROKbMG03zPVcxsDwYAfYG5aWegFckkdO8Dv03r/Br4naRDgbYR8ee0/A7gnnR+pU4RMR0gIt4FSK/3PxFRme4vIFlz5fH8m2VWNycOs30n4I6I+M4OhdL3atXb2/l9iueF2ob/v7UG5ltVZvvuYWCYpMNg+/rWHyP5/6tmVtnzgMcj4q/AG5JOTcvPB/4cEW8ClZK+lF7jQEmtP9BWmJXI/3Ix20cRsVTS1SSrzh0AbAUuI1mEp196bD3JOAgk02L/Mk0MNbOfQpJEbpE0Pr3G8A+wGWYl8+y4ZjmRtDkiDmroOMzqm29VmZlZJu5xmJlZJu5xmJlZJk4cZmaWiROHmZll4sRhZmaZOHGYmVkm/wvesl3AyrlhMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_dhv95RI9NcD",
        "outputId": "793f5578-1ae1-45f9-da14-2128f1561231"
      },
      "source": [
        "model_all = Sequential()\n",
        "model_all.add(Dense(8, input_dim = 20,activation='linear'))\n",
        "model_all.add(Dense(4,activation='linear'))\n",
        "model_all.add(Dense(1,activation='linear'))\n",
        "model_all.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_all.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.5528 - accuracy: 0.8732 - val_loss: 0.2798 - val_accuracy: 0.8926\n",
            "Epoch 2/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3424 - accuracy: 0.8442 - val_loss: 0.4663 - val_accuracy: 0.8453\n",
            "Epoch 3/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.4528 - accuracy: 0.8682 - val_loss: 0.3549 - val_accuracy: 0.8863\n",
            "Epoch 4/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3566 - accuracy: 0.8814 - val_loss: 0.3152 - val_accuracy: 0.8926\n",
            "Epoch 5/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3112 - accuracy: 0.8856 - val_loss: 0.2686 - val_accuracy: 0.8948\n",
            "Epoch 6/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3352 - accuracy: 0.8865 - val_loss: 0.2556 - val_accuracy: 0.8939\n",
            "Epoch 7/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3036 - accuracy: 0.8743 - val_loss: 0.3123 - val_accuracy: 0.8979\n",
            "Epoch 8/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3069 - accuracy: 0.8889 - val_loss: 0.2633 - val_accuracy: 0.8964\n",
            "Epoch 9/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2911 - accuracy: 0.8796 - val_loss: 0.7743 - val_accuracy: 0.4352\n",
            "Epoch 10/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.5912 - accuracy: 0.6960 - val_loss: 0.4045 - val_accuracy: 0.8978\n",
            "Epoch 11/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3896 - accuracy: 0.8874 - val_loss: 0.3148 - val_accuracy: 0.8938\n",
            "Epoch 12/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3076 - accuracy: 0.8879 - val_loss: 0.2720 - val_accuracy: 0.8939\n",
            "Epoch 13/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2777 - accuracy: 0.8888 - val_loss: 0.2551 - val_accuracy: 0.8962\n",
            "Epoch 14/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2601 - accuracy: 0.8920 - val_loss: 0.2466 - val_accuracy: 0.8983\n",
            "Epoch 15/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2526 - accuracy: 0.8926 - val_loss: 0.2406 - val_accuracy: 0.9005\n",
            "Epoch 16/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2537 - accuracy: 0.8938 - val_loss: 0.2615 - val_accuracy: 0.8939\n",
            "Epoch 17/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2556 - accuracy: 0.8857 - val_loss: 0.2294 - val_accuracy: 0.9024\n",
            "Epoch 18/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2439 - accuracy: 0.8967 - val_loss: 0.2290 - val_accuracy: 0.9037\n",
            "Epoch 19/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2388 - accuracy: 0.8960 - val_loss: 0.2258 - val_accuracy: 0.9076\n",
            "Epoch 20/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2312 - accuracy: 0.9011 - val_loss: 0.2252 - val_accuracy: 0.9087\n",
            "Epoch 21/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2326 - accuracy: 0.9005 - val_loss: 0.2261 - val_accuracy: 0.9082\n",
            "Epoch 22/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2698 - accuracy: 0.8997 - val_loss: 0.2255 - val_accuracy: 0.9071\n",
            "Epoch 23/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2352 - accuracy: 0.8986 - val_loss: 0.2694 - val_accuracy: 0.9008\n",
            "Epoch 24/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2670 - accuracy: 0.8917 - val_loss: 0.2300 - val_accuracy: 0.9019\n",
            "Epoch 25/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2501 - accuracy: 0.8944 - val_loss: 0.2279 - val_accuracy: 0.9066\n",
            "Epoch 26/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2343 - accuracy: 0.8974 - val_loss: 0.2217 - val_accuracy: 0.9082\n",
            "Epoch 27/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3187 - accuracy: 0.8877 - val_loss: 0.2256 - val_accuracy: 0.9080\n",
            "Epoch 28/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2368 - accuracy: 0.9022 - val_loss: 0.2243 - val_accuracy: 0.9092\n",
            "Epoch 29/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2309 - accuracy: 0.9032 - val_loss: 0.2242 - val_accuracy: 0.9099\n",
            "Epoch 30/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2419 - accuracy: 0.9032 - val_loss: 0.2271 - val_accuracy: 0.9106\n",
            "Epoch 31/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2388 - accuracy: 0.9050 - val_loss: 0.2218 - val_accuracy: 0.9109\n",
            "Epoch 32/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2528 - accuracy: 0.9053 - val_loss: 0.2283 - val_accuracy: 0.9071\n",
            "Epoch 33/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2420 - accuracy: 0.9033 - val_loss: 0.2208 - val_accuracy: 0.9056\n",
            "Epoch 34/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2413 - accuracy: 0.9015 - val_loss: 0.2250 - val_accuracy: 0.9067\n",
            "Epoch 35/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2430 - accuracy: 0.9034 - val_loss: 0.2258 - val_accuracy: 0.9116\n",
            "Epoch 36/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2481 - accuracy: 0.9038 - val_loss: 0.2235 - val_accuracy: 0.9105\n",
            "Epoch 37/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2390 - accuracy: 0.9099 - val_loss: 0.2237 - val_accuracy: 0.9108\n",
            "Epoch 38/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2542 - accuracy: 0.9044 - val_loss: 0.2509 - val_accuracy: 0.9037\n",
            "Epoch 39/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2494 - accuracy: 0.8995 - val_loss: 0.2473 - val_accuracy: 0.9112\n",
            "Epoch 40/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2365 - accuracy: 0.9038 - val_loss: 0.2201 - val_accuracy: 0.9063\n",
            "Epoch 41/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2212 - accuracy: 0.9015 - val_loss: 0.2188 - val_accuracy: 0.9085\n",
            "Epoch 42/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2301 - accuracy: 0.9052 - val_loss: 0.2181 - val_accuracy: 0.9104\n",
            "Epoch 43/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2296 - accuracy: 0.9041 - val_loss: 0.2334 - val_accuracy: 0.9054\n",
            "Epoch 44/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2487 - accuracy: 0.8969 - val_loss: 0.3111 - val_accuracy: 0.8935\n",
            "Epoch 45/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3189 - accuracy: 0.8901 - val_loss: 0.2943 - val_accuracy: 0.8989\n",
            "Epoch 46/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3017 - accuracy: 0.8983 - val_loss: 0.2778 - val_accuracy: 0.9041\n",
            "Epoch 47/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2950 - accuracy: 0.8990 - val_loss: 0.2621 - val_accuracy: 0.9069\n",
            "Epoch 48/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2643 - accuracy: 0.9001 - val_loss: 0.2409 - val_accuracy: 0.9077\n",
            "Epoch 49/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2431 - accuracy: 0.9035 - val_loss: 0.2331 - val_accuracy: 0.9081\n",
            "Epoch 50/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2507 - accuracy: 0.9024 - val_loss: 0.2234 - val_accuracy: 0.9084\n",
            "Epoch 51/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2370 - accuracy: 0.9043 - val_loss: 0.2208 - val_accuracy: 0.9093\n",
            "Epoch 52/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2498 - accuracy: 0.9031 - val_loss: 0.2228 - val_accuracy: 0.9071\n",
            "Epoch 53/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2324 - accuracy: 0.8988 - val_loss: 0.2209 - val_accuracy: 0.9080\n",
            "Epoch 54/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2297 - accuracy: 0.9013 - val_loss: 0.2183 - val_accuracy: 0.9074\n",
            "Epoch 55/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2196 - accuracy: 0.9034 - val_loss: 0.2173 - val_accuracy: 0.9093\n",
            "Epoch 56/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2403 - accuracy: 0.9064 - val_loss: 0.2251 - val_accuracy: 0.9085\n",
            "Epoch 57/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.4465 - accuracy: 0.8203 - val_loss: 0.2361 - val_accuracy: 0.9020\n",
            "Epoch 58/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2318 - accuracy: 0.8981 - val_loss: 0.2251 - val_accuracy: 0.9050\n",
            "Epoch 59/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2258 - accuracy: 0.9015 - val_loss: 0.2204 - val_accuracy: 0.9076\n",
            "Epoch 60/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2587 - accuracy: 0.8971 - val_loss: 0.2263 - val_accuracy: 0.9052\n",
            "Epoch 61/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2290 - accuracy: 0.9006 - val_loss: 0.2262 - val_accuracy: 0.9033\n",
            "Epoch 62/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2797 - accuracy: 0.8967 - val_loss: 0.2213 - val_accuracy: 0.9033\n",
            "Epoch 63/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2247 - accuracy: 0.8988 - val_loss: 0.2188 - val_accuracy: 0.9079\n",
            "Epoch 64/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2335 - accuracy: 0.9068 - val_loss: 0.2175 - val_accuracy: 0.9070\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hbi1X46W9709",
        "outputId": "b86ab920-dbab-417c-b3e7-588291d0d72c"
      },
      "source": [
        "model_all1 = Sequential()\n",
        "model_all1.add(Dense(8, input_dim = 20,activation='linear'))\n",
        "model_all1.add(Dense(4,activation='linear'))\n",
        "model_all1.add(Dense(1,activation='linear'))\n",
        "model_all1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "history = model_all1.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64 )\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training data', 'validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 2.4617 - accuracy: 0.6859 - val_loss: 0.3507 - val_accuracy: 0.8977\n",
            "Epoch 2/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.3353 - accuracy: 0.8934 - val_loss: 0.2718 - val_accuracy: 0.9032\n",
            "Epoch 3/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2776 - accuracy: 0.8977 - val_loss: 0.2672 - val_accuracy: 0.9074\n",
            "Epoch 4/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2556 - accuracy: 0.9011 - val_loss: 0.2308 - val_accuracy: 0.9062\n",
            "Epoch 5/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2488 - accuracy: 0.9020 - val_loss: 0.2555 - val_accuracy: 0.9073\n",
            "Epoch 6/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2764 - accuracy: 0.8906 - val_loss: 0.2375 - val_accuracy: 0.9024\n",
            "Epoch 7/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2700 - accuracy: 0.8924 - val_loss: 0.2343 - val_accuracy: 0.9016\n",
            "Epoch 8/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2435 - accuracy: 0.8964 - val_loss: 0.2228 - val_accuracy: 0.9041\n",
            "Epoch 9/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2325 - accuracy: 0.8971 - val_loss: 0.2306 - val_accuracy: 0.9079\n",
            "Epoch 10/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2354 - accuracy: 0.8997 - val_loss: 0.4289 - val_accuracy: 0.9011\n",
            "Epoch 11/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2592 - accuracy: 0.8982 - val_loss: 0.2214 - val_accuracy: 0.9060\n",
            "Epoch 12/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2243 - accuracy: 0.8991 - val_loss: 0.2202 - val_accuracy: 0.9083\n",
            "Epoch 13/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2309 - accuracy: 0.8995 - val_loss: 0.2192 - val_accuracy: 0.9084\n",
            "Epoch 14/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2409 - accuracy: 0.9030 - val_loss: 1.1429 - val_accuracy: 0.2951\n",
            "Epoch 15/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.6383 - accuracy: 0.6693 - val_loss: 0.3622 - val_accuracy: 0.9026\n",
            "Epoch 16/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3839 - accuracy: 0.8949 - val_loss: 0.2801 - val_accuracy: 0.9050\n",
            "Epoch 17/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.3003 - accuracy: 0.9023 - val_loss: 0.2574 - val_accuracy: 0.9067\n",
            "Epoch 18/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2441 - accuracy: 0.9028 - val_loss: 0.2271 - val_accuracy: 0.9066\n",
            "Epoch 19/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2291 - accuracy: 0.9019 - val_loss: 0.2218 - val_accuracy: 0.9068\n",
            "Epoch 20/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2365 - accuracy: 0.9019 - val_loss: 0.2195 - val_accuracy: 0.9077\n",
            "Epoch 21/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2356 - accuracy: 0.8967 - val_loss: 0.2356 - val_accuracy: 0.9062\n",
            "Epoch 22/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2347 - accuracy: 0.9014 - val_loss: 0.2208 - val_accuracy: 0.9064\n",
            "Epoch 23/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2645 - accuracy: 0.8847 - val_loss: 0.2378 - val_accuracy: 0.9037\n",
            "Epoch 24/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2288 - accuracy: 0.8971 - val_loss: 0.2234 - val_accuracy: 0.9049\n",
            "Epoch 25/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2291 - accuracy: 0.9017 - val_loss: 0.2198 - val_accuracy: 0.9071\n",
            "Epoch 26/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2229 - accuracy: 0.9019 - val_loss: 0.2753 - val_accuracy: 0.9131\n",
            "Epoch 27/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2299 - accuracy: 0.9053 - val_loss: 0.2164 - val_accuracy: 0.9087\n",
            "Epoch 28/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2288 - accuracy: 0.9010 - val_loss: 0.2179 - val_accuracy: 0.9076\n",
            "Epoch 29/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2388 - accuracy: 0.9013 - val_loss: 0.2339 - val_accuracy: 0.9057\n",
            "Epoch 30/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2433 - accuracy: 0.9019 - val_loss: 0.2158 - val_accuracy: 0.9068\n",
            "Epoch 31/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2334 - accuracy: 0.9019 - val_loss: 0.2507 - val_accuracy: 0.9104\n",
            "Epoch 32/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2515 - accuracy: 0.8987 - val_loss: 0.2161 - val_accuracy: 0.9072\n",
            "Epoch 33/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2421 - accuracy: 0.8986 - val_loss: 0.2272 - val_accuracy: 0.9035\n",
            "Epoch 34/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2262 - accuracy: 0.9003 - val_loss: 0.2223 - val_accuracy: 0.9084\n",
            "Epoch 35/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2306 - accuracy: 0.9030 - val_loss: 0.2464 - val_accuracy: 0.9008\n",
            "Epoch 36/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2375 - accuracy: 0.8971 - val_loss: 0.2175 - val_accuracy: 0.9050\n",
            "Epoch 37/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2236 - accuracy: 0.9028 - val_loss: 0.2204 - val_accuracy: 0.9073\n",
            "Epoch 38/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2341 - accuracy: 0.9003 - val_loss: 0.2249 - val_accuracy: 0.9061\n",
            "Epoch 39/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2250 - accuracy: 0.9026 - val_loss: 0.2204 - val_accuracy: 0.9084\n",
            "Epoch 40/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2409 - accuracy: 0.8988 - val_loss: 0.2239 - val_accuracy: 0.9081\n",
            "Epoch 41/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2350 - accuracy: 0.9029 - val_loss: 0.2228 - val_accuracy: 0.9094\n",
            "Epoch 42/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2330 - accuracy: 0.9032 - val_loss: 0.2184 - val_accuracy: 0.9076\n",
            "Epoch 43/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2281 - accuracy: 0.9042 - val_loss: 0.2215 - val_accuracy: 0.9086\n",
            "Epoch 44/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2340 - accuracy: 0.9027 - val_loss: 0.2294 - val_accuracy: 0.9095\n",
            "Epoch 45/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2486 - accuracy: 0.9050 - val_loss: 0.2227 - val_accuracy: 0.9052\n",
            "Epoch 46/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2301 - accuracy: 0.9024 - val_loss: 0.2281 - val_accuracy: 0.9054\n",
            "Epoch 47/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2363 - accuracy: 0.9033 - val_loss: 0.2169 - val_accuracy: 0.9075\n",
            "Epoch 48/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2194 - accuracy: 0.9035 - val_loss: 0.2333 - val_accuracy: 0.9134\n",
            "Epoch 49/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2315 - accuracy: 0.9060 - val_loss: 0.2257 - val_accuracy: 0.9054\n",
            "Epoch 50/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2446 - accuracy: 0.8980 - val_loss: 0.2334 - val_accuracy: 0.9058\n",
            "Epoch 51/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2281 - accuracy: 0.9018 - val_loss: 0.2175 - val_accuracy: 0.9089\n",
            "Epoch 52/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2227 - accuracy: 0.9058 - val_loss: 0.2234 - val_accuracy: 0.9101\n",
            "Epoch 53/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2340 - accuracy: 0.9071 - val_loss: 0.2235 - val_accuracy: 0.9109\n",
            "Epoch 54/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2461 - accuracy: 0.9057 - val_loss: 0.2206 - val_accuracy: 0.9088\n",
            "Epoch 55/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2358 - accuracy: 0.9016 - val_loss: 0.2302 - val_accuracy: 0.9090\n",
            "Epoch 56/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2426 - accuracy: 0.9025 - val_loss: 0.2225 - val_accuracy: 0.9114\n",
            "Epoch 57/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2493 - accuracy: 0.9023 - val_loss: 0.2242 - val_accuracy: 0.9097\n",
            "Epoch 58/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2324 - accuracy: 0.9052 - val_loss: 0.2238 - val_accuracy: 0.9098\n",
            "Epoch 59/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2340 - accuracy: 0.9059 - val_loss: 0.2275 - val_accuracy: 0.9085\n",
            "Epoch 60/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2550 - accuracy: 0.9043 - val_loss: 0.2271 - val_accuracy: 0.9101\n",
            "Epoch 61/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2270 - accuracy: 0.9059 - val_loss: 0.2227 - val_accuracy: 0.9069\n",
            "Epoch 62/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2357 - accuracy: 0.9003 - val_loss: 0.2202 - val_accuracy: 0.9060\n",
            "Epoch 63/64\n",
            "901/901 [==============================] - 1s 2ms/step - loss: 0.2266 - accuracy: 0.9031 - val_loss: 0.2168 - val_accuracy: 0.9078\n",
            "Epoch 64/64\n",
            "901/901 [==============================] - 2s 2ms/step - loss: 0.2587 - accuracy: 0.9024 - val_loss: 0.2395 - val_accuracy: 0.9011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwCLDhYasGVH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXWL-adhGVvE"
      },
      "source": [
        "**Significance of Each feature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V4pD7tNtvY3",
        "outputId": "bddc2131-0d72-4aa2-bb68-5d423a936867"
      },
      "source": [
        "acc = []\n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "for i in range(20):\n",
        "  X = dataset[:,i]\n",
        "  Y = dataset[:,-1]\n",
        "  XVALID = X[:index]\n",
        "  YVALID = Y[:index]\n",
        "  XTRAIN = X[index:]\n",
        "  YTRAIN = Y[index:]\n",
        "  model1 = Sequential()\n",
        "  model1.add(Dense(8, input_dim = 1,activation='relu'))\n",
        "  model1.add(Dense(4,activation='relu'))\n",
        "  model1.add(Dense(1,activation='sigmoid'))\n",
        "  model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  callback_a = ModelCheckpoint(filepath = 'model1.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "  # The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "  callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=40, verbose=1)\n",
        "  history1 = model1.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64,batch_size = 30, callbacks=[callback_a,callback_b])\n",
        "  model1.load_weights('model1.hdf5')\n",
        "  p1 = model1.predict(XVALID)\n",
        "  accuracy = model1.evaluate(XVALID, YVALID)\n",
        "  acc.append(accuracy[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.6179 - accuracy: 0.8466 - val_loss: 0.4617 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4332 - accuracy: 0.8869 - val_loss: 0.3846 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3689 - accuracy: 0.8898 - val_loss: 0.3628 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3571 - accuracy: 0.8864 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3528 - accuracy: 0.8861 - val_loss: 0.3537 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3399 - accuracy: 0.8914 - val_loss: 0.3521 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.88423 to 0.88431, saving model to model1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3407 - accuracy: 0.8903 - val_loss: 0.3513 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88431\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3407 - accuracy: 0.8892 - val_loss: 0.3509 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88431\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3398 - accuracy: 0.8903 - val_loss: 0.3508 - val_accuracy: 0.8843\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88431\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3429 - accuracy: 0.8881 - val_loss: 0.3505 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.88431 to 0.88472, saving model to model1.hdf5\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3402 - accuracy: 0.8893 - val_loss: 0.3505 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.88472 to 0.88496, saving model to model1.hdf5\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3429 - accuracy: 0.8885 - val_loss: 0.3505 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88496\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3457 - accuracy: 0.8869 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88496\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3490 - accuracy: 0.8851 - val_loss: 0.3505 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88496\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3425 - accuracy: 0.8890 - val_loss: 0.3505 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88496\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3476 - accuracy: 0.8863 - val_loss: 0.3505 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88496\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3448 - accuracy: 0.8871 - val_loss: 0.3504 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88496\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3400 - accuracy: 0.8899 - val_loss: 0.3504 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88496\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3403 - accuracy: 0.8895 - val_loss: 0.3504 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88496\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3439 - accuracy: 0.8881 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88496\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3380 - accuracy: 0.8907 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88496\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3395 - accuracy: 0.8901 - val_loss: 0.3504 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88496\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3372 - accuracy: 0.8908 - val_loss: 0.3504 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88496\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3413 - accuracy: 0.8895 - val_loss: 0.3506 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88496\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3394 - accuracy: 0.8899 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88496\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3441 - accuracy: 0.8877 - val_loss: 0.3505 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88496\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3497 - accuracy: 0.8853 - val_loss: 0.3505 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88496\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3425 - accuracy: 0.8886 - val_loss: 0.3506 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88496\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3431 - accuracy: 0.8887 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88496\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 1s 1ms/step - loss: 0.3453 - accuracy: 0.8865 - val_loss: 0.3505 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88496\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3375 - accuracy: 0.8911 - val_loss: 0.3504 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88496\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3407 - accuracy: 0.8893 - val_loss: 0.3505 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88496\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3444 - accuracy: 0.8876 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88496\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3410 - accuracy: 0.8893 - val_loss: 0.3507 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88496\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8880 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88496\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3478 - accuracy: 0.8858 - val_loss: 0.3504 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88496\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3477 - accuracy: 0.8861 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88496\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3461 - accuracy: 0.8869 - val_loss: 0.3506 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88496\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3424 - accuracy: 0.8889 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88496\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3368 - accuracy: 0.8911 - val_loss: 0.3504 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88496\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3392 - accuracy: 0.8901 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88496\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3418 - accuracy: 0.8886 - val_loss: 0.3504 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88496\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3393 - accuracy: 0.8900 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88496\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3403 - accuracy: 0.8891 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88496\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3428 - accuracy: 0.8881 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88496\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3354 - accuracy: 0.8914 - val_loss: 0.3504 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88496\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3477 - accuracy: 0.8863 - val_loss: 0.3506 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88496\n",
            "Epoch 00047: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3505 - accuracy: 0.8850\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.8428 - val_loss: 0.3648 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3467 - accuracy: 0.8914 - val_loss: 0.3590 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3503 - accuracy: 0.8882 - val_loss: 0.3571 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3468 - accuracy: 0.8895 - val_loss: 0.3566 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3475 - accuracy: 0.8892 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3519 - accuracy: 0.8869 - val_loss: 0.3564 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3483 - accuracy: 0.8889 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3481 - accuracy: 0.8888 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3507 - accuracy: 0.8878 - val_loss: 0.3564 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3481 - accuracy: 0.8890 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8890 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3528 - accuracy: 0.8865 - val_loss: 0.3564 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3410 - accuracy: 0.8922 - val_loss: 0.3564 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3497 - accuracy: 0.8879 - val_loss: 0.3564 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3494 - accuracy: 0.8881 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3574 - accuracy: 0.8845 - val_loss: 0.3565 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3421 - accuracy: 0.8919 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3490 - accuracy: 0.8885 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3525 - accuracy: 0.8867 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3507 - accuracy: 0.8876 - val_loss: 0.3575 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8898 - val_loss: 0.3565 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3476 - accuracy: 0.8891 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3469 - accuracy: 0.8894 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8887 - val_loss: 0.3565 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3527 - accuracy: 0.8865 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3464 - accuracy: 0.8896 - val_loss: 0.3565 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3548 - accuracy: 0.8857 - val_loss: 0.3562 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3506 - accuracy: 0.8876 - val_loss: 0.3566 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3438 - accuracy: 0.8906 - val_loss: 0.3582 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3560 - accuracy: 0.8850 - val_loss: 0.3573 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3531 - accuracy: 0.8863 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3498 - accuracy: 0.8880 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3513 - accuracy: 0.8873 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3456 - accuracy: 0.8899 - val_loss: 0.3562 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3450 - accuracy: 0.8904 - val_loss: 0.3563 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3515 - accuracy: 0.8872 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8881 - val_loss: 0.3562 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8893 - val_loss: 0.3562 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3471 - accuracy: 0.8892 - val_loss: 0.3564 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3479 - accuracy: 0.8890 - val_loss: 0.3564 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3465 - accuracy: 0.8896 - val_loss: 0.3566 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 1ms/step - loss: 0.3648 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4356 - accuracy: 0.8885 - val_loss: 0.3600 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3507 - accuracy: 0.8890 - val_loss: 0.3580 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3508 - accuracy: 0.8874 - val_loss: 0.3573 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3498 - accuracy: 0.8881 - val_loss: 0.3571 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3512 - accuracy: 0.8875 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3505 - accuracy: 0.8878 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3432 - accuracy: 0.8909 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8899 - val_loss: 0.3572 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3449 - accuracy: 0.8904 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3441 - accuracy: 0.8909 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3545 - accuracy: 0.8856 - val_loss: 0.3576 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3533 - accuracy: 0.8863 - val_loss: 0.3571 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8901 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3538 - accuracy: 0.8860 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3421 - accuracy: 0.8917 - val_loss: 0.3579 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3507 - accuracy: 0.8874 - val_loss: 0.3572 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3528 - accuracy: 0.8864 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3443 - accuracy: 0.8907 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3503 - accuracy: 0.8877 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3522 - accuracy: 0.8866 - val_loss: 0.3574 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3532 - accuracy: 0.8861 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3466 - accuracy: 0.8897 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3490 - accuracy: 0.8881 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3465 - accuracy: 0.8896 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3585 - accuracy: 0.8837 - val_loss: 0.3579 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3446 - accuracy: 0.8902 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3500 - accuracy: 0.8879 - val_loss: 0.3571 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3466 - accuracy: 0.8894 - val_loss: 0.3572 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8899 - val_loss: 0.3572 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3447 - accuracy: 0.8901 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8883 - val_loss: 0.3574 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3497 - accuracy: 0.8877 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3505 - accuracy: 0.8877 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3508 - accuracy: 0.8874 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3530 - accuracy: 0.8863 - val_loss: 0.3575 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3504 - accuracy: 0.8877 - val_loss: 0.3573 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3457 - accuracy: 0.8901 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3486 - accuracy: 0.8884 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3457 - accuracy: 0.8897 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3476 - accuracy: 0.8887 - val_loss: 0.3573 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3475 - accuracy: 0.8890 - val_loss: 0.3572 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 1ms/step - loss: 0.3600 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.5334 - accuracy: 0.7821 - val_loss: 0.3636 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3599 - accuracy: 0.8852 - val_loss: 0.3605 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3568 - accuracy: 0.8856 - val_loss: 0.3579 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8906 - val_loss: 0.3574 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3525 - accuracy: 0.8869 - val_loss: 0.3577 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8889 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3482 - accuracy: 0.8887 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3478 - accuracy: 0.8889 - val_loss: 0.3576 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3414 - accuracy: 0.8919 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3415 - accuracy: 0.8918 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3471 - accuracy: 0.8890 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3491 - accuracy: 0.8886 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3495 - accuracy: 0.8879 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8902 - val_loss: 0.3578 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3465 - accuracy: 0.8895 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3494 - accuracy: 0.8879 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3519 - accuracy: 0.8868 - val_loss: 0.3575 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3512 - accuracy: 0.8875 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3467 - accuracy: 0.8893 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3479 - accuracy: 0.8884 - val_loss: 0.3575 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3462 - accuracy: 0.8895 - val_loss: 0.3571 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3521 - accuracy: 0.8868 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.3457 - accuracy: 0.8898 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3469 - accuracy: 0.8893 - val_loss: 0.3576 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3501 - accuracy: 0.8878 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3533 - accuracy: 0.8861 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3441 - accuracy: 0.8908 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3538 - accuracy: 0.8857 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3472 - accuracy: 0.8892 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8887 - val_loss: 0.3568 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3463 - accuracy: 0.8895 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3507 - accuracy: 0.8876 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8877 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3533 - accuracy: 0.8863 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3475 - accuracy: 0.8891 - val_loss: 0.3572 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3474 - accuracy: 0.8890 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8876 - val_loss: 0.3572 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3565 - accuracy: 0.8848 - val_loss: 0.3570 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3453 - accuracy: 0.8899 - val_loss: 0.3577 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8878 - val_loss: 0.3567 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3477 - accuracy: 0.8888 - val_loss: 0.3571 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3636 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4770 - accuracy: 0.8876 - val_loss: 0.3529 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3483 - accuracy: 0.8864 - val_loss: 0.3526 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8878 - val_loss: 0.3526 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3419 - accuracy: 0.8895 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8856 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3500 - accuracy: 0.8855 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3467 - accuracy: 0.8870 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8892 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3438 - accuracy: 0.8883 - val_loss: 0.3526 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3517 - accuracy: 0.8854 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3448 - accuracy: 0.8884 - val_loss: 0.3529 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3467 - accuracy: 0.8876 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3440 - accuracy: 0.8886 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3487 - accuracy: 0.8862 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3445 - accuracy: 0.8888 - val_loss: 0.3526 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3425 - accuracy: 0.8891 - val_loss: 0.3535 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3393 - accuracy: 0.8911 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3484 - accuracy: 0.8863 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3395 - accuracy: 0.8905 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3473 - accuracy: 0.8866 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3402 - accuracy: 0.8902 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3373 - accuracy: 0.8918 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3485 - accuracy: 0.8862 - val_loss: 0.3533 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8884 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3453 - accuracy: 0.8882 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3407 - accuracy: 0.8905 - val_loss: 0.3529 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3475 - accuracy: 0.8873 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3402 - accuracy: 0.8909 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3458 - accuracy: 0.8880 - val_loss: 0.3530 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3442 - accuracy: 0.8883 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3368 - accuracy: 0.8917 - val_loss: 0.3526 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3475 - accuracy: 0.8867 - val_loss: 0.3529 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3494 - accuracy: 0.8869 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3464 - accuracy: 0.8874 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3411 - accuracy: 0.8898 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3435 - accuracy: 0.8891 - val_loss: 0.3531 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3426 - accuracy: 0.8890 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3471 - accuracy: 0.8871 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3443 - accuracy: 0.8883 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8882 - val_loss: 0.3524 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3446 - accuracy: 0.8883 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3529 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.6197 - accuracy: 0.8351 - val_loss: 0.4616 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4321 - accuracy: 0.8881 - val_loss: 0.3847 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3694 - accuracy: 0.8895 - val_loss: 0.3632 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3517 - accuracy: 0.8897 - val_loss: 0.3588 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3462 - accuracy: 0.8905 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3499 - accuracy: 0.8884 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3495 - accuracy: 0.8886 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3442 - accuracy: 0.8911 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3489 - accuracy: 0.8889 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3501 - accuracy: 0.8883 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3533 - accuracy: 0.8867 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3437 - accuracy: 0.8914 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8915 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3491 - accuracy: 0.8888 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3582 - accuracy: 0.8844 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3536 - accuracy: 0.8866 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3481 - accuracy: 0.8892 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3478 - accuracy: 0.8894 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3528 - accuracy: 0.8870 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3469 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3465 - accuracy: 0.8900 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3427 - accuracy: 0.8918 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3466 - accuracy: 0.8900 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3559 - accuracy: 0.8855 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3500 - accuracy: 0.8883 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3473 - accuracy: 0.8896 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3429 - accuracy: 0.8917 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3461 - accuracy: 0.8902 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3524 - accuracy: 0.8872 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3461 - accuracy: 0.8902 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3543 - accuracy: 0.8863 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8893 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3494 - accuracy: 0.8886 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3498 - accuracy: 0.8884 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3478 - accuracy: 0.8894 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3458 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8905 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3449 - accuracy: 0.8908 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3565 - accuracy: 0.8852 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3442 - accuracy: 0.8911 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.4616 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4772 - accuracy: 0.8727 - val_loss: 0.3595 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3537 - accuracy: 0.8868 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8887 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3503 - accuracy: 0.8883 - val_loss: 0.3587 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8906 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3519 - accuracy: 0.8875 - val_loss: 0.3587 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3452 - accuracy: 0.8908 - val_loss: 0.3587 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3515 - accuracy: 0.8876 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3523 - accuracy: 0.8873 - val_loss: 0.3587 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3486 - accuracy: 0.8891 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3550 - accuracy: 0.8859 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3474 - accuracy: 0.8897 - val_loss: 0.3589 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3473 - accuracy: 0.8897 - val_loss: 0.3588 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3546 - accuracy: 0.8862 - val_loss: 0.3594 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3506 - accuracy: 0.8880 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3478 - accuracy: 0.8894 - val_loss: 0.3591 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3528 - accuracy: 0.8871 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3498 - accuracy: 0.8885 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3462 - accuracy: 0.8903 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3493 - accuracy: 0.8888 - val_loss: 0.3591 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8894 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3471 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3504 - accuracy: 0.8883 - val_loss: 0.3588 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8884 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3468 - accuracy: 0.8899 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3546 - accuracy: 0.8862 - val_loss: 0.3587 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8907 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3483 - accuracy: 0.8892 - val_loss: 0.3596 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8894 - val_loss: 0.3595 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3520 - accuracy: 0.8876 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8883 - val_loss: 0.3587 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3457 - accuracy: 0.8905 - val_loss: 0.3593 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8898 - val_loss: 0.3594 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3402 - accuracy: 0.8930 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3404 - accuracy: 0.8930 - val_loss: 0.3594 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3525 - accuracy: 0.8872 - val_loss: 0.3591 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3472 - accuracy: 0.8897 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3425 - accuracy: 0.8920 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3486 - accuracy: 0.8891 - val_loss: 0.3587 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3444 - accuracy: 0.8911 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3595 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.6174 - accuracy: 0.8761 - val_loss: 0.4620 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4311 - accuracy: 0.8895 - val_loss: 0.3849 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3718 - accuracy: 0.8880 - val_loss: 0.3632 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3539 - accuracy: 0.8884 - val_loss: 0.3589 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3520 - accuracy: 0.8876 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3441 - accuracy: 0.8912 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3537 - accuracy: 0.8866 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3458 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3506 - accuracy: 0.8881 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3533 - accuracy: 0.8867 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3570 - accuracy: 0.8850 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3528 - accuracy: 0.8870 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3548 - accuracy: 0.8860 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3518 - accuracy: 0.8875 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3534 - accuracy: 0.8867 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3497 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3527 - accuracy: 0.8870 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3448 - accuracy: 0.8908 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3487 - accuracy: 0.8889 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3535 - accuracy: 0.8866 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3527 - accuracy: 0.8870 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3504 - accuracy: 0.8881 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3564 - accuracy: 0.8853 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3424 - accuracy: 0.8920 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3458 - accuracy: 0.8904 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3483 - accuracy: 0.8891 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3472 - accuracy: 0.8897 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3538 - accuracy: 0.8865 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3439 - accuracy: 0.8913 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3505 - accuracy: 0.8881 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3516 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3519 - accuracy: 0.8874 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3490 - accuracy: 0.8888 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3494 - accuracy: 0.8886 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3515 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3511 - accuracy: 0.8878 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3476 - accuracy: 0.8895 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3547 - accuracy: 0.8861 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8905 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.4620 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4066 - accuracy: 0.8857 - val_loss: 0.3688 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3598 - accuracy: 0.8872 - val_loss: 0.3619 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3494 - accuracy: 0.8899 - val_loss: 0.3595 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3498 - accuracy: 0.8880 - val_loss: 0.3556 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8893 - val_loss: 0.3547 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8872 - val_loss: 0.3544 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3446 - accuracy: 0.8894 - val_loss: 0.3536 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3386 - accuracy: 0.8918 - val_loss: 0.3513 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3433 - accuracy: 0.8884 - val_loss: 0.3497 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3381 - accuracy: 0.8893 - val_loss: 0.3457 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3342 - accuracy: 0.8906 - val_loss: 0.3441 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3344 - accuracy: 0.8897 - val_loss: 0.3443 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3406 - accuracy: 0.8864 - val_loss: 0.3428 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3377 - accuracy: 0.8876 - val_loss: 0.3436 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3359 - accuracy: 0.8889 - val_loss: 0.3427 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3395 - accuracy: 0.8880 - val_loss: 0.3425 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3335 - accuracy: 0.8884 - val_loss: 0.3432 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3343 - accuracy: 0.8891 - val_loss: 0.3440 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3361 - accuracy: 0.8882 - val_loss: 0.3441 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3316 - accuracy: 0.8905 - val_loss: 0.3426 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3306 - accuracy: 0.8906 - val_loss: 0.3427 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3320 - accuracy: 0.8910 - val_loss: 0.3427 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3349 - accuracy: 0.8895 - val_loss: 0.3426 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3380 - accuracy: 0.8873 - val_loss: 0.3428 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3362 - accuracy: 0.8897 - val_loss: 0.3427 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3377 - accuracy: 0.8881 - val_loss: 0.3428 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3393 - accuracy: 0.8875 - val_loss: 0.3430 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3351 - accuracy: 0.8893 - val_loss: 0.3429 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3363 - accuracy: 0.8876 - val_loss: 0.3445 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3351 - accuracy: 0.8886 - val_loss: 0.3427 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3369 - accuracy: 0.8866 - val_loss: 0.3428 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3322 - accuracy: 0.8903 - val_loss: 0.3458 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3336 - accuracy: 0.8899 - val_loss: 0.3426 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3323 - accuracy: 0.8897 - val_loss: 0.3432 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3363 - accuracy: 0.8897 - val_loss: 0.3426 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3406 - accuracy: 0.8859 - val_loss: 0.3429 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3322 - accuracy: 0.8900 - val_loss: 0.3427 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3358 - accuracy: 0.8887 - val_loss: 0.3428 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3327 - accuracy: 0.8900 - val_loss: 0.3440 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3326 - accuracy: 0.8907 - val_loss: 0.3430 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3351 - accuracy: 0.8887 - val_loss: 0.3428 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3354 - accuracy: 0.8899 - val_loss: 0.3429 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88423\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3367 - accuracy: 0.8884 - val_loss: 0.3428 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88423\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3286 - accuracy: 0.8906 - val_loss: 0.3427 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88423\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3415 - accuracy: 0.8854 - val_loss: 0.3427 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88423\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3357 - accuracy: 0.8895 - val_loss: 0.3427 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88423\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3358 - accuracy: 0.8883 - val_loss: 0.3426 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88423\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3399 - accuracy: 0.8875 - val_loss: 0.3439 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88423\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3382 - accuracy: 0.8880 - val_loss: 0.3433 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88423\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3302 - accuracy: 0.8905 - val_loss: 0.3428 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.88423\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3384 - accuracy: 0.8866 - val_loss: 0.3446 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88423\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3370 - accuracy: 0.8879 - val_loss: 0.3433 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88423\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3418 - accuracy: 0.8861 - val_loss: 0.3426 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88423\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3311 - accuracy: 0.8908 - val_loss: 0.3428 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88423\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8859 - val_loss: 0.3434 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88423\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3303 - accuracy: 0.8920 - val_loss: 0.3427 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.88423\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3363 - accuracy: 0.8884 - val_loss: 0.3426 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.88423\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3302 - accuracy: 0.8911 - val_loss: 0.3427 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.88423\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3367 - accuracy: 0.8891 - val_loss: 0.3438 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.88423\n",
            "Epoch 00059: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3688 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.6398 - accuracy: 0.7453 - val_loss: 0.4650 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4351 - accuracy: 0.8879 - val_loss: 0.3861 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3717 - accuracy: 0.8887 - val_loss: 0.3635 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3565 - accuracy: 0.8871 - val_loss: 0.3588 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8885 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3499 - accuracy: 0.8884 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3535 - accuracy: 0.8866 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3501 - accuracy: 0.8883 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3466 - accuracy: 0.8900 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3466 - accuracy: 0.8900 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3473 - accuracy: 0.8896 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3473 - accuracy: 0.8896 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3551 - accuracy: 0.8859 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3460 - accuracy: 0.8902 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8905 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3578 - accuracy: 0.8845 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3449 - accuracy: 0.8908 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3410 - accuracy: 0.8926 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3534 - accuracy: 0.8867 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3465 - accuracy: 0.8900 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3522 - accuracy: 0.8873 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3495 - accuracy: 0.8886 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3520 - accuracy: 0.8874 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3527 - accuracy: 0.8870 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3484 - accuracy: 0.8891 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3535 - accuracy: 0.8867 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3422 - accuracy: 0.8921 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3429 - accuracy: 0.8918 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3479 - accuracy: 0.8893 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3513 - accuracy: 0.8877 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3514 - accuracy: 0.8877 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3541 - accuracy: 0.8864 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3459 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3549 - accuracy: 0.8860 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3483 - accuracy: 0.8892 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3520 - accuracy: 0.8874 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3526 - accuracy: 0.8871 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3516 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3515 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.4650 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.5304 - accuracy: 0.8562 - val_loss: 0.3086 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88544, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2897 - accuracy: 0.8949 - val_loss: 0.2988 - val_accuracy: 0.8881\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88544 to 0.88812, saving model to model1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2971 - accuracy: 0.8914 - val_loss: 0.2988 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88812 to 0.88868, saving model to model1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2946 - accuracy: 0.8936 - val_loss: 0.2986 - val_accuracy: 0.8899\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.88868 to 0.88990, saving model to model1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2902 - accuracy: 0.8955 - val_loss: 0.2985 - val_accuracy: 0.8901\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.88990 to 0.89014, saving model to model1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2898 - accuracy: 0.8936 - val_loss: 0.2998 - val_accuracy: 0.8919\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.89014 to 0.89192, saving model to model1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.8961 - val_loss: 0.2985 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.89192\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8977 - val_loss: 0.2990 - val_accuracy: 0.8912\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.89192\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.8955 - val_loss: 0.2978 - val_accuracy: 0.8897\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.89192\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2884 - accuracy: 0.8933 - val_loss: 0.2983 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.89192\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2892 - accuracy: 0.8956 - val_loss: 0.2975 - val_accuracy: 0.8888\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.89192\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2890 - accuracy: 0.8955 - val_loss: 0.2958 - val_accuracy: 0.8919\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.89192\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2902 - accuracy: 0.8937 - val_loss: 0.2940 - val_accuracy: 0.8910\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.89192\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.8943 - val_loss: 0.2931 - val_accuracy: 0.8909\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89192\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2875 - accuracy: 0.8931 - val_loss: 0.2897 - val_accuracy: 0.8900\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.89192\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2772 - accuracy: 0.8965 - val_loss: 0.2895 - val_accuracy: 0.8905\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.89192\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2851 - accuracy: 0.8919 - val_loss: 0.2881 - val_accuracy: 0.8898\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.89192\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2811 - accuracy: 0.8932 - val_loss: 0.2877 - val_accuracy: 0.8914\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.89192\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2792 - accuracy: 0.8960 - val_loss: 0.2882 - val_accuracy: 0.8908\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89192\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2823 - accuracy: 0.8942 - val_loss: 0.2887 - val_accuracy: 0.8874\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89192\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2820 - accuracy: 0.8936 - val_loss: 0.2875 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89192\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2795 - accuracy: 0.8959 - val_loss: 0.2875 - val_accuracy: 0.8902\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89192\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2823 - accuracy: 0.8937 - val_loss: 0.2875 - val_accuracy: 0.8884\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89192\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.8928 - val_loss: 0.2876 - val_accuracy: 0.8897\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89192\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2772 - accuracy: 0.8957 - val_loss: 0.2874 - val_accuracy: 0.8879\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89192\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2757 - accuracy: 0.8967 - val_loss: 0.2874 - val_accuracy: 0.8878\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89192\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2809 - accuracy: 0.8945 - val_loss: 0.2874 - val_accuracy: 0.8888\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.89192\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2793 - accuracy: 0.8949 - val_loss: 0.2874 - val_accuracy: 0.8879\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.89192\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2764 - accuracy: 0.8955 - val_loss: 0.2875 - val_accuracy: 0.8878\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89192\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2812 - accuracy: 0.8948 - val_loss: 0.2878 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89192\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2785 - accuracy: 0.8956 - val_loss: 0.2874 - val_accuracy: 0.8896\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89192\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2727 - accuracy: 0.9008 - val_loss: 0.2875 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89192\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2880 - accuracy: 0.8901 - val_loss: 0.2880 - val_accuracy: 0.8871\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89192\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2802 - accuracy: 0.8959 - val_loss: 0.2875 - val_accuracy: 0.8884\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89192\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2794 - accuracy: 0.8959 - val_loss: 0.2881 - val_accuracy: 0.8874\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89192\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2768 - accuracy: 0.8965 - val_loss: 0.2874 - val_accuracy: 0.8886\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89192\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8898 - val_loss: 0.2893 - val_accuracy: 0.8876\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89192\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2851 - accuracy: 0.8936 - val_loss: 0.2874 - val_accuracy: 0.8898\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89192\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.8922 - val_loss: 0.2873 - val_accuracy: 0.8884\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89192\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2803 - accuracy: 0.8938 - val_loss: 0.2873 - val_accuracy: 0.8885\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89192\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2796 - accuracy: 0.8958 - val_loss: 0.2875 - val_accuracy: 0.8897\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89192\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2998 - accuracy: 0.8919\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4789 - accuracy: 0.8813 - val_loss: 0.3559 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3495 - accuracy: 0.8874 - val_loss: 0.3569 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3458 - accuracy: 0.8890 - val_loss: 0.3558 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3472 - accuracy: 0.8883 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8883 - val_loss: 0.3557 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3460 - accuracy: 0.8890 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8893 - val_loss: 0.3555 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3474 - accuracy: 0.8880 - val_loss: 0.3555 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8893 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3466 - accuracy: 0.8889 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3471 - accuracy: 0.8883 - val_loss: 0.3555 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8896 - val_loss: 0.3552 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3472 - accuracy: 0.8883 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3414 - accuracy: 0.8909 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3421 - accuracy: 0.8906 - val_loss: 0.3557 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3503 - accuracy: 0.8873 - val_loss: 0.3556 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3517 - accuracy: 0.8864 - val_loss: 0.3555 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3476 - accuracy: 0.8882 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3485 - accuracy: 0.8878 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3420 - accuracy: 0.8907 - val_loss: 0.3552 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3406 - accuracy: 0.8915 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3417 - accuracy: 0.8911 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3494 - accuracy: 0.8871 - val_loss: 0.3556 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3484 - accuracy: 0.8876 - val_loss: 0.3559 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8874 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3464 - accuracy: 0.8884 - val_loss: 0.3552 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3423 - accuracy: 0.8908 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8904 - val_loss: 0.3552 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3450 - accuracy: 0.8892 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3500 - accuracy: 0.8870 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3519 - accuracy: 0.8857 - val_loss: 0.3555 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3473 - accuracy: 0.8882 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8884 - val_loss: 0.3552 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3457 - accuracy: 0.8891 - val_loss: 0.3553 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3487 - accuracy: 0.8876 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3504 - accuracy: 0.8865 - val_loss: 0.3560 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3491 - accuracy: 0.8872 - val_loss: 0.3552 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8892 - val_loss: 0.3554 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3442 - accuracy: 0.8894 - val_loss: 0.3555 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3477 - accuracy: 0.8880 - val_loss: 0.3552 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3508 - accuracy: 0.8865 - val_loss: 0.3557 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3559 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.5406 - accuracy: 0.6950 - val_loss: 0.3398 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3293 - accuracy: 0.8893 - val_loss: 0.3366 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3227 - accuracy: 0.8917 - val_loss: 0.3339 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3351 - accuracy: 0.8843 - val_loss: 0.3327 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3201 - accuracy: 0.8895 - val_loss: 0.3305 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3230 - accuracy: 0.8902 - val_loss: 0.3289 - val_accuracy: 0.8882\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.88423 to 0.88820, saving model to model1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3189 - accuracy: 0.8956 - val_loss: 0.3287 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.88820 to 0.89354, saving model to model1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3226 - accuracy: 0.8974 - val_loss: 0.3281 - val_accuracy: 0.8935\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.89354\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3216 - accuracy: 0.8969 - val_loss: 0.3275 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.89354 to 0.89386, saving model to model1.hdf5\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3167 - accuracy: 0.8988 - val_loss: 0.3274 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.89386 to 0.89394, saving model to model1.hdf5\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3261 - accuracy: 0.8953 - val_loss: 0.3275 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.89394\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3188 - accuracy: 0.8986 - val_loss: 0.3271 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.89394\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3213 - accuracy: 0.8968 - val_loss: 0.3270 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.89394\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3217 - accuracy: 0.8965 - val_loss: 0.3278 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89394\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3262 - accuracy: 0.8955 - val_loss: 0.3270 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.89394 to 0.89403, saving model to model1.hdf5\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3182 - accuracy: 0.8985 - val_loss: 0.3273 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.89403\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3125 - accuracy: 0.9015 - val_loss: 0.3271 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.89403\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3239 - accuracy: 0.8952 - val_loss: 0.3267 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.89403\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3083 - accuracy: 0.9030 - val_loss: 0.3267 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89403\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3178 - accuracy: 0.9000 - val_loss: 0.3266 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89403\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3167 - accuracy: 0.8992 - val_loss: 0.3266 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89403\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3174 - accuracy: 0.8988 - val_loss: 0.3266 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89403\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3271 - accuracy: 0.8934 - val_loss: 0.3267 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89403\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3208 - accuracy: 0.8973 - val_loss: 0.3266 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89403\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3215 - accuracy: 0.8964 - val_loss: 0.3273 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89403\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3206 - accuracy: 0.8975 - val_loss: 0.3279 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89403\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3160 - accuracy: 0.8991 - val_loss: 0.3265 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.89403 to 0.89427, saving model to model1.hdf5\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3191 - accuracy: 0.8979 - val_loss: 0.3266 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.89427\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3137 - accuracy: 0.9003 - val_loss: 0.3265 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89427\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3162 - accuracy: 0.8989 - val_loss: 0.3272 - val_accuracy: 0.8940\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89427\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3183 - accuracy: 0.8981 - val_loss: 0.3267 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89427\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3216 - accuracy: 0.8965 - val_loss: 0.3267 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89427\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3231 - accuracy: 0.8958 - val_loss: 0.3265 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89427\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3197 - accuracy: 0.8977 - val_loss: 0.3265 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89427\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3174 - accuracy: 0.8990 - val_loss: 0.3267 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89427\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3183 - accuracy: 0.8984 - val_loss: 0.3265 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89427\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3170 - accuracy: 0.8981 - val_loss: 0.3266 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89427\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3168 - accuracy: 0.8992 - val_loss: 0.3270 - val_accuracy: 0.8948\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.89427 to 0.89483, saving model to model1.hdf5\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3197 - accuracy: 0.8972 - val_loss: 0.3275 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89483\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3210 - accuracy: 0.8967 - val_loss: 0.3265 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89483\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3200 - accuracy: 0.8970 - val_loss: 0.3266 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89483\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3270 - accuracy: 0.8948\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4737 - accuracy: 0.8809 - val_loss: 0.3413 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3268 - accuracy: 0.8922 - val_loss: 0.3392 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88423 to 0.88577, saving model to model1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3303 - accuracy: 0.8904 - val_loss: 0.3395 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88577\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3305 - accuracy: 0.8897 - val_loss: 0.3389 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88577\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3287 - accuracy: 0.8908 - val_loss: 0.3389 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88577\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3319 - accuracy: 0.8900 - val_loss: 0.3388 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88577\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3295 - accuracy: 0.8902 - val_loss: 0.3387 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88577\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3310 - accuracy: 0.8896 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88577\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3295 - accuracy: 0.8909 - val_loss: 0.3387 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88577\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3313 - accuracy: 0.8895 - val_loss: 0.3387 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88577\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3239 - accuracy: 0.8940 - val_loss: 0.3387 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88577\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3264 - accuracy: 0.8916 - val_loss: 0.3391 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88577\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3338 - accuracy: 0.8874 - val_loss: 0.3390 - val_accuracy: 0.8848\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88577\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3294 - accuracy: 0.8905 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88577\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3273 - accuracy: 0.8917 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88577\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3279 - accuracy: 0.8911 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88577\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3321 - accuracy: 0.8890 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88577\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3348 - accuracy: 0.8876 - val_loss: 0.3388 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88577\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3343 - accuracy: 0.8875 - val_loss: 0.3388 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88577\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3335 - accuracy: 0.8890 - val_loss: 0.3385 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88577\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3336 - accuracy: 0.8886 - val_loss: 0.3388 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88577\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3306 - accuracy: 0.8901 - val_loss: 0.3385 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88577\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3249 - accuracy: 0.8933 - val_loss: 0.3388 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88577\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3278 - accuracy: 0.8910 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88577\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3323 - accuracy: 0.8883 - val_loss: 0.3393 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88577\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3366 - accuracy: 0.8868 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88577\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3374 - accuracy: 0.8868 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88577\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3259 - accuracy: 0.8918 - val_loss: 0.3387 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88577\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3286 - accuracy: 0.8910 - val_loss: 0.3393 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88577\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3279 - accuracy: 0.8907 - val_loss: 0.3388 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88577\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3353 - accuracy: 0.8897 - val_loss: 0.3393 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88577\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3327 - accuracy: 0.8888 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88577\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3323 - accuracy: 0.8889 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88577\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3362 - accuracy: 0.8876 - val_loss: 0.3389 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88577\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3317 - accuracy: 0.8902 - val_loss: 0.3388 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88577\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3409 - accuracy: 0.8850 - val_loss: 0.3388 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88577\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3244 - accuracy: 0.8924 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88577\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3282 - accuracy: 0.8910 - val_loss: 0.3385 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88577\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3325 - accuracy: 0.8894 - val_loss: 0.3386 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88577\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3322 - accuracy: 0.8890 - val_loss: 0.3390 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88577\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3295 - accuracy: 0.8912 - val_loss: 0.3389 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88577\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3392 - accuracy: 0.8858\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3617 - accuracy: 0.8885 - val_loss: 0.3544 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3471 - accuracy: 0.8873 - val_loss: 0.3533 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8881 - val_loss: 0.3530 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3445 - accuracy: 0.8887 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3439 - accuracy: 0.8885 - val_loss: 0.3530 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3406 - accuracy: 0.8896 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3451 - accuracy: 0.8881 - val_loss: 0.3535 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3406 - accuracy: 0.8895 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3400 - accuracy: 0.8906 - val_loss: 0.3528 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3463 - accuracy: 0.8874 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3422 - accuracy: 0.8890 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3439 - accuracy: 0.8887 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3430 - accuracy: 0.8888 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3460 - accuracy: 0.8872 - val_loss: 0.3528 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3441 - accuracy: 0.8881 - val_loss: 0.3526 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3365 - accuracy: 0.8915 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3373 - accuracy: 0.8911 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3440 - accuracy: 0.8883 - val_loss: 0.3528 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3453 - accuracy: 0.8874 - val_loss: 0.3537 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3463 - accuracy: 0.8873 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3419 - accuracy: 0.8892 - val_loss: 0.3526 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3487 - accuracy: 0.8858 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8862 - val_loss: 0.3531 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3381 - accuracy: 0.8907 - val_loss: 0.3526 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8878 - val_loss: 0.3537 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3431 - accuracy: 0.8886 - val_loss: 0.3527 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8877 - val_loss: 0.3525 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3413 - accuracy: 0.8901 - val_loss: 0.3520 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3381 - accuracy: 0.8908 - val_loss: 0.3515 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3484 - accuracy: 0.8859 - val_loss: 0.3507 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3397 - accuracy: 0.8899 - val_loss: 0.3496 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3356 - accuracy: 0.8908 - val_loss: 0.3486 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3391 - accuracy: 0.8888 - val_loss: 0.3469 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3353 - accuracy: 0.8896 - val_loss: 0.3452 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3323 - accuracy: 0.8896 - val_loss: 0.3430 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3369 - accuracy: 0.8864 - val_loss: 0.3401 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3279 - accuracy: 0.8897 - val_loss: 0.3368 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3299 - accuracy: 0.8874 - val_loss: 0.3347 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3246 - accuracy: 0.8888 - val_loss: 0.3327 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3230 - accuracy: 0.8890 - val_loss: 0.3308 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3141 - accuracy: 0.8925 - val_loss: 0.3310 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.88423 to 0.89467, saving model to model1.hdf5\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3310 - accuracy: 0.8947\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3969 - accuracy: 0.8897 - val_loss: 0.3186 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3066 - accuracy: 0.8907 - val_loss: 0.3162 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3144 - accuracy: 0.8854 - val_loss: 0.3163 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3046 - accuracy: 0.8906 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3075 - accuracy: 0.8892 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3072 - accuracy: 0.8885 - val_loss: 0.3157 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3092 - accuracy: 0.8886 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3048 - accuracy: 0.8911 - val_loss: 0.3156 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3098 - accuracy: 0.8892 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3061 - accuracy: 0.8899 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3118 - accuracy: 0.8870 - val_loss: 0.3159 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3129 - accuracy: 0.8861 - val_loss: 0.3157 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3085 - accuracy: 0.8876 - val_loss: 0.3158 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3074 - accuracy: 0.8904 - val_loss: 0.3160 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3116 - accuracy: 0.8877 - val_loss: 0.3156 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3119 - accuracy: 0.8875 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3058 - accuracy: 0.8912 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3157 - accuracy: 0.8862 - val_loss: 0.3161 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3114 - accuracy: 0.8869 - val_loss: 0.3160 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3060 - accuracy: 0.8895 - val_loss: 0.3156 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3116 - accuracy: 0.8880 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3073 - accuracy: 0.8910 - val_loss: 0.3156 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3116 - accuracy: 0.8873 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3097 - accuracy: 0.8869 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3111 - accuracy: 0.8877 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3139 - accuracy: 0.8869 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3084 - accuracy: 0.8882 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3030 - accuracy: 0.8910 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3067 - accuracy: 0.8895 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3068 - accuracy: 0.8885 - val_loss: 0.3156 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3095 - accuracy: 0.8887 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3179 - accuracy: 0.8839 - val_loss: 0.3160 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3072 - accuracy: 0.8895 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3118 - accuracy: 0.8866 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3074 - accuracy: 0.8906 - val_loss: 0.3156 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3106 - accuracy: 0.8875 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3151 - accuracy: 0.8861 - val_loss: 0.3160 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3058 - accuracy: 0.8906 - val_loss: 0.3162 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3041 - accuracy: 0.8920 - val_loss: 0.3163 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3028 - accuracy: 0.8905 - val_loss: 0.3158 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3016 - accuracy: 0.8919 - val_loss: 0.3155 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3186 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.6171 - accuracy: 0.8898 - val_loss: 0.4620 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4311 - accuracy: 0.8900 - val_loss: 0.3848 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3701 - accuracy: 0.8890 - val_loss: 0.3632 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3556 - accuracy: 0.8875 - val_loss: 0.3588 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3449 - accuracy: 0.8912 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3490 - accuracy: 0.8888 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3460 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3489 - accuracy: 0.8889 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3563 - accuracy: 0.8853 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3510 - accuracy: 0.8879 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3539 - accuracy: 0.8864 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8915 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3394 - accuracy: 0.8934 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3520 - accuracy: 0.8874 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3449 - accuracy: 0.8908 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3512 - accuracy: 0.8878 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3440 - accuracy: 0.8912 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3557 - accuracy: 0.8856 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3511 - accuracy: 0.8878 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3468 - accuracy: 0.8899 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3552 - accuracy: 0.8858 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3464 - accuracy: 0.8900 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3474 - accuracy: 0.8896 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3477 - accuracy: 0.8894 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3479 - accuracy: 0.8893 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3505 - accuracy: 0.8881 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3437 - accuracy: 0.8914 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3459 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3515 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3474 - accuracy: 0.8896 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3438 - accuracy: 0.8913 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3519 - accuracy: 0.8874 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3469 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3465 - accuracy: 0.8900 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3440 - accuracy: 0.8912 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3551 - accuracy: 0.8859 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8915 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3528 - accuracy: 0.8870 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.4620 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.6301 - accuracy: 0.7546 - val_loss: 0.4628 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4357 - accuracy: 0.8853 - val_loss: 0.3833 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3719 - accuracy: 0.8866 - val_loss: 0.3581 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3450 - accuracy: 0.8902 - val_loss: 0.3468 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3418 - accuracy: 0.8861 - val_loss: 0.3431 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3386 - accuracy: 0.8868 - val_loss: 0.3428 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3336 - accuracy: 0.8883 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3410 - accuracy: 0.8833 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3334 - accuracy: 0.8874 - val_loss: 0.3428 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3356 - accuracy: 0.8859 - val_loss: 0.3432 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3282 - accuracy: 0.8899 - val_loss: 0.3428 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3399 - accuracy: 0.8840 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3374 - accuracy: 0.8849 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3318 - accuracy: 0.8873 - val_loss: 0.3436 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3373 - accuracy: 0.8845 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3298 - accuracy: 0.8885 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3344 - accuracy: 0.8865 - val_loss: 0.3424 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3366 - accuracy: 0.8852 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3344 - accuracy: 0.8865 - val_loss: 0.3426 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3311 - accuracy: 0.8880 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3375 - accuracy: 0.8851 - val_loss: 0.3431 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3357 - accuracy: 0.8856 - val_loss: 0.3426 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3398 - accuracy: 0.8846 - val_loss: 0.3424 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3292 - accuracy: 0.8888 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3353 - accuracy: 0.8873 - val_loss: 0.3428 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3391 - accuracy: 0.8844 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3329 - accuracy: 0.8885 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3418 - accuracy: 0.8833 - val_loss: 0.3424 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3258 - accuracy: 0.8915 - val_loss: 0.3427 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3338 - accuracy: 0.8869 - val_loss: 0.3428 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3316 - accuracy: 0.8885 - val_loss: 0.3424 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3319 - accuracy: 0.8883 - val_loss: 0.3424 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3322 - accuracy: 0.8882 - val_loss: 0.3423 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3372 - accuracy: 0.8853 - val_loss: 0.3433 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3311 - accuracy: 0.8877 - val_loss: 0.3428 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3253 - accuracy: 0.8902 - val_loss: 0.3425 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3374 - accuracy: 0.8847 - val_loss: 0.3428 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3348 - accuracy: 0.8860 - val_loss: 0.3430 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3301 - accuracy: 0.8888 - val_loss: 0.3449 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3342 - accuracy: 0.8880 - val_loss: 0.3431 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3345 - accuracy: 0.8865 - val_loss: 0.3424 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3373 - accuracy: 0.8863 - val_loss: 0.3427 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88423\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3389 - accuracy: 0.8849 - val_loss: 0.3424 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88423\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3362 - accuracy: 0.8864 - val_loss: 0.3425 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88423\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3376 - accuracy: 0.8845 - val_loss: 0.3435 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88423\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3319 - accuracy: 0.8877 - val_loss: 0.3426 - val_accuracy: 0.8820\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88423\n",
            "Epoch 00046: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.4628 - accuracy: 0.8842\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3992 - accuracy: 0.8864 - val_loss: 0.3130 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3038 - accuracy: 0.8888 - val_loss: 0.3125 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3036 - accuracy: 0.8893 - val_loss: 0.3121 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3049 - accuracy: 0.8876 - val_loss: 0.3130 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3049 - accuracy: 0.8886 - val_loss: 0.3110 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3039 - accuracy: 0.8896 - val_loss: 0.3126 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3062 - accuracy: 0.8854 - val_loss: 0.3099 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2997 - accuracy: 0.8888 - val_loss: 0.3094 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3022 - accuracy: 0.8886 - val_loss: 0.3094 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3105 - accuracy: 0.8845 - val_loss: 0.3087 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2952 - accuracy: 0.8918 - val_loss: 0.3081 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3006 - accuracy: 0.8876 - val_loss: 0.3084 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3028 - accuracy: 0.8867 - val_loss: 0.3056 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2900 - accuracy: 0.8916 - val_loss: 0.3053 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2932 - accuracy: 0.8921 - val_loss: 0.3045 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2998 - accuracy: 0.8879 - val_loss: 0.3040 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3005 - accuracy: 0.8856 - val_loss: 0.3038 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2934 - accuracy: 0.8897 - val_loss: 0.3031 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3018 - accuracy: 0.8879 - val_loss: 0.3039 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2989 - accuracy: 0.8872 - val_loss: 0.3025 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2978 - accuracy: 0.8885 - val_loss: 0.3031 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2951 - accuracy: 0.8898 - val_loss: 0.3020 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2894 - accuracy: 0.8914 - val_loss: 0.3022 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2971 - accuracy: 0.8894 - val_loss: 0.3022 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.88423 to 0.88536, saving model to model1.hdf5\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2944 - accuracy: 0.8897 - val_loss: 0.3019 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88536\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2954 - accuracy: 0.8890 - val_loss: 0.3019 - val_accuracy: 0.8846\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88536\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3040 - accuracy: 0.8855 - val_loss: 0.3015 - val_accuracy: 0.8846\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88536\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2990 - accuracy: 0.8897 - val_loss: 0.3012 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88536\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2997 - accuracy: 0.8885 - val_loss: 0.3025 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.88536 to 0.88544, saving model to model1.hdf5\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2974 - accuracy: 0.8903 - val_loss: 0.3012 - val_accuracy: 0.8853\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88544\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2905 - accuracy: 0.8919 - val_loss: 0.3016 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.88544 to 0.88561, saving model to model1.hdf5\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2990 - accuracy: 0.8902 - val_loss: 0.3014 - val_accuracy: 0.8846\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88561\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2987 - accuracy: 0.8883 - val_loss: 0.3010 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88561\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2968 - accuracy: 0.8901 - val_loss: 0.3009 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88561\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2946 - accuracy: 0.8914 - val_loss: 0.3020 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88561\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2924 - accuracy: 0.8921 - val_loss: 0.3011 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88561\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3002 - accuracy: 0.8876 - val_loss: 0.3016 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88561\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2986 - accuracy: 0.8894 - val_loss: 0.3018 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88561\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2911 - accuracy: 0.8908 - val_loss: 0.3009 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88561\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2933 - accuracy: 0.8934 - val_loss: 0.3009 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88561\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2968 - accuracy: 0.8927 - val_loss: 0.3012 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88561\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2859 - accuracy: 0.8939 - val_loss: 0.3017 - val_accuracy: 0.8848\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88561\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2980 - accuracy: 0.8882 - val_loss: 0.3016 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88561\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2943 - accuracy: 0.8893 - val_loss: 0.3008 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88561\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.8944 - val_loss: 0.3014 - val_accuracy: 0.8817\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88561\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2987 - accuracy: 0.8904 - val_loss: 0.3018 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88561\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2944 - accuracy: 0.8913 - val_loss: 0.3012 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88561\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2941 - accuracy: 0.8909 - val_loss: 0.3008 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88561\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2934 - accuracy: 0.8922 - val_loss: 0.3008 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88561\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2921 - accuracy: 0.8929 - val_loss: 0.3010 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.88561\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2983 - accuracy: 0.8915 - val_loss: 0.3013 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88561\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2903 - accuracy: 0.8925 - val_loss: 0.3009 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88561\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2906 - accuracy: 0.8934 - val_loss: 0.3011 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88561\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2919 - accuracy: 0.8922 - val_loss: 0.3014 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88561\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3006 - accuracy: 0.8870 - val_loss: 0.3008 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88561\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2938 - accuracy: 0.8910 - val_loss: 0.3012 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.88561\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2956 - accuracy: 0.8892 - val_loss: 0.3020 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.88561\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2951 - accuracy: 0.8912 - val_loss: 0.3010 - val_accuracy: 0.8817\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.88561\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2940 - accuracy: 0.8899 - val_loss: 0.3008 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.88561\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2957 - accuracy: 0.8895 - val_loss: 0.3011 - val_accuracy: 0.8847\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.88561\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2919 - accuracy: 0.8924 - val_loss: 0.3008 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.88561\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2969 - accuracy: 0.8899 - val_loss: 0.3007 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.88561\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2989 - accuracy: 0.8894 - val_loss: 0.3008 - val_accuracy: 0.8846\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.88561\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2942 - accuracy: 0.8914 - val_loss: 0.3008 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.88561\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3016 - accuracy: 0.8856\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.6174 - accuracy: 0.8884 - val_loss: 0.4611 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4312 - accuracy: 0.8886 - val_loss: 0.3846 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3725 - accuracy: 0.8874 - val_loss: 0.3630 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3539 - accuracy: 0.8884 - val_loss: 0.3588 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3457 - accuracy: 0.8907 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3457 - accuracy: 0.8904 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3493 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3579 - accuracy: 0.8845 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3443 - accuracy: 0.8910 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8883 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3517 - accuracy: 0.8875 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3523 - accuracy: 0.8872 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3447 - accuracy: 0.8909 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3459 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3486 - accuracy: 0.8890 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3422 - accuracy: 0.8921 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3513 - accuracy: 0.8877 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3453 - accuracy: 0.8906 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3445 - accuracy: 0.8910 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3456 - accuracy: 0.8905 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3471 - accuracy: 0.8897 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3497 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3415 - accuracy: 0.8924 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3453 - accuracy: 0.8906 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3523 - accuracy: 0.8872 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3500 - accuracy: 0.8883 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3483 - accuracy: 0.8891 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3524 - accuracy: 0.8872 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3504 - accuracy: 0.8882 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3526 - accuracy: 0.8871 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8885 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3532 - accuracy: 0.8868 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8905 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3539 - accuracy: 0.8864 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3454 - accuracy: 0.8906 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3544 - accuracy: 0.8862 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3486 - accuracy: 0.8890 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3536 - accuracy: 0.8866 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 00041: early stopping\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.4611 - accuracy: 0.8842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "GXJmAjSywzBc",
        "outputId": "de024ac7-d094-40e2-d884-12dc0981cfd1"
      },
      "source": [
        "column_names = data.columns\n",
        "plt.bar(column_names[:20], acc)\n",
        "plt.ylim(0.85,0.92)\n",
        "plt.xlabel('Features', fontsize=10)\n",
        "plt.ylabel('Accuracy', fontsize=10)\n",
        "plt.xticks(column_names[:20], fontsize=5, rotation=90)\n",
        "plt.title('Significance of each feature')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1MG-NEFp0O"
      },
      "source": [
        "**Removing One feature at a time**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1MGFn7Nvwm3",
        "outputId": "7aa737c4-7de9-4005-ed17-214f279d153d"
      },
      "source": [
        "accuracy_r = [] \n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "for i in range(20):\n",
        "  templist = np.delete(dataset,i,1)\n",
        "  X = templist[:,:-1]\n",
        "  Y = dataset[:,-1]\n",
        "  XVALID = X[:index]\n",
        "  YVALID = Y[:index]\n",
        "  XTRAIN = X[index:]\n",
        "  YTRAIN = Y[index:]\n",
        "  model_r = Sequential()\n",
        "  model_r.add(Dense(8, input_dim = 19, activation='relu'))\n",
        "  model_r.add(Dense(4,activation='relu'))\n",
        "  model_r.add(Dense(1,activation='sigmoid'))\n",
        "  model_r.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  callback_a = ModelCheckpoint(filepath = 'model_r.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "  # The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "  callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=64, verbose=1)\n",
        "  history_r = model_r.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64, batch_size = 30, callbacks = [callback_a,callback_b])\n",
        "  model_r.load_weights('model_r.hdf5')\n",
        "  p_r = model_r.predict(XVALID)\n",
        "  acc_r = model_r.evaluate(XVALID, YVALID)\n",
        "  accuracy_r.append(acc_r[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9148 - val_loss: 0.2028 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91176\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9140 - val_loss: 0.1924 - val_accuracy: 0.9116\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91176\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9147 - val_loss: 0.1926 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91176\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9124 - val_loss: 0.1926 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91176\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.1836 - accuracy: 0.9195 - val_loss: 0.1914 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91176\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9117 - val_loss: 0.1916 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91176\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9137 - val_loss: 0.1920 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91176\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9142 - val_loss: 0.1944 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91176\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1828 - accuracy: 0.9170 - val_loss: 0.1905 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91176\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9172 - val_loss: 0.1899 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91176\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9147 - val_loss: 0.1890 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91176\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9144 - val_loss: 0.1904 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91176\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9135 - val_loss: 0.1898 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91176\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9144 - val_loss: 0.1903 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91176\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9168 - val_loss: 0.1884 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91176\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9152 - val_loss: 0.1881 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91176\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9144 - val_loss: 0.1877 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91176\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9152 - val_loss: 0.1887 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91176\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1802 - accuracy: 0.9180 - val_loss: 0.1876 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91176\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1789 - accuracy: 0.9180 - val_loss: 0.1915 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91176\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9176 - val_loss: 0.1877 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91176\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9147 - val_loss: 0.1881 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91176\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1788 - accuracy: 0.9186 - val_loss: 0.1904 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91176\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9132 - val_loss: 0.1867 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91176\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1801 - accuracy: 0.9175 - val_loss: 0.1878 - val_accuracy: 0.9120\n",
            "\n",
            "Epoch 00060: val_accuracy improved from 0.91176 to 0.91200, saving model to model_r.hdf5\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1783 - accuracy: 0.9166 - val_loss: 0.1868 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91200\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9145 - val_loss: 0.1862 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91200\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1812 - accuracy: 0.9155 - val_loss: 0.1873 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91200\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9132 - val_loss: 0.1882 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91200\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9120\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4749 - accuracy: 0.8735 - val_loss: 0.2869 - val_accuracy: 0.8846\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88455, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2722 - accuracy: 0.8936 - val_loss: 0.2490 - val_accuracy: 0.9013\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88455 to 0.90131, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2349 - accuracy: 0.9058 - val_loss: 0.2269 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90131 to 0.90544, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2182 - accuracy: 0.9071 - val_loss: 0.2207 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90544 to 0.90633, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2101 - accuracy: 0.9126 - val_loss: 0.2159 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90633 to 0.90787, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2077 - accuracy: 0.9131 - val_loss: 0.2146 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90787\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2078 - accuracy: 0.9117 - val_loss: 0.2130 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90787\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2066 - accuracy: 0.9122 - val_loss: 0.2138 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90787\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2050 - accuracy: 0.9110 - val_loss: 0.2108 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90787\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2043 - accuracy: 0.9122 - val_loss: 0.2103 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90787\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9136 - val_loss: 0.2104 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90787\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2053 - accuracy: 0.9103 - val_loss: 0.2084 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90787\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9147 - val_loss: 0.2054 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90787\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1997 - accuracy: 0.9123 - val_loss: 0.2042 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90787\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9112 - val_loss: 0.2030 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90787\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9128 - val_loss: 0.2041 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90787\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9113 - val_loss: 0.2010 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90787\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9117 - val_loss: 0.2008 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90787\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9129 - val_loss: 0.2000 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.90787 to 0.90795, saving model to model_r.hdf5\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9138 - val_loss: 0.1992 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90795\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9160 - val_loss: 0.1988 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.90795 to 0.90868, saving model to model_r.hdf5\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9154 - val_loss: 0.1981 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90868\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9164 - val_loss: 0.1985 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.90868 to 0.90933, saving model to model_r.hdf5\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9165 - val_loss: 0.2006 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90933\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9147 - val_loss: 0.1981 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90933\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9175 - val_loss: 0.1976 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.90933 to 0.90989, saving model to model_r.hdf5\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9142 - val_loss: 0.1966 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90989\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9140 - val_loss: 0.1967 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90989\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9141 - val_loss: 0.1962 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90989\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9155 - val_loss: 0.1975 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.90989 to 0.91038, saving model to model_r.hdf5\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9141 - val_loss: 0.1966 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91038\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9149 - val_loss: 0.1964 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91038\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9142 - val_loss: 0.1955 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91038\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9164 - val_loss: 0.1951 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91038\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9149 - val_loss: 0.1956 - val_accuracy: 0.9112\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.91038 to 0.91119, saving model to model_r.hdf5\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9191 - val_loss: 0.1967 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91119\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9124 - val_loss: 0.1953 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.91119 to 0.91143, saving model to model_r.hdf5\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9173 - val_loss: 0.2000 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91143\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9152 - val_loss: 0.1944 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91143\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9103 - val_loss: 0.1951 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91143\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9167 - val_loss: 0.1947 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91143\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9163 - val_loss: 0.1951 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91143\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9149 - val_loss: 0.1943 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91143\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9164 - val_loss: 0.1955 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91143\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9124 - val_loss: 0.1940 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91143\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9155 - val_loss: 0.1956 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91143\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9157 - val_loss: 0.1938 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91143\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9152 - val_loss: 0.1939 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91143\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9119 - val_loss: 0.1938 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91143\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9143 - val_loss: 0.1931 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91143\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9163 - val_loss: 0.1947 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91143\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9173 - val_loss: 0.1930 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91143\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9156 - val_loss: 0.1929 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91143\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9149 - val_loss: 0.1943 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91143\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9141 - val_loss: 0.1942 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91143\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9131 - val_loss: 0.1935 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91143\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9145 - val_loss: 0.1925 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91143\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9149 - val_loss: 0.1927 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91143\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9157 - val_loss: 0.1932 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91143\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9152 - val_loss: 0.1945 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91143\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9143 - val_loss: 0.1958 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91143\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9148 - val_loss: 0.1938 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91143\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9155 - val_loss: 0.1921 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91143\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9173 - val_loss: 0.1942 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91143\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1953 - accuracy: 0.9114\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3376 - accuracy: 0.8888 - val_loss: 0.2898 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2791 - accuracy: 0.8882 - val_loss: 0.2558 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88423 to 0.89637, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2435 - accuracy: 0.8980 - val_loss: 0.2224 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89637 to 0.90593, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2079 - accuracy: 0.9120 - val_loss: 0.2112 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90593\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9145 - val_loss: 0.2123 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90593 to 0.90601, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9123 - val_loss: 0.2090 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90601 to 0.90674, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1978 - accuracy: 0.9135 - val_loss: 0.2059 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90674 to 0.90698, saving model to model_r.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9127 - val_loss: 0.2037 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90698\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9125 - val_loss: 0.2039 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.90698 to 0.90730, saving model to model_r.hdf5\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9119 - val_loss: 0.2026 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90730\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9158 - val_loss: 0.2022 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90730\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9123 - val_loss: 0.2026 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90730\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1959 - accuracy: 0.9127 - val_loss: 0.2014 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.90730 to 0.90803, saving model to model_r.hdf5\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9151 - val_loss: 0.2011 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90803 to 0.90876, saving model to model_r.hdf5\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9142 - val_loss: 0.2002 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90876\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9125 - val_loss: 0.1997 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.90876 to 0.90884, saving model to model_r.hdf5\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9119 - val_loss: 0.1994 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90884\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9168 - val_loss: 0.1989 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90884\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9149 - val_loss: 0.2023 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90884\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9163 - val_loss: 0.2072 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90884\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9178 - val_loss: 0.2061 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90884\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9175 - val_loss: 0.1988 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90884\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9182 - val_loss: 0.1982 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.90884 to 0.91022, saving model to model_r.hdf5\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9126 - val_loss: 0.2017 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91022\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9164 - val_loss: 0.1975 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91022\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9162 - val_loss: 0.1978 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91022\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9135 - val_loss: 0.1981 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91022\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9136 - val_loss: 0.2020 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91022\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9128 - val_loss: 0.1969 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91022\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9148 - val_loss: 0.1963 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91022\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9138 - val_loss: 0.1973 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91022\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9131 - val_loss: 0.2025 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91022\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9148 - val_loss: 0.1953 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91022\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9168 - val_loss: 0.1973 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91022\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9156 - val_loss: 0.1940 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91022\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9137 - val_loss: 0.1937 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91022\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9144 - val_loss: 0.1939 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91022\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9142 - val_loss: 0.1937 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91022\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9139 - val_loss: 0.1934 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91022\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9151 - val_loss: 0.1928 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91022\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9103 - val_loss: 0.1933 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91022\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9144 - val_loss: 0.1924 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91022\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9160 - val_loss: 0.1939 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91022\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9131 - val_loss: 0.1917 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91022\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9146 - val_loss: 0.1918 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91022\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9152 - val_loss: 0.1949 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91022\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9148 - val_loss: 0.1943 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91022\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9172 - val_loss: 0.1913 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91022\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9146 - val_loss: 0.1918 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91022\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9161 - val_loss: 0.1913 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91022\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9164 - val_loss: 0.1912 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91022\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9150 - val_loss: 0.1943 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91022\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9165 - val_loss: 0.1919 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91022\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9150 - val_loss: 0.1911 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91022\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9140 - val_loss: 0.1950 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91022\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9174 - val_loss: 0.1920 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91022\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9145 - val_loss: 0.1916 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91022\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9125 - val_loss: 0.1924 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91022\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9131 - val_loss: 0.1913 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91022\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9149 - val_loss: 0.1909 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91022\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9135 - val_loss: 0.1916 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91022\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9136 - val_loss: 0.1913 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91022\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9161 - val_loss: 0.1906 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91022\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9123 - val_loss: 0.1937 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91022\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9102\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3248 - accuracy: 0.8893 - val_loss: 0.2890 - val_accuracy: 0.8889\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88892, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2760 - accuracy: 0.8963 - val_loss: 0.2594 - val_accuracy: 0.9006\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88892 to 0.90058, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2472 - accuracy: 0.9026 - val_loss: 0.2313 - val_accuracy: 0.9034\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90058 to 0.90342, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2198 - accuracy: 0.9093 - val_loss: 0.2181 - val_accuracy: 0.9056\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90342 to 0.90560, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2112 - accuracy: 0.9105 - val_loss: 0.2146 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90560 to 0.90690, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2057 - accuracy: 0.9119 - val_loss: 0.2100 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90690 to 0.90706, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2046 - accuracy: 0.9114 - val_loss: 0.2080 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90706\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2058 - accuracy: 0.9137 - val_loss: 0.2064 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90706 to 0.90787, saving model to model_r.hdf5\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9120 - val_loss: 0.2054 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90787\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9096 - val_loss: 0.2059 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90787 to 0.90835, saving model to model_r.hdf5\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1970 - accuracy: 0.9125 - val_loss: 0.2037 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90835\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9116 - val_loss: 0.2054 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90835\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9138 - val_loss: 0.2035 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90835\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9138 - val_loss: 0.2031 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90835\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9128 - val_loss: 0.2049 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90835\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9142 - val_loss: 0.2018 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90835\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9138 - val_loss: 0.2007 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90835\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9153 - val_loss: 0.2007 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90835\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2009 - accuracy: 0.9124 - val_loss: 0.2002 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.90835 to 0.90933, saving model to model_r.hdf5\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9146 - val_loss: 0.2057 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90933\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9130 - val_loss: 0.2006 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90933\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9114 - val_loss: 0.2024 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90933\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9116 - val_loss: 0.2022 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90933\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9116 - val_loss: 0.1997 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90933\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9137 - val_loss: 0.1986 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90933\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9135 - val_loss: 0.1987 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90933\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1978 - accuracy: 0.9108 - val_loss: 0.2003 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90933\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9147 - val_loss: 0.2011 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90933\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9141 - val_loss: 0.1984 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90933\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9111 - val_loss: 0.1993 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90933\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9127 - val_loss: 0.1979 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.90933 to 0.91062, saving model to model_r.hdf5\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9153 - val_loss: 0.2004 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91062\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9169 - val_loss: 0.1987 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91062\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9143 - val_loss: 0.1973 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91062\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9127 - val_loss: 0.1999 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91062\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9153 - val_loss: 0.1979 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91062\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9117 - val_loss: 0.1981 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91062\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9156 - val_loss: 0.1975 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91062\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9137 - val_loss: 0.1969 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91062\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9157 - val_loss: 0.1991 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91062\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9124 - val_loss: 0.1977 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91062\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9166 - val_loss: 0.1979 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91062\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9160 - val_loss: 0.1971 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91062\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9151 - val_loss: 0.1966 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91062\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9158 - val_loss: 0.1967 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91062\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1946 - accuracy: 0.9127 - val_loss: 0.1981 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91062\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9130 - val_loss: 0.1959 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91062\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9168 - val_loss: 0.1962 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91062\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9141 - val_loss: 0.1964 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91062\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9133 - val_loss: 0.1960 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91062\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9147 - val_loss: 0.1994 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91062\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9171 - val_loss: 0.1954 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91062\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9147 - val_loss: 0.2007 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91062\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9165 - val_loss: 0.1965 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91062\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9165 - val_loss: 0.1958 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91062\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9152 - val_loss: 0.1954 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91062\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9167 - val_loss: 0.1944 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91062\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9150 - val_loss: 0.1946 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91062\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9165 - val_loss: 0.1957 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91062\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9150 - val_loss: 0.1940 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91062\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9130 - val_loss: 0.1947 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91062\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9153 - val_loss: 0.1944 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91062\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9145 - val_loss: 0.1945 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91062\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9183 - val_loss: 0.1934 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91062\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1979 - accuracy: 0.9106\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4591 - accuracy: 0.8864 - val_loss: 0.2984 - val_accuracy: 0.8846\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88463, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2765 - accuracy: 0.8953 - val_loss: 0.2596 - val_accuracy: 0.9025\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88463 to 0.90253, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2438 - accuracy: 0.9061 - val_loss: 0.2325 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90253 to 0.90585, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2240 - accuracy: 0.9090 - val_loss: 0.2217 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90585 to 0.90682, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2156 - accuracy: 0.9116 - val_loss: 0.2201 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90682 to 0.90892, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2138 - accuracy: 0.9107 - val_loss: 0.2163 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90892\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2094 - accuracy: 0.9109 - val_loss: 0.2147 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90892\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2055 - accuracy: 0.9134 - val_loss: 0.2124 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90892\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2040 - accuracy: 0.9129 - val_loss: 0.2149 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90892\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2008 - accuracy: 0.9134 - val_loss: 0.2100 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90892\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2039 - accuracy: 0.9135 - val_loss: 0.2093 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.90892 to 0.91062, saving model to model_r.hdf5\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2019 - accuracy: 0.9136 - val_loss: 0.2073 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.91062\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9146 - val_loss: 0.2077 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91062\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9158 - val_loss: 0.2051 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.91062\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9121 - val_loss: 0.2049 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.91062\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9145 - val_loss: 0.2035 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91062\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9151 - val_loss: 0.2037 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.91062\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9118 - val_loss: 0.2018 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91062\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1968 - accuracy: 0.9131 - val_loss: 0.2014 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91062\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9131 - val_loss: 0.2017 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91062\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9146 - val_loss: 0.2010 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91062\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9138 - val_loss: 0.1990 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91062\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9160 - val_loss: 0.1992 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91062\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9131 - val_loss: 0.2006 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91062\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9121 - val_loss: 0.1976 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91062\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9149 - val_loss: 0.1972 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91062\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9137 - val_loss: 0.1977 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91062\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9151 - val_loss: 0.1986 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91062\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9150 - val_loss: 0.1980 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91062\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9149 - val_loss: 0.1966 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91062\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9129 - val_loss: 0.1976 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91062\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9155 - val_loss: 0.1954 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91062\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9137 - val_loss: 0.1983 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91062\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9177 - val_loss: 0.1946 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91062\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9135 - val_loss: 0.1949 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91062\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9117 - val_loss: 0.1942 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91062\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9143 - val_loss: 0.1939 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91062\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9106 - val_loss: 0.1946 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91062\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9148 - val_loss: 0.1935 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91062\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9142 - val_loss: 0.1943 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91062\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9141 - val_loss: 0.1935 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91062\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9127 - val_loss: 0.1933 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91062\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9116 - val_loss: 0.1921 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91062\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9140 - val_loss: 0.1920 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91062\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9105 - val_loss: 0.1929 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91062\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9151 - val_loss: 0.1925 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91062\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9127 - val_loss: 0.1903 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91062\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9107 - val_loss: 0.1910 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91062\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9134 - val_loss: 0.1921 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91062\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9146 - val_loss: 0.1913 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91062\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9138 - val_loss: 0.1900 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91062\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9143 - val_loss: 0.1900 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91062\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9138 - val_loss: 0.1895 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91062\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9111 - val_loss: 0.1897 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91062\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9173 - val_loss: 0.1915 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91062\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9154 - val_loss: 0.1894 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91062\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9148 - val_loss: 0.1902 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91062\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9162 - val_loss: 0.1887 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.91062 to 0.91127, saving model to model_r.hdf5\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9136 - val_loss: 0.1885 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91127\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9135 - val_loss: 0.1878 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91127\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9153 - val_loss: 0.1883 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91127\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9146 - val_loss: 0.1897 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91127\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9159 - val_loss: 0.1872 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91127\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1782 - accuracy: 0.9161 - val_loss: 0.1871 - val_accuracy: 0.9115\n",
            "\n",
            "Epoch 00064: val_accuracy improved from 0.91127 to 0.91151, saving model to model_r.hdf5\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1871 - accuracy: 0.9115\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4129 - accuracy: 0.8096 - val_loss: 0.2767 - val_accuracy: 0.8894\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88941, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2577 - accuracy: 0.8979 - val_loss: 0.2363 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88941 to 0.90495, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2236 - accuracy: 0.9097 - val_loss: 0.2156 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90495 to 0.90779, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2062 - accuracy: 0.9135 - val_loss: 0.2092 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90779\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9150 - val_loss: 0.2068 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90779\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9169 - val_loss: 0.2055 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90779 to 0.90803, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9149 - val_loss: 0.2044 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90803\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9149 - val_loss: 0.2060 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90803\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9139 - val_loss: 0.2051 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90803\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9146 - val_loss: 0.2035 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90803\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9093 - val_loss: 0.2017 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90803\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9140 - val_loss: 0.2020 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90803\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9134 - val_loss: 0.2048 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90803\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9111 - val_loss: 0.2000 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90803\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9134 - val_loss: 0.2008 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90803\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9124 - val_loss: 0.1991 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90803\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9161 - val_loss: 0.1987 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90803\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9114 - val_loss: 0.1975 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90803\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1946 - accuracy: 0.9123 - val_loss: 0.1969 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90803\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9138 - val_loss: 0.1972 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90803\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9117 - val_loss: 0.1953 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90803\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9148 - val_loss: 0.1948 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90803\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9128 - val_loss: 0.1938 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90803\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9140 - val_loss: 0.1926 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90803\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9184 - val_loss: 0.1930 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.90803 to 0.90827, saving model to model_r.hdf5\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9132 - val_loss: 0.1910 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.90827 to 0.90941, saving model to model_r.hdf5\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9139 - val_loss: 0.1922 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90941\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9132 - val_loss: 0.1963 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90941\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9166 - val_loss: 0.1893 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90941\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9134 - val_loss: 0.1925 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90941\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9173 - val_loss: 0.1909 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90941\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9113 - val_loss: 0.1929 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90941\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9126 - val_loss: 0.1888 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90941\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9149 - val_loss: 0.1899 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90941\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9128 - val_loss: 0.1871 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90941\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9122 - val_loss: 0.1871 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90941\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9124 - val_loss: 0.1871 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90941\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9103 - val_loss: 0.1860 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90941\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9144 - val_loss: 0.1870 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90941\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1797 - accuracy: 0.9156 - val_loss: 0.1871 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90941\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1813 - accuracy: 0.9145 - val_loss: 0.1862 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90941\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9121 - val_loss: 0.1869 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90941\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9107 - val_loss: 0.1858 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.90941 to 0.90949, saving model to model_r.hdf5\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9146 - val_loss: 0.1858 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00044: val_accuracy improved from 0.90949 to 0.91014, saving model to model_r.hdf5\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9126 - val_loss: 0.1898 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91014\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9134 - val_loss: 0.1879 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91014\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9135 - val_loss: 0.1853 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91014\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9112 - val_loss: 0.1850 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91014\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9157 - val_loss: 0.1851 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91014\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9158 - val_loss: 0.1849 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91014\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9169 - val_loss: 0.1866 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91014\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9136 - val_loss: 0.1850 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91014\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9136 - val_loss: 0.1877 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91014\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1809 - accuracy: 0.9149 - val_loss: 0.1872 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00054: val_accuracy improved from 0.91014 to 0.91030, saving model to model_r.hdf5\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9152 - val_loss: 0.1858 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91030\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9145 - val_loss: 0.1846 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91030\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9142 - val_loss: 0.1852 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00057: val_accuracy improved from 0.91030 to 0.91046, saving model to model_r.hdf5\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1791 - accuracy: 0.9147 - val_loss: 0.1843 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91046\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9151 - val_loss: 0.1838 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91046\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1787 - accuracy: 0.9176 - val_loss: 0.1877 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00060: val_accuracy improved from 0.91046 to 0.91086, saving model to model_r.hdf5\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9134 - val_loss: 0.1846 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91086\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1817 - accuracy: 0.9139 - val_loss: 0.1842 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91086\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1793 - accuracy: 0.9160 - val_loss: 0.1849 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91086\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9114 - val_loss: 0.1847 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00064: val_accuracy improved from 0.91086 to 0.91127, saving model to model_r.hdf5\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9113\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4398 - accuracy: 0.7868 - val_loss: 0.2760 - val_accuracy: 0.8998\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89977, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2694 - accuracy: 0.9000 - val_loss: 0.2525 - val_accuracy: 0.9030\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89977 to 0.90301, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2465 - accuracy: 0.9010 - val_loss: 0.2318 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90301 to 0.90520, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2238 - accuracy: 0.9081 - val_loss: 0.2226 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90520 to 0.90746, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2187 - accuracy: 0.9097 - val_loss: 0.2191 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90746 to 0.90763, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2047 - accuracy: 0.9174 - val_loss: 0.2171 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90763 to 0.90852, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2097 - accuracy: 0.9115 - val_loss: 0.2149 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90852 to 0.90868, saving model to model_r.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2104 - accuracy: 0.9124 - val_loss: 0.2119 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90868\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2052 - accuracy: 0.9137 - val_loss: 0.2088 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90868\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1999 - accuracy: 0.9145 - val_loss: 0.2062 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90868 to 0.90916, saving model to model_r.hdf5\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1955 - accuracy: 0.9171 - val_loss: 0.2064 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.90916 to 0.90925, saving model to model_r.hdf5\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9148 - val_loss: 0.2030 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.90925 to 0.90965, saving model to model_r.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9126 - val_loss: 0.2020 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90965\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9146 - val_loss: 0.2012 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90965 to 0.90997, saving model to model_r.hdf5\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9161 - val_loss: 0.2001 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90997\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9146 - val_loss: 0.2001 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90997\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9103 - val_loss: 0.1986 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90997\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9127 - val_loss: 0.1986 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90997\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9112 - val_loss: 0.1980 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90997\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9143 - val_loss: 0.1987 - val_accuracy: 0.9038\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90997\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9107 - val_loss: 0.1959 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90997\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9130 - val_loss: 0.1957 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90997\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9100 - val_loss: 0.1944 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90997\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9137 - val_loss: 0.1951 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90997\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9092 - val_loss: 0.1967 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90997\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9140 - val_loss: 0.1929 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90997\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9147 - val_loss: 0.1924 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90997\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9113 - val_loss: 0.1928 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90997\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9133 - val_loss: 0.1914 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90997\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9109 - val_loss: 0.1914 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90997\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9162 - val_loss: 0.1914 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90997\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9145 - val_loss: 0.1904 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90997\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9168 - val_loss: 0.1928 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90997\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9124 - val_loss: 0.1906 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90997\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9129 - val_loss: 0.1900 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90997\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9156 - val_loss: 0.1904 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90997\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9139 - val_loss: 0.1893 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90997\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9136 - val_loss: 0.1900 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90997\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9132 - val_loss: 0.1909 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90997\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9133 - val_loss: 0.1933 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90997\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9122 - val_loss: 0.1889 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90997\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9139 - val_loss: 0.1893 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90997\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9138 - val_loss: 0.1898 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90997\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9177 - val_loss: 0.1931 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90997\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9165 - val_loss: 0.1894 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90997\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9125 - val_loss: 0.1893 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90997\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9125 - val_loss: 0.1887 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90997\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9164 - val_loss: 0.1885 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.90997 to 0.91014, saving model to model_r.hdf5\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9146 - val_loss: 0.1884 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91014\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9137 - val_loss: 0.1891 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91014\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9144 - val_loss: 0.1915 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.91014 to 0.91127, saving model to model_r.hdf5\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9136 - val_loss: 0.1906 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91127\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9161 - val_loss: 0.1872 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91127\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9129 - val_loss: 0.1894 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91127\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9155 - val_loss: 0.1881 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91127\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9137 - val_loss: 0.1878 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91127\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9139 - val_loss: 0.1889 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91127\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9148 - val_loss: 0.1871 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91127\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9130 - val_loss: 0.1896 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91127\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9162 - val_loss: 0.1873 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91127\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9141 - val_loss: 0.1865 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91127\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9129 - val_loss: 0.1863 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91127\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1785 - accuracy: 0.9159 - val_loss: 0.1880 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91127\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9141 - val_loss: 0.1869 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91127\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9113\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4519 - accuracy: 0.8827 - val_loss: 0.2913 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2776 - accuracy: 0.8889 - val_loss: 0.2639 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2462 - accuracy: 0.8889 - val_loss: 0.2330 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2253 - accuracy: 0.9027 - val_loss: 0.2172 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.88423 to 0.90722, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2130 - accuracy: 0.9099 - val_loss: 0.2112 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90722 to 0.90730, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2083 - accuracy: 0.9112 - val_loss: 0.2144 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90730\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9159 - val_loss: 0.2071 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90730\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9125 - val_loss: 0.2114 - val_accuracy: 0.9009\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90730\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9109 - val_loss: 0.2059 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90730\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9086 - val_loss: 0.2060 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90730 to 0.90763, saving model to model_r.hdf5\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9131 - val_loss: 0.2063 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90763\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9104 - val_loss: 0.2077 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90763\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9123 - val_loss: 0.2057 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90763\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2029 - accuracy: 0.9102 - val_loss: 0.2057 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90763\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9099 - val_loss: 0.2062 - val_accuracy: 0.9051\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90763\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9116 - val_loss: 0.2069 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90763\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9115 - val_loss: 0.2089 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90763\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9103 - val_loss: 0.2066 - val_accuracy: 0.9044\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90763\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9100 - val_loss: 0.2070 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90763\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9109 - val_loss: 0.2066 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.90763 to 0.90811, saving model to model_r.hdf5\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9110 - val_loss: 0.2055 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90811\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2029 - accuracy: 0.9116 - val_loss: 0.2059 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90811\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9135 - val_loss: 0.2061 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90811\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9123 - val_loss: 0.2063 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90811\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9124 - val_loss: 0.2068 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90811\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9130 - val_loss: 0.2054 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90811\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9129 - val_loss: 0.2071 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90811\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9108 - val_loss: 0.2053 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90811\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9118 - val_loss: 0.2052 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90811\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2003 - accuracy: 0.9099 - val_loss: 0.2061 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90811\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9105 - val_loss: 0.2053 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90811\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2057 - accuracy: 0.9090 - val_loss: 0.2052 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90811\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2003 - accuracy: 0.9095 - val_loss: 0.2066 - val_accuracy: 0.9036\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90811\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2067 - accuracy: 0.9071 - val_loss: 0.2050 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90811\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9119 - val_loss: 0.2071 - val_accuracy: 0.9028\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90811\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9103 - val_loss: 0.2048 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90811\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1997 - accuracy: 0.9105 - val_loss: 0.2065 - val_accuracy: 0.9031\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90811\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9119 - val_loss: 0.2042 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90811\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2025 - accuracy: 0.9080 - val_loss: 0.2115 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90811\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2000 - accuracy: 0.9114 - val_loss: 0.2048 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90811\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9105 - val_loss: 0.2038 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90811\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2000 - accuracy: 0.9117 - val_loss: 0.2121 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90811\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1970 - accuracy: 0.9106 - val_loss: 0.2045 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00043: val_accuracy improved from 0.90811 to 0.90827, saving model to model_r.hdf5\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9079 - val_loss: 0.2041 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90827\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9109 - val_loss: 0.2035 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90827\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9108 - val_loss: 0.2035 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90827\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2006 - accuracy: 0.9088 - val_loss: 0.2028 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90827\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1999 - accuracy: 0.9105 - val_loss: 0.2038 - val_accuracy: 0.9033\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90827\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9112 - val_loss: 0.2092 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90827\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2020 - accuracy: 0.9069 - val_loss: 0.2021 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90827\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9114 - val_loss: 0.2028 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90827\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9103 - val_loss: 0.2014 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90827\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2003 - accuracy: 0.9071 - val_loss: 0.2068 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90827\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9123 - val_loss: 0.2012 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90827\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9100 - val_loss: 0.2011 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90827\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9104 - val_loss: 0.2048 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90827\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9117 - val_loss: 0.2010 - val_accuracy: 0.9051\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90827\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9117 - val_loss: 0.2005 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90827\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9126 - val_loss: 0.2007 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90827\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9061 - val_loss: 0.2013 - val_accuracy: 0.9042\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90827\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9067 - val_loss: 0.2006 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90827\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2025 - accuracy: 0.9057 - val_loss: 0.2003 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90827\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9092 - val_loss: 0.2004 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90827\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9130 - val_loss: 0.2006 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90827\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2045 - accuracy: 0.9083\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3734 - accuracy: 0.8899 - val_loss: 0.2689 - val_accuracy: 0.8992\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89921, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2491 - accuracy: 0.9008 - val_loss: 0.2276 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89921 to 0.90479, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2178 - accuracy: 0.9081 - val_loss: 0.2099 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90479 to 0.90682, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9128 - val_loss: 0.2103 - val_accuracy: 0.9053\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90682\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9141 - val_loss: 0.2057 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90682 to 0.90714, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9153 - val_loss: 0.2028 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90714\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1974 - accuracy: 0.9133 - val_loss: 0.2038 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90714\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9150 - val_loss: 0.2019 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90714\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1955 - accuracy: 0.9122 - val_loss: 0.2012 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90714\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9145 - val_loss: 0.2002 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90714\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9139 - val_loss: 0.2004 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90714\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9153 - val_loss: 0.1993 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90714\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1959 - accuracy: 0.9127 - val_loss: 0.2000 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90714\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9138 - val_loss: 0.1989 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90714 to 0.90722, saving model to model_r.hdf5\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9141 - val_loss: 0.2002 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90722\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9135 - val_loss: 0.2006 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.90722 to 0.90755, saving model to model_r.hdf5\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9140 - val_loss: 0.1990 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.90755 to 0.90779, saving model to model_r.hdf5\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9144 - val_loss: 0.1982 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.90779 to 0.90827, saving model to model_r.hdf5\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9140 - val_loss: 0.1978 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90827\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9130 - val_loss: 0.1978 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90827\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9137 - val_loss: 0.1978 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90827\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9124 - val_loss: 0.1972 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.90827 to 0.90868, saving model to model_r.hdf5\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9160 - val_loss: 0.1973 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90868\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9152 - val_loss: 0.1981 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90868\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9150 - val_loss: 0.1967 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.90868 to 0.90908, saving model to model_r.hdf5\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9128 - val_loss: 0.2034 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90908\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1946 - accuracy: 0.9122 - val_loss: 0.1969 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90908\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9139 - val_loss: 0.1965 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90908\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9155 - val_loss: 0.2029 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90908\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9163 - val_loss: 0.1966 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90908\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9140 - val_loss: 0.1955 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90908\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9163 - val_loss: 0.1977 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90908\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9137 - val_loss: 0.1958 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90908\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9129 - val_loss: 0.1949 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.90908 to 0.90941, saving model to model_r.hdf5\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9138 - val_loss: 0.2025 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90941\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9146 - val_loss: 0.1956 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.90941 to 0.90949, saving model to model_r.hdf5\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9161 - val_loss: 0.1950 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90949\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1895 - accuracy: 0.9157 - val_loss: 0.1945 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90949\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9144 - val_loss: 0.1950 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90949\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9128 - val_loss: 0.1945 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90949\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9132 - val_loss: 0.1960 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.90949 to 0.90973, saving model to model_r.hdf5\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9149 - val_loss: 0.1954 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90973\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9156 - val_loss: 0.1952 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90973\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9154 - val_loss: 0.1959 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90973\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9108 - val_loss: 0.1938 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.90973 to 0.90997, saving model to model_r.hdf5\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9146 - val_loss: 0.1940 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.90997 to 0.91030, saving model to model_r.hdf5\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9154 - val_loss: 0.1938 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91030\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9181 - val_loss: 0.1943 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91030\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9161 - val_loss: 0.1935 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91030\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9187 - val_loss: 0.1937 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91030\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9146 - val_loss: 0.1955 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91030\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9133 - val_loss: 0.1933 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91030\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9156 - val_loss: 0.1935 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91030\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9142 - val_loss: 0.1926 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00054: val_accuracy improved from 0.91030 to 0.91054, saving model to model_r.hdf5\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9148 - val_loss: 0.1942 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91054\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9136 - val_loss: 0.1924 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91054\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9135 - val_loss: 0.1934 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91054\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9156 - val_loss: 0.1954 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.91054 to 0.91070, saving model to model_r.hdf5\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9134 - val_loss: 0.1925 - val_accuracy: 0.9116\n",
            "\n",
            "Epoch 00059: val_accuracy improved from 0.91070 to 0.91159, saving model to model_r.hdf5\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9187 - val_loss: 0.1916 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91159\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9174 - val_loss: 0.1933 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91159\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9164 - val_loss: 0.1921 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91159\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9147 - val_loss: 0.1918 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91159\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9158 - val_loss: 0.1924 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91159\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1925 - accuracy: 0.9116\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3346 - accuracy: 0.8907 - val_loss: 0.2725 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89605, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2650 - accuracy: 0.8979 - val_loss: 0.2374 - val_accuracy: 0.9028\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89605 to 0.90277, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2259 - accuracy: 0.9053 - val_loss: 0.2169 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90277 to 0.90576, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2106 - accuracy: 0.9102 - val_loss: 0.2095 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90576 to 0.90690, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2040 - accuracy: 0.9118 - val_loss: 0.2071 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90690 to 0.90771, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2027 - accuracy: 0.9125 - val_loss: 0.2068 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90771 to 0.90787, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1968 - accuracy: 0.9141 - val_loss: 0.2061 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90787 to 0.90795, saving model to model_r.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1978 - accuracy: 0.9126 - val_loss: 0.2052 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90795\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9104 - val_loss: 0.2030 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90795\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9143 - val_loss: 0.2029 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90795\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9136 - val_loss: 0.2029 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90795\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9130 - val_loss: 0.2016 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.90795 to 0.90827, saving model to model_r.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9130 - val_loss: 0.2012 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.90827 to 0.90835, saving model to model_r.hdf5\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9142 - val_loss: 0.2008 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90835\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9159 - val_loss: 0.2003 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.90835 to 0.90860, saving model to model_r.hdf5\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9138 - val_loss: 0.2005 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90860\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9149 - val_loss: 0.1998 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90860\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9129 - val_loss: 0.1992 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90860\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9108 - val_loss: 0.2007 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90860\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9159 - val_loss: 0.1992 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90860\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9138 - val_loss: 0.1981 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.90860 to 0.90884, saving model to model_r.hdf5\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9158 - val_loss: 0.1988 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90884\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9120 - val_loss: 0.1974 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90884\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9133 - val_loss: 0.1976 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90884\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9137 - val_loss: 0.1963 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90884\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9149 - val_loss: 0.1964 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90884\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9175 - val_loss: 0.1968 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90884\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9118 - val_loss: 0.1983 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90884\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9145 - val_loss: 0.1961 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90884\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9122 - val_loss: 0.1956 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90884\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9147 - val_loss: 0.1946 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90884\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9129 - val_loss: 0.1952 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90884\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9146 - val_loss: 0.1935 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90884\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9144 - val_loss: 0.1936 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90884\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9121 - val_loss: 0.1943 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90884\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9127 - val_loss: 0.1962 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90884\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9106 - val_loss: 0.1932 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90884\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9142 - val_loss: 0.1932 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90884\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9122 - val_loss: 0.1918 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90884\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9117 - val_loss: 0.1924 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90884\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9120 - val_loss: 0.1918 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90884\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9101 - val_loss: 0.1915 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90884\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9171 - val_loss: 0.1922 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90884\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1857 - accuracy: 0.9148 - val_loss: 0.1930 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90884\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9144 - val_loss: 0.1929 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.90884 to 0.90900, saving model to model_r.hdf5\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9151 - val_loss: 0.1916 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90900\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9139 - val_loss: 0.1914 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.90900 to 0.90925, saving model to model_r.hdf5\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9156 - val_loss: 0.1910 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00048: val_accuracy improved from 0.90925 to 0.90941, saving model to model_r.hdf5\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9170 - val_loss: 0.1908 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90941\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9160 - val_loss: 0.1925 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90941\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9117 - val_loss: 0.1923 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.90941 to 0.90949, saving model to model_r.hdf5\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9142 - val_loss: 0.1911 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90949\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9127 - val_loss: 0.1918 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90949\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9149 - val_loss: 0.1927 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90949\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9171 - val_loss: 0.1928 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90949\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9131 - val_loss: 0.1947 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90949\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9140 - val_loss: 0.1895 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90949\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9153 - val_loss: 0.1897 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.90949 to 0.90957, saving model to model_r.hdf5\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9113 - val_loss: 0.1939 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90957\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9145 - val_loss: 0.1914 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90957\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9157 - val_loss: 0.1944 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90957\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9132 - val_loss: 0.1902 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00062: val_accuracy improved from 0.90957 to 0.90997, saving model to model_r.hdf5\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9131 - val_loss: 0.1904 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90997\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9139 - val_loss: 0.1918 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90997\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9100\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.4085 - accuracy: 0.8229 - val_loss: 0.2971 - val_accuracy: 0.8920\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89200, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2866 - accuracy: 0.8974 - val_loss: 0.2887 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89200 to 0.89783, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2769 - accuracy: 0.9012 - val_loss: 0.2873 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89783 to 0.89880, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2816 - accuracy: 0.9000 - val_loss: 0.2886 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.89880\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2846 - accuracy: 0.8996 - val_loss: 0.2864 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.89880 to 0.89888, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.8963 - val_loss: 0.2870 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.89888 to 0.89913, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2788 - accuracy: 0.8992 - val_loss: 0.2865 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.89913\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2803 - accuracy: 0.8986 - val_loss: 0.2865 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.89913 to 0.89937, saving model to model_r.hdf5\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2834 - accuracy: 0.8999 - val_loss: 0.2860 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.89937\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2815 - accuracy: 0.9005 - val_loss: 0.2861 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.89937\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2736 - accuracy: 0.9019 - val_loss: 0.2858 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.89937\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2802 - accuracy: 0.8998 - val_loss: 0.2860 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.89937 to 0.89961, saving model to model_r.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2783 - accuracy: 0.9019 - val_loss: 0.2853 - val_accuracy: 0.8993\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.89961\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2793 - accuracy: 0.8994 - val_loss: 0.2870 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89961\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2766 - accuracy: 0.9020 - val_loss: 0.2873 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.89961\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2783 - accuracy: 0.9002 - val_loss: 0.2866 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.89961\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.8978 - val_loss: 0.2852 - val_accuracy: 0.8991\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.89961\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2785 - accuracy: 0.8997 - val_loss: 0.2851 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.89961\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2773 - accuracy: 0.9007 - val_loss: 0.2875 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89961\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2764 - accuracy: 0.9020 - val_loss: 0.2850 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89961\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2779 - accuracy: 0.8997 - val_loss: 0.2850 - val_accuracy: 0.8993\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89961\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2817 - accuracy: 0.8998 - val_loss: 0.2867 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89961\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2817 - accuracy: 0.8986 - val_loss: 0.2849 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89961\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2781 - accuracy: 0.9007 - val_loss: 0.2882 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89961\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2788 - accuracy: 0.8985 - val_loss: 0.2850 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89961\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2765 - accuracy: 0.9003 - val_loss: 0.2854 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89961\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2781 - accuracy: 0.9001 - val_loss: 0.2857 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.89961\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2698 - accuracy: 0.9030 - val_loss: 0.2842 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00028: val_accuracy improved from 0.89961 to 0.89969, saving model to model_r.hdf5\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2774 - accuracy: 0.9003 - val_loss: 0.2845 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89969\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2791 - accuracy: 0.8997 - val_loss: 0.2846 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89969\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.9002 - val_loss: 0.2850 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89969\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2705 - accuracy: 0.9044 - val_loss: 0.2839 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89969\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2774 - accuracy: 0.9011 - val_loss: 0.2840 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89969\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2687 - accuracy: 0.9049 - val_loss: 0.2842 - val_accuracy: 0.8983\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89969\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2771 - accuracy: 0.9011 - val_loss: 0.2840 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89969\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2730 - accuracy: 0.9030 - val_loss: 0.2849 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89969\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2798 - accuracy: 0.9007 - val_loss: 0.2851 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89969\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2768 - accuracy: 0.8995 - val_loss: 0.2835 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89969\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2799 - accuracy: 0.8998 - val_loss: 0.2835 - val_accuracy: 0.8989\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89969\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2693 - accuracy: 0.9049 - val_loss: 0.2836 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89969\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2687 - accuracy: 0.9040 - val_loss: 0.2838 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89969\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2686 - accuracy: 0.9030 - val_loss: 0.2836 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89969\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2777 - accuracy: 0.9008 - val_loss: 0.2849 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89969\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2752 - accuracy: 0.9002 - val_loss: 0.2840 - val_accuracy: 0.8995\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89969\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2761 - accuracy: 0.9019 - val_loss: 0.2837 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89969\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2771 - accuracy: 0.9007 - val_loss: 0.2842 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89969\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2768 - accuracy: 0.8975 - val_loss: 0.2841 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89969\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2729 - accuracy: 0.9024 - val_loss: 0.2834 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89969\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2739 - accuracy: 0.9009 - val_loss: 0.2848 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89969\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2783 - accuracy: 0.8980 - val_loss: 0.2853 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89969\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2739 - accuracy: 0.9032 - val_loss: 0.2834 - val_accuracy: 0.8985\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89969\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2800 - accuracy: 0.8979 - val_loss: 0.2840 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89969\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2727 - accuracy: 0.9007 - val_loss: 0.2835 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89969\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2763 - accuracy: 0.9012 - val_loss: 0.2845 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89969\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2758 - accuracy: 0.9004 - val_loss: 0.2858 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89969\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8994 - val_loss: 0.2846 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89969\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2742 - accuracy: 0.9018 - val_loss: 0.2848 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89969\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2765 - accuracy: 0.9006 - val_loss: 0.2843 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89969\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2743 - accuracy: 0.9018 - val_loss: 0.2840 - val_accuracy: 0.8984\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89969\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2763 - accuracy: 0.9004 - val_loss: 0.2838 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89969\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2688 - accuracy: 0.9025 - val_loss: 0.2837 - val_accuracy: 0.8993\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89969\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2677 - accuracy: 0.9034 - val_loss: 0.2841 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89969\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2722 - accuracy: 0.9026 - val_loss: 0.2835 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89969\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2771 - accuracy: 0.9003 - val_loss: 0.2831 - val_accuracy: 0.8988\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89969\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2842 - accuracy: 0.8997\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4719 - accuracy: 0.7508 - val_loss: 0.2925 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2877 - accuracy: 0.8854 - val_loss: 0.2621 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2464 - accuracy: 0.8909 - val_loss: 0.2359 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88423 to 0.90447, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2255 - accuracy: 0.9070 - val_loss: 0.2229 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90447 to 0.90706, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2134 - accuracy: 0.9099 - val_loss: 0.2118 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90706 to 0.90811, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2062 - accuracy: 0.9138 - val_loss: 0.2075 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90811 to 0.90868, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2046 - accuracy: 0.9128 - val_loss: 0.2065 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90868\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9128 - val_loss: 0.2107 - val_accuracy: 0.9033\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90868\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2042 - accuracy: 0.9107 - val_loss: 0.2043 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90868\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1970 - accuracy: 0.9146 - val_loss: 0.2040 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90868\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9147 - val_loss: 0.2029 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90868\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9146 - val_loss: 0.2037 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90868\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9087 - val_loss: 0.2026 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90868\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9091 - val_loss: 0.2026 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90868\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9139 - val_loss: 0.2015 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90868\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9128 - val_loss: 0.2062 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90868\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1988 - accuracy: 0.9124 - val_loss: 0.2034 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90868\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9142 - val_loss: 0.2000 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90868\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9113 - val_loss: 0.1994 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90868\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9151 - val_loss: 0.1986 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90868\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9123 - val_loss: 0.1986 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90868\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9139 - val_loss: 0.1982 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90868\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9139 - val_loss: 0.1986 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90868\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9137 - val_loss: 0.1971 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90868\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9137 - val_loss: 0.2015 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90868\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9159 - val_loss: 0.2000 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90868\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9110 - val_loss: 0.1980 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90868\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9158 - val_loss: 0.1959 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90868\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9110 - val_loss: 0.1957 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90868\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9141 - val_loss: 0.1956 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90868\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9114 - val_loss: 0.1952 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90868\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9149 - val_loss: 0.1953 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90868\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9147 - val_loss: 0.1968 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90868\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9141 - val_loss: 0.1955 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90868\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9125 - val_loss: 0.1963 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.90868 to 0.90876, saving model to model_r.hdf5\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9138 - val_loss: 0.1940 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90876\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9098 - val_loss: 0.1940 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.90876 to 0.90892, saving model to model_r.hdf5\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9134 - val_loss: 0.1945 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90892\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9118 - val_loss: 0.1945 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90892\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9123 - val_loss: 0.1954 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90892\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9121 - val_loss: 0.1946 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90892\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9142 - val_loss: 0.1943 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00042: val_accuracy improved from 0.90892 to 0.90916, saving model to model_r.hdf5\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9148 - val_loss: 0.1977 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90916\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9170 - val_loss: 0.1936 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90916\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9152 - val_loss: 0.1942 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90916\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9142 - val_loss: 0.1940 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.90916 to 0.90997, saving model to model_r.hdf5\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9146 - val_loss: 0.1939 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90997\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9136 - val_loss: 0.1942 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90997\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9114 - val_loss: 0.1942 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90997\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9129 - val_loss: 0.1974 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90997\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9135 - val_loss: 0.1933 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90997\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9117 - val_loss: 0.1929 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90997\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9121 - val_loss: 0.1931 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90997\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9142 - val_loss: 0.1942 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90997\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9140 - val_loss: 0.1929 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90997\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9122 - val_loss: 0.1967 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90997\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9125 - val_loss: 0.1933 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90997\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9134 - val_loss: 0.1941 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90997\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9141 - val_loss: 0.1950 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90997\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9143 - val_loss: 0.1948 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90997\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1959 - accuracy: 0.9111 - val_loss: 0.1955 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90997\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9128 - val_loss: 0.1928 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90997\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9156 - val_loss: 0.1929 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90997\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9120 - val_loss: 0.1929 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90997\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1940 - accuracy: 0.9100\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4225 - accuracy: 0.8869 - val_loss: 0.3053 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2882 - accuracy: 0.8898 - val_loss: 0.2836 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2695 - accuracy: 0.8907 - val_loss: 0.2627 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2507 - accuracy: 0.8865 - val_loss: 0.2410 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2272 - accuracy: 0.9035 - val_loss: 0.2242 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.88423 to 0.90665, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2132 - accuracy: 0.9096 - val_loss: 0.2153 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90665\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2112 - accuracy: 0.9089 - val_loss: 0.2115 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90665 to 0.90682, saving model to model_r.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9086 - val_loss: 0.2094 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90682 to 0.90714, saving model to model_r.hdf5\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2023 - accuracy: 0.9098 - val_loss: 0.2079 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90714\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9109 - val_loss: 0.2076 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90714\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9096 - val_loss: 0.2071 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90714\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9087 - val_loss: 0.2068 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90714\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9089 - val_loss: 0.2070 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90714\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9083 - val_loss: 0.2067 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90714\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9082 - val_loss: 0.2059 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90714\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2042 - accuracy: 0.9094 - val_loss: 0.2062 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90714\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9103 - val_loss: 0.2076 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90714\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2049 - accuracy: 0.9062 - val_loss: 0.2077 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90714\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2038 - accuracy: 0.9070 - val_loss: 0.2060 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90714\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9094 - val_loss: 0.2057 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90714\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9076 - val_loss: 0.2057 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90714\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9089 - val_loss: 0.2060 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90714\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2041 - accuracy: 0.9074 - val_loss: 0.2058 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90714\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9089 - val_loss: 0.2055 - val_accuracy: 0.9040\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90714\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9078 - val_loss: 0.2057 - val_accuracy: 0.9034\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90714\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9103 - val_loss: 0.2058 - val_accuracy: 0.9043\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90714\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9096 - val_loss: 0.2061 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90714\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9093 - val_loss: 0.2062 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90714\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9093 - val_loss: 0.2056 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90714\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9067 - val_loss: 0.2070 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90714\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2068 - accuracy: 0.9056 - val_loss: 0.2053 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90714\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9079 - val_loss: 0.2067 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90714\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9066 - val_loss: 0.2056 - val_accuracy: 0.9051\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90714\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9104 - val_loss: 0.2057 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90714\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9077 - val_loss: 0.2059 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90714\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9103 - val_loss: 0.2050 - val_accuracy: 0.9040\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90714\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2005 - accuracy: 0.9096 - val_loss: 0.2055 - val_accuracy: 0.9032\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90714\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9080 - val_loss: 0.2059 - val_accuracy: 0.9030\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90714\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2053 - accuracy: 0.9084 - val_loss: 0.2056 - val_accuracy: 0.9049\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90714\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2021 - accuracy: 0.9099 - val_loss: 0.2053 - val_accuracy: 0.9038\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90714\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9081 - val_loss: 0.2052 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90714\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2075 - accuracy: 0.9060 - val_loss: 0.2050 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90714\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1999 - accuracy: 0.9099 - val_loss: 0.2066 - val_accuracy: 0.9024\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90714\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2033 - accuracy: 0.9099 - val_loss: 0.2046 - val_accuracy: 0.9043\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90714\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9109 - val_loss: 0.2045 - val_accuracy: 0.9038\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90714\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9125 - val_loss: 0.2056 - val_accuracy: 0.9028\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90714\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1988 - accuracy: 0.9094 - val_loss: 0.2058 - val_accuracy: 0.9049\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90714\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9060 - val_loss: 0.2057 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90714\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9089 - val_loss: 0.2042 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90714\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9081 - val_loss: 0.2046 - val_accuracy: 0.9026\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90714\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9082 - val_loss: 0.2043 - val_accuracy: 0.9036\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90714\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9077 - val_loss: 0.2048 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90714\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2046 - accuracy: 0.9061 - val_loss: 0.2038 - val_accuracy: 0.9032\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90714\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2020 - accuracy: 0.9093 - val_loss: 0.2036 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90714\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9087 - val_loss: 0.2040 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90714\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1989 - accuracy: 0.9098 - val_loss: 0.2042 - val_accuracy: 0.9024\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90714\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2016 - accuracy: 0.9056 - val_loss: 0.2077 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90714\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9094 - val_loss: 0.2045 - val_accuracy: 0.9011\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90714\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9055 - val_loss: 0.2031 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90714\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9119 - val_loss: 0.2030 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90714\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9076 - val_loss: 0.2028 - val_accuracy: 0.9044\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90714\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2006 - accuracy: 0.9098 - val_loss: 0.2038 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90714\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9064 - val_loss: 0.2030 - val_accuracy: 0.9033\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90714\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9093 - val_loss: 0.2027 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90714\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2094 - accuracy: 0.9071\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.3663 - accuracy: 0.8550 - val_loss: 0.2782 - val_accuracy: 0.8901\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89006, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2685 - accuracy: 0.8955 - val_loss: 0.2392 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89006 to 0.90479, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2251 - accuracy: 0.9087 - val_loss: 0.2193 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90479 to 0.90876, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2061 - accuracy: 0.9142 - val_loss: 0.2162 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90876\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9167 - val_loss: 0.2099 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90876\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2065 - accuracy: 0.9119 - val_loss: 0.2101 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90876\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9134 - val_loss: 0.2055 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90876\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9146 - val_loss: 0.2083 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90876\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9142 - val_loss: 0.2035 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90876\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2005 - accuracy: 0.9129 - val_loss: 0.2064 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90876\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9140 - val_loss: 0.2022 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90876\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2015 - accuracy: 0.9115 - val_loss: 0.2033 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90876\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9160 - val_loss: 0.2016 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90876\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9133 - val_loss: 0.2014 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90876\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9154 - val_loss: 0.2013 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90876\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9150 - val_loss: 0.2006 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90876\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9111 - val_loss: 0.2018 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90876\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9131 - val_loss: 0.1999 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90876\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9126 - val_loss: 0.2000 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90876\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9134 - val_loss: 0.1999 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90876\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9119 - val_loss: 0.2013 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90876\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9139 - val_loss: 0.1989 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90876\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9118 - val_loss: 0.2034 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90876\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9131 - val_loss: 0.2003 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90876\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9111 - val_loss: 0.1989 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90876\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9155 - val_loss: 0.1992 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90876\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9135 - val_loss: 0.2012 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90876\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9147 - val_loss: 0.1986 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90876\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9128 - val_loss: 0.1985 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90876\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9131 - val_loss: 0.1978 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90876\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9143 - val_loss: 0.2003 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90876\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1955 - accuracy: 0.9142 - val_loss: 0.1985 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90876\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9141 - val_loss: 0.2012 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90876\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9127 - val_loss: 0.1976 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90876\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9127 - val_loss: 0.1991 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90876\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9138 - val_loss: 0.1973 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90876\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9135 - val_loss: 0.1983 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90876\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9138 - val_loss: 0.1975 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90876\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9140 - val_loss: 0.1981 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90876\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9105 - val_loss: 0.1975 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90876\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9151 - val_loss: 0.1971 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90876\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9158 - val_loss: 0.1962 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90876\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9131 - val_loss: 0.1972 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90876\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9139 - val_loss: 0.1978 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90876\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9142 - val_loss: 0.1962 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90876\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9158 - val_loss: 0.1976 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90876\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1928 - accuracy: 0.9115 - val_loss: 0.1957 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90876\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9135 - val_loss: 0.1953 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90876\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9163 - val_loss: 0.1976 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90876\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9136 - val_loss: 0.1963 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90876\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9152 - val_loss: 0.1949 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90876\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9141 - val_loss: 0.1946 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90876\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9158 - val_loss: 0.1938 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90876\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9141 - val_loss: 0.1953 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90876\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9136 - val_loss: 0.1937 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90876\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9162 - val_loss: 0.1977 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90876\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9163 - val_loss: 0.1945 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90876\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9148 - val_loss: 0.1930 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.90876 to 0.90900, saving model to model_r.hdf5\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9163 - val_loss: 0.1935 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90900\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9144 - val_loss: 0.1930 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90900\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9130 - val_loss: 0.1931 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90900\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9171 - val_loss: 0.1954 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90900\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9156 - val_loss: 0.1929 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90900\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9173 - val_loss: 0.1917 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00064: val_accuracy improved from 0.90900 to 0.90933, saving model to model_r.hdf5\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9093\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.3539 - accuracy: 0.8702 - val_loss: 0.2880 - val_accuracy: 0.8875\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88747, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2754 - accuracy: 0.8936 - val_loss: 0.2513 - val_accuracy: 0.9023\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88747 to 0.90228, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2360 - accuracy: 0.9062 - val_loss: 0.2254 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90228 to 0.90609, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2157 - accuracy: 0.9116 - val_loss: 0.2199 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90609 to 0.90827, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2111 - accuracy: 0.9126 - val_loss: 0.2169 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90827 to 0.90876, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2045 - accuracy: 0.9168 - val_loss: 0.2140 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90876 to 0.90900, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2148 - accuracy: 0.9095 - val_loss: 0.2124 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90900\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2076 - accuracy: 0.9141 - val_loss: 0.2116 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90900\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2042 - accuracy: 0.9127 - val_loss: 0.2101 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90900\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9139 - val_loss: 0.2077 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90900\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9130 - val_loss: 0.2062 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90900\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2003 - accuracy: 0.9128 - val_loss: 0.2057 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90900\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9127 - val_loss: 0.2042 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90900\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9145 - val_loss: 0.2051 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90900\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9143 - val_loss: 0.2060 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90900\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9138 - val_loss: 0.2022 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90900\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1959 - accuracy: 0.9129 - val_loss: 0.2035 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90900\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9132 - val_loss: 0.2017 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90900\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9100 - val_loss: 0.2034 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90900\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9130 - val_loss: 0.2006 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90900\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1938 - accuracy: 0.9148 - val_loss: 0.1998 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90900\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9126 - val_loss: 0.1998 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90900\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9142 - val_loss: 0.2015 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90900\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9085 - val_loss: 0.1994 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90900\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9105 - val_loss: 0.1991 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90900\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9116 - val_loss: 0.1983 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90900\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9150 - val_loss: 0.2036 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90900\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9108 - val_loss: 0.1994 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90900\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9102 - val_loss: 0.1969 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90900\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9124 - val_loss: 0.1968 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90900\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9108 - val_loss: 0.1968 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90900\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9140 - val_loss: 0.1964 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90900\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9157 - val_loss: 0.2053 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90900\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9146 - val_loss: 0.1989 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90900\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9137 - val_loss: 0.1949 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90900\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9140 - val_loss: 0.1967 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90900\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9139 - val_loss: 0.1944 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90900\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9146 - val_loss: 0.1948 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90900\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9122 - val_loss: 0.1936 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90900\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9165 - val_loss: 0.1930 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90900\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9171 - val_loss: 0.1932 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90900\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9134 - val_loss: 0.1921 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90900\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9160 - val_loss: 0.1949 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90900\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9136 - val_loss: 0.1927 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90900\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9155 - val_loss: 0.1930 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90900\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9148 - val_loss: 0.1911 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90900\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9125 - val_loss: 0.1928 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90900\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1814 - accuracy: 0.9158 - val_loss: 0.1953 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90900\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9143 - val_loss: 0.1904 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90900\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9147 - val_loss: 0.1916 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.90900 to 0.90925, saving model to model_r.hdf5\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9172 - val_loss: 0.1916 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90925\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9155 - val_loss: 0.1900 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00052: val_accuracy improved from 0.90925 to 0.90957, saving model to model_r.hdf5\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9177 - val_loss: 0.1909 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90957\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1786 - accuracy: 0.9168 - val_loss: 0.1913 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90957\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9175 - val_loss: 0.1902 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90957\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9130 - val_loss: 0.1900 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90957\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1835 - accuracy: 0.9163 - val_loss: 0.1899 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00057: val_accuracy improved from 0.90957 to 0.90965, saving model to model_r.hdf5\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9110 - val_loss: 0.1913 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90965\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9158 - val_loss: 0.1911 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90965\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9156 - val_loss: 0.1894 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90965\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9154 - val_loss: 0.1903 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00061: val_accuracy improved from 0.90965 to 0.90973, saving model to model_r.hdf5\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9132 - val_loss: 0.1894 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90973\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1778 - accuracy: 0.9187 - val_loss: 0.1904 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90973\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9132 - val_loss: 0.1888 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00064: val_accuracy improved from 0.90973 to 0.90989, saving model to model_r.hdf5\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1888 - accuracy: 0.9099\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.4244 - accuracy: 0.8772 - val_loss: 0.2737 - val_accuracy: 0.8994\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89945, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2587 - accuracy: 0.9020 - val_loss: 0.2391 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89945 to 0.90617, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2312 - accuracy: 0.9066 - val_loss: 0.2257 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90617 to 0.90633, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2257 - accuracy: 0.9072 - val_loss: 0.2223 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90633 to 0.90682, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2148 - accuracy: 0.9115 - val_loss: 0.2206 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90682 to 0.90819, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2143 - accuracy: 0.9112 - val_loss: 0.2209 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90819\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2065 - accuracy: 0.9145 - val_loss: 0.2258 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90819 to 0.90989, saving model to model_r.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2119 - accuracy: 0.9096 - val_loss: 0.2183 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90989\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2079 - accuracy: 0.9126 - val_loss: 0.2158 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90989\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2031 - accuracy: 0.9143 - val_loss: 0.2163 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90989\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2068 - accuracy: 0.9118 - val_loss: 0.2130 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90989\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2008 - accuracy: 0.9150 - val_loss: 0.2105 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90989\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2023 - accuracy: 0.9112 - val_loss: 0.2085 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90989\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9134 - val_loss: 0.2063 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90989\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2020 - accuracy: 0.9125 - val_loss: 0.2062 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90989\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9134 - val_loss: 0.2040 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.90989 to 0.91030, saving model to model_r.hdf5\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9145 - val_loss: 0.2036 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.91030\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9141 - val_loss: 0.2019 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91030\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9123 - val_loss: 0.2013 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91030\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9129 - val_loss: 0.2023 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91030\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1968 - accuracy: 0.9125 - val_loss: 0.2023 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91030\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9137 - val_loss: 0.2028 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91030\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9153 - val_loss: 0.1999 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91030\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9122 - val_loss: 0.2000 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91030\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9157 - val_loss: 0.2000 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91030\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9142 - val_loss: 0.1995 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.91030 to 0.91086, saving model to model_r.hdf5\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9141 - val_loss: 0.2003 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91086\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9124 - val_loss: 0.1997 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91086\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9126 - val_loss: 0.1988 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91086\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9138 - val_loss: 0.1990 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91086\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9172 - val_loss: 0.1994 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91086\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9162 - val_loss: 0.1993 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91086\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9153 - val_loss: 0.2051 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91086\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9108 - val_loss: 0.2037 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91086\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9160 - val_loss: 0.1973 - val_accuracy: 0.9118\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.91086 to 0.91176, saving model to model_r.hdf5\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9166 - val_loss: 0.2015 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91176\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9153 - val_loss: 0.1975 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91176\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9159 - val_loss: 0.1972 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91176\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9172 - val_loss: 0.1975 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91176\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1900 - accuracy: 0.9159 - val_loss: 0.1969 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91176\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9143 - val_loss: 0.1972 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91176\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9159 - val_loss: 0.1991 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91176\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9164 - val_loss: 0.1970 - val_accuracy: 0.9116\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91176\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9151 - val_loss: 0.1974 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91176\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9165 - val_loss: 0.1971 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91176\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9157 - val_loss: 0.1966 - val_accuracy: 0.9122\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.91176 to 0.91216, saving model to model_r.hdf5\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9140 - val_loss: 0.1966 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91216\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9146 - val_loss: 0.1982 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91216\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9185 - val_loss: 0.1966 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91216\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9183 - val_loss: 0.1966 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91216\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9156 - val_loss: 0.1966 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91216\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9127 - val_loss: 0.1968 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91216\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9161 - val_loss: 0.1957 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91216\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9141 - val_loss: 0.1972 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91216\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9175 - val_loss: 0.1954 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91216\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9181 - val_loss: 0.1955 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91216\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9157 - val_loss: 0.1950 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91216\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9140 - val_loss: 0.1948 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91216\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9166 - val_loss: 0.1950 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91216\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9140 - val_loss: 0.1971 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91216\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9165 - val_loss: 0.1945 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91216\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9154 - val_loss: 0.1964 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91216\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9166 - val_loss: 0.1936 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91216\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9161 - val_loss: 0.1958 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91216\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1966 - accuracy: 0.9122\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.4290 - accuracy: 0.8380 - val_loss: 0.2912 - val_accuracy: 0.8925\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89249, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2751 - accuracy: 0.8971 - val_loss: 0.2404 - val_accuracy: 0.9028\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89249 to 0.90277, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2299 - accuracy: 0.9070 - val_loss: 0.2241 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90277 to 0.90657, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2150 - accuracy: 0.9096 - val_loss: 0.2240 - val_accuracy: 0.9051\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90657\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2165 - accuracy: 0.9096 - val_loss: 0.2195 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90657 to 0.90779, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2146 - accuracy: 0.9094 - val_loss: 0.2188 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90779 to 0.90787, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2083 - accuracy: 0.9123 - val_loss: 0.2180 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90787\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2122 - accuracy: 0.9111 - val_loss: 0.2158 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90787\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2058 - accuracy: 0.9111 - val_loss: 0.2133 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90787\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2067 - accuracy: 0.9115 - val_loss: 0.2126 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90787\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2040 - accuracy: 0.9104 - val_loss: 0.2093 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90787\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9121 - val_loss: 0.2088 - val_accuracy: 0.9036\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90787\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9156 - val_loss: 0.2089 - val_accuracy: 0.9028\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90787\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9140 - val_loss: 0.2044 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90787\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9103 - val_loss: 0.2036 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90787\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9124 - val_loss: 0.2057 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90787\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9095 - val_loss: 0.1996 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90787\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9093 - val_loss: 0.1983 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.90787 to 0.90803, saving model to model_r.hdf5\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9106 - val_loss: 0.2066 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90803\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9101 - val_loss: 0.1977 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90803\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9087 - val_loss: 0.1989 - val_accuracy: 0.9024\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90803\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1947 - accuracy: 0.9113 - val_loss: 0.1982 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90803\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9087 - val_loss: 0.1999 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.90803 to 0.90916, saving model to model_r.hdf5\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9123 - val_loss: 0.1967 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90916\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9112 - val_loss: 0.1958 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90916\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9085 - val_loss: 0.1965 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90916\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9118 - val_loss: 0.2029 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90916\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9081 - val_loss: 0.1947 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90916\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9096 - val_loss: 0.1952 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90916\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9088 - val_loss: 0.1945 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90916\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9142 - val_loss: 0.1946 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90916\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9107 - val_loss: 0.2014 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90916\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9089 - val_loss: 0.1941 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90916\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9077 - val_loss: 0.1934 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90916\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9122 - val_loss: 0.1945 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90916\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9108 - val_loss: 0.1947 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.90916 to 0.90949, saving model to model_r.hdf5\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9140 - val_loss: 0.1931 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90949\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9120 - val_loss: 0.1997 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.90949 to 0.90957, saving model to model_r.hdf5\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9128 - val_loss: 0.1929 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90957\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9079 - val_loss: 0.1981 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90957\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9122 - val_loss: 0.1923 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90957\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9104 - val_loss: 0.1936 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90957\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9121 - val_loss: 0.1928 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90957\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9128 - val_loss: 0.1948 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90957\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9089 - val_loss: 0.1932 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90957\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9139 - val_loss: 0.1948 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90957\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9150 - val_loss: 0.1935 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90957\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9130 - val_loss: 0.1918 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90957\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9125 - val_loss: 0.1930 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90957\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9140 - val_loss: 0.1925 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90957\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9126 - val_loss: 0.1915 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90957\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9108 - val_loss: 0.1919 - val_accuracy: 0.9042\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90957\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9123 - val_loss: 0.1924 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90957\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1852 - accuracy: 0.9129 - val_loss: 0.1912 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90957\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9141 - val_loss: 0.1910 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90957\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9121 - val_loss: 0.1907 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90957\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9130 - val_loss: 0.1908 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90957\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9141 - val_loss: 0.1930 - val_accuracy: 0.9030\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90957\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9121 - val_loss: 0.1922 - val_accuracy: 0.9018\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90957\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1821 - accuracy: 0.9143 - val_loss: 0.1909 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90957\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9123 - val_loss: 0.1916 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90957\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9155 - val_loss: 0.1902 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90957\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9131 - val_loss: 0.1926 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90957\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9134 - val_loss: 0.1949 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90957\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1997 - accuracy: 0.9096\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.5198 - accuracy: 0.7179 - val_loss: 0.2866 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89411, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2688 - accuracy: 0.8975 - val_loss: 0.2442 - val_accuracy: 0.9007\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89411 to 0.90066, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2300 - accuracy: 0.9052 - val_loss: 0.2262 - val_accuracy: 0.9029\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90066 to 0.90293, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2128 - accuracy: 0.9121 - val_loss: 0.2180 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90293 to 0.90617, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2118 - accuracy: 0.9113 - val_loss: 0.2149 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90617 to 0.90714, saving model to model_r.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2033 - accuracy: 0.9143 - val_loss: 0.2125 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90714 to 0.90722, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2069 - accuracy: 0.9114 - val_loss: 0.2094 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90722 to 0.90755, saving model to model_r.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9150 - val_loss: 0.2124 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90755\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2008 - accuracy: 0.9121 - val_loss: 0.2050 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.90755 to 0.90803, saving model to model_r.hdf5\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9115 - val_loss: 0.2048 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90803\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9130 - val_loss: 0.2031 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.90803 to 0.90844, saving model to model_r.hdf5\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9123 - val_loss: 0.2015 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90844\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9149 - val_loss: 0.2053 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90844\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9113 - val_loss: 0.2026 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90844\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9126 - val_loss: 0.1986 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90844\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9141 - val_loss: 0.1962 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90844\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9106 - val_loss: 0.2011 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90844\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9123 - val_loss: 0.1948 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90844\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9108 - val_loss: 0.2119 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90844\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9157 - val_loss: 0.1945 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90844\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9151 - val_loss: 0.1948 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90844\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9150 - val_loss: 0.1955 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90844\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9124 - val_loss: 0.1988 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90844\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9139 - val_loss: 0.1984 - val_accuracy: 0.9056\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90844\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9130 - val_loss: 0.1923 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90844\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9110 - val_loss: 0.1938 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90844\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9145 - val_loss: 0.1907 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90844\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9110 - val_loss: 0.1911 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90844\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1832 - accuracy: 0.9157 - val_loss: 0.1901 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90844\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9153 - val_loss: 0.1905 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90844\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9113 - val_loss: 0.1913 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90844\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9094 - val_loss: 0.1925 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90844\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9130 - val_loss: 0.1898 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90844\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9122 - val_loss: 0.1907 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90844\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9113 - val_loss: 0.1893 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90844\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9164 - val_loss: 0.1916 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90844\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1803 - accuracy: 0.9143 - val_loss: 0.1905 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90844\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9163 - val_loss: 0.1891 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90844\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9175 - val_loss: 0.1893 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90844\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9143 - val_loss: 0.1883 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90844\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9141 - val_loss: 0.1882 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90844\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9171 - val_loss: 0.1899 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90844\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9148 - val_loss: 0.1876 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90844\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1799 - accuracy: 0.9175 - val_loss: 0.1888 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90844\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1794 - accuracy: 0.9152 - val_loss: 0.1887 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90844\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9136 - val_loss: 0.1879 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90844\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9108 - val_loss: 0.1890 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90844\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9143 - val_loss: 0.1890 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90844\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9126 - val_loss: 0.1889 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90844\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9157 - val_loss: 0.1886 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90844\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9165 - val_loss: 0.1969 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90844\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9120 - val_loss: 0.1956 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90844\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9141 - val_loss: 0.1898 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90844\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1833 - accuracy: 0.9144 - val_loss: 0.1898 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90844\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1831 - accuracy: 0.9157 - val_loss: 0.1886 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90844\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1839 - accuracy: 0.9129 - val_loss: 0.1889 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90844\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9140 - val_loss: 0.1963 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90844\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9127 - val_loss: 0.1878 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90844\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9154 - val_loss: 0.1873 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90844\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9131 - val_loss: 0.1885 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90844\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1823 - accuracy: 0.9157 - val_loss: 0.1895 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90844\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1774 - accuracy: 0.9163 - val_loss: 0.1872 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90844\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9126 - val_loss: 0.1879 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90844\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9114 - val_loss: 0.1897 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90844\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2031 - accuracy: 0.9084\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.4269 - accuracy: 0.8527 - val_loss: 0.2670 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89734, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2477 - accuracy: 0.9024 - val_loss: 0.2272 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89734 to 0.90674, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2223 - accuracy: 0.9096 - val_loss: 0.2174 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90674 to 0.90738, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2170 - accuracy: 0.9080 - val_loss: 0.2146 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90738 to 0.90827, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2082 - accuracy: 0.9099 - val_loss: 0.2123 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90827\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2058 - accuracy: 0.9113 - val_loss: 0.2068 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90827 to 0.90835, saving model to model_r.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9135 - val_loss: 0.2054 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90835\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9143 - val_loss: 0.2029 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90835\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2004 - accuracy: 0.9122 - val_loss: 0.2018 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.90835 to 0.90876, saving model to model_r.hdf5\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9124 - val_loss: 0.2037 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90876\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9122 - val_loss: 0.2027 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90876\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9144 - val_loss: 0.1968 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.90876 to 0.90900, saving model to model_r.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9116 - val_loss: 0.1965 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.90900 to 0.90965, saving model to model_r.hdf5\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9103 - val_loss: 0.1960 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90965\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9148 - val_loss: 0.1948 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90965\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1928 - accuracy: 0.9132 - val_loss: 0.1975 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90965\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9115 - val_loss: 0.1946 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90965\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9112 - val_loss: 0.1931 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90965\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9139 - val_loss: 0.1949 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90965\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9147 - val_loss: 0.1925 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90965\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9119 - val_loss: 0.1912 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90965\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9106 - val_loss: 0.1916 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90965\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9141 - val_loss: 0.1925 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90965\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9121 - val_loss: 0.1920 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90965\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9121 - val_loss: 0.1910 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90965\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9136 - val_loss: 0.1902 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90965\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9108 - val_loss: 0.1922 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90965\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9119 - val_loss: 0.1946 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90965\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9110 - val_loss: 0.1897 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90965\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9143 - val_loss: 0.1908 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90965\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9114 - val_loss: 0.1917 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90965\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9120 - val_loss: 0.1889 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90965\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9148 - val_loss: 0.1930 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90965\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9155 - val_loss: 0.1894 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90965\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9147 - val_loss: 0.1900 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90965\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9131 - val_loss: 0.1969 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90965\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9111 - val_loss: 0.1914 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90965\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9114 - val_loss: 0.1890 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90965\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9117 - val_loss: 0.1887 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90965\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9131 - val_loss: 0.1919 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90965\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9125 - val_loss: 0.1889 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90965\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1907 - accuracy: 0.9130 - val_loss: 0.1906 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90965\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9131 - val_loss: 0.1889 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90965\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9140 - val_loss: 0.1886 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90965\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9113 - val_loss: 0.1890 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90965\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1845 - accuracy: 0.9138 - val_loss: 0.1884 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90965\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9141 - val_loss: 0.1879 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90965\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9098 - val_loss: 0.1994 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90965\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9119 - val_loss: 0.1900 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90965\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9112 - val_loss: 0.1898 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90965\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1850 - accuracy: 0.9124 - val_loss: 0.1945 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90965\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9112 - val_loss: 0.1884 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90965\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9149 - val_loss: 0.1942 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.90965\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9118 - val_loss: 0.1889 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90965\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9111 - val_loss: 0.1901 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90965\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9112 - val_loss: 0.1884 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90965\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9111 - val_loss: 0.1888 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90965\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9114 - val_loss: 0.1884 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90965\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1810 - accuracy: 0.9164 - val_loss: 0.1888 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90965\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9153 - val_loss: 0.1881 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90965\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9152 - val_loss: 0.1893 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90965\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1851 - accuracy: 0.9159 - val_loss: 0.1884 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90965\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9130 - val_loss: 0.1902 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90965\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9146 - val_loss: 0.1880 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90965\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1965 - accuracy: 0.9097\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.4297 - accuracy: 0.8634 - val_loss: 0.2829 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89605, saving model to model_r.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2667 - accuracy: 0.9003 - val_loss: 0.2462 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89605 to 0.90269, saving model to model_r.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2297 - accuracy: 0.9073 - val_loss: 0.2259 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90269 to 0.90665, saving model to model_r.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2221 - accuracy: 0.9079 - val_loss: 0.2203 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90665 to 0.90803, saving model to model_r.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2151 - accuracy: 0.9098 - val_loss: 0.2202 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90803\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2093 - accuracy: 0.9127 - val_loss: 0.2163 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90803\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2096 - accuracy: 0.9110 - val_loss: 0.2161 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90803\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2092 - accuracy: 0.9116 - val_loss: 0.2141 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90803\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2054 - accuracy: 0.9129 - val_loss: 0.2125 - val_accuracy: 0.9053\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90803\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2039 - accuracy: 0.9129 - val_loss: 0.2150 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90803\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9135 - val_loss: 0.2089 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90803\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9135 - val_loss: 0.2074 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90803\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2037 - accuracy: 0.9103 - val_loss: 0.2053 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90803\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9143 - val_loss: 0.2073 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90803\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9156 - val_loss: 0.2038 - val_accuracy: 0.9065\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90803\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9158 - val_loss: 0.2043 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90803\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1926 - accuracy: 0.9146 - val_loss: 0.2012 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90803\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1974 - accuracy: 0.9134 - val_loss: 0.2008 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90803\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9135 - val_loss: 0.2002 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90803\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1955 - accuracy: 0.9124 - val_loss: 0.2001 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90803\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9139 - val_loss: 0.1996 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.90803 to 0.90876, saving model to model_r.hdf5\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9121 - val_loss: 0.1997 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90876\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9141 - val_loss: 0.1992 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90876\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9154 - val_loss: 0.2010 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.90876 to 0.90925, saving model to model_r.hdf5\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9162 - val_loss: 0.2001 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90925\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1959 - accuracy: 0.9102 - val_loss: 0.2005 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.90925 to 0.90933, saving model to model_r.hdf5\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9155 - val_loss: 0.1987 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90933\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9130 - val_loss: 0.1991 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90933\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9137 - val_loss: 0.1974 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90933\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9154 - val_loss: 0.1976 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90933\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9120 - val_loss: 0.1991 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90933\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9158 - val_loss: 0.2000 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90933\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9145 - val_loss: 0.1978 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90933\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9142 - val_loss: 0.1967 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.90933 to 0.90981, saving model to model_r.hdf5\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9152 - val_loss: 0.1977 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90981\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9142 - val_loss: 0.1968 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90981\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9132 - val_loss: 0.1986 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.90981 to 0.91078, saving model to model_r.hdf5\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9149 - val_loss: 0.1973 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91078\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9137 - val_loss: 0.1963 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91078\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9126 - val_loss: 0.1967 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91078\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9138 - val_loss: 0.1991 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91078\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9163 - val_loss: 0.2005 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91078\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9150 - val_loss: 0.1981 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91078\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9153 - val_loss: 0.1963 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91078\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9171 - val_loss: 0.1977 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91078\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9130 - val_loss: 0.1960 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91078\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9151 - val_loss: 0.1961 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91078\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9153 - val_loss: 0.1961 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91078\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9182 - val_loss: 0.1963 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91078\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9146 - val_loss: 0.1954 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.91078 to 0.91135, saving model to model_r.hdf5\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9151 - val_loss: 0.1968 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91135\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9147 - val_loss: 0.1965 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91135\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9130 - val_loss: 0.1955 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91135\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9135 - val_loss: 0.1971 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91135\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9136 - val_loss: 0.1956 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91135\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9156 - val_loss: 0.1954 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91135\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9142 - val_loss: 0.1966 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91135\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9150 - val_loss: 0.1956 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91135\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9146 - val_loss: 0.1956 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91135\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9133 - val_loss: 0.1956 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91135\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9148 - val_loss: 0.1958 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91135\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9128 - val_loss: 0.1991 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91135\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9162 - val_loss: 0.1968 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91135\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9176 - val_loss: 0.1992 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91135\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1954 - accuracy: 0.9114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "E5KBS28gr3xc",
        "outputId": "b7f0754d-902c-4060-bdc1-1aa72973629d"
      },
      "source": [
        "column_names = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan','contact', 'month', 'day_of_week', 'mean', 'campaign', 'pdays','previous', 'poutcome', 'emp_var-rate', 'cons.price.idx','cons.conf.idx', 'euribor3m', 'nr.employed']\n",
        "plt.bar(column_names, accuracy_r[:20])\n",
        "plt.ylim(0.85,0.925)\n",
        "plt.xlabel('Features', fontsize=10)\n",
        "plt.ylabel('Accuracy', fontsize=10)\n",
        "plt.xticks(column_names[:20], fontsize=5, rotation=90)\n",
        "plt.title('Validation accuracy of Removing one feature at a time ')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKh4A6VAHLRN"
      },
      "source": [
        "**Removing Features Iteratively**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leRaxvC0srGJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qF_tIFTO8pC",
        "outputId": "ed4f5c41-ea9d-4c14-9216-e7753bd3ee87"
      },
      "source": [
        "accuracy_2 = [] \n",
        "t = dataset.shape\n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "for i in range(21):\n",
        "  print()\n",
        "  print(i)\n",
        "  print()\n",
        "  X  = dataset[:,i:-1]\n",
        "  Y = dataset[:,-1]\n",
        "  XVALID = X[:index]\n",
        "  YVALID = Y[:index]\n",
        "  XTRAIN = X[index:]\n",
        "  YTRAIN = Y[index:]\n",
        "  model_r1 = Sequential()\n",
        "  model_r1.add(Dense(8, input_dim = t[1]-i-1, activation='relu'))\n",
        "  model_r1.add(Dense(4,activation='relu'))\n",
        "  model_r1.add(Dense(1,activation='sigmoid'))\n",
        "  model_r1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "  callback_a = ModelCheckpoint(filepath = 'model_r1.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "  # The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "  callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=64, verbose=1)\n",
        "  history_r1 = model_r1.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64, batch_size = 30, callbacks = [callback_a,callback_b])\n",
        "  model_r1.load_weights('model_r1.hdf5')\n",
        "  p_r1 = model_r1.predict(XVALID)\n",
        "  acc_r1 = model_r1.evaluate(XVALID, YVALID)\n",
        "  accuracy_2.append(acc_r1[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9154 - val_loss: 0.1932 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91240\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9139 - val_loss: 0.1911 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91240\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9154 - val_loss: 0.1913 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91240\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9163 - val_loss: 0.1996 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91240\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9115 - val_loss: 0.1907 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91240\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9136 - val_loss: 0.1905 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91240\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9128 - val_loss: 0.1906 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91240\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1861 - accuracy: 0.9156 - val_loss: 0.1904 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91240\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9110 - val_loss: 0.1911 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91240\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1776 - accuracy: 0.9198 - val_loss: 0.1902 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91240\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9168 - val_loss: 0.1906 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91240\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9154 - val_loss: 0.1906 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91240\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1841 - accuracy: 0.9158 - val_loss: 0.1905 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91240\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9156 - val_loss: 0.1903 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91240\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9135 - val_loss: 0.1913 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91240\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9124\n",
            "\n",
            "2\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3792 - accuracy: 0.8871 - val_loss: 0.2916 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2787 - accuracy: 0.8880 - val_loss: 0.2625 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88423 to 0.89394, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2448 - accuracy: 0.9030 - val_loss: 0.2300 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89394 to 0.90544, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2184 - accuracy: 0.9120 - val_loss: 0.2151 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90544 to 0.90868, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2023 - accuracy: 0.9149 - val_loss: 0.2123 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90868\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2043 - accuracy: 0.9110 - val_loss: 0.2065 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90868\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2082 - accuracy: 0.9105 - val_loss: 0.2047 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90868 to 0.90933, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1978 - accuracy: 0.9159 - val_loss: 0.2031 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90933\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9136 - val_loss: 0.2062 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90933\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9140 - val_loss: 0.2017 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90933\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9169 - val_loss: 0.2014 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90933\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9136 - val_loss: 0.2013 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.90933 to 0.90941, saving model to model_r1.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9140 - val_loss: 0.2010 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90941\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9128 - val_loss: 0.2018 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90941\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9157 - val_loss: 0.2010 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90941\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9136 - val_loss: 0.2006 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.90941 to 0.90949, saving model to model_r1.hdf5\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9148 - val_loss: 0.2017 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90949\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9149 - val_loss: 0.2002 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.90949 to 0.90973, saving model to model_r1.hdf5\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9142 - val_loss: 0.2003 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90973\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9163 - val_loss: 0.2002 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90973\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9159 - val_loss: 0.2020 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90973\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9136 - val_loss: 0.2002 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90973\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9155 - val_loss: 0.1999 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90973\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1946 - accuracy: 0.9137 - val_loss: 0.1997 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.90973 to 0.90981, saving model to model_r1.hdf5\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1993 - accuracy: 0.9125 - val_loss: 0.2008 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90981\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9153 - val_loss: 0.2006 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90981\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9133 - val_loss: 0.1988 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90981\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1988 - accuracy: 0.9103 - val_loss: 0.1988 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90981\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9141 - val_loss: 0.1996 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90981\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9125 - val_loss: 0.1984 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.90981 to 0.91038, saving model to model_r1.hdf5\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9152 - val_loss: 0.1989 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91038\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9148 - val_loss: 0.2069 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91038\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9161 - val_loss: 0.1989 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91038\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9146 - val_loss: 0.1991 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91038\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9152 - val_loss: 0.1986 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91038\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9142 - val_loss: 0.1985 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91038\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1921 - accuracy: 0.9140 - val_loss: 0.1983 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91038\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9154 - val_loss: 0.1992 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91038\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9133 - val_loss: 0.1995 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91038\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9155 - val_loss: 0.1988 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91038\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1936 - accuracy: 0.9143 - val_loss: 0.1973 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91038\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9152 - val_loss: 0.1970 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91038\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9163 - val_loss: 0.1991 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91038\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9157 - val_loss: 0.1985 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91038\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9138 - val_loss: 0.1974 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91038\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9122 - val_loss: 0.1999 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91038\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9165 - val_loss: 0.1955 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.91038 to 0.91111, saving model to model_r1.hdf5\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9149 - val_loss: 0.1961 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91111\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1892 - accuracy: 0.9135 - val_loss: 0.2035 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91111\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9162 - val_loss: 0.1976 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91111\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9146 - val_loss: 0.1956 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91111\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9130 - val_loss: 0.1955 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91111\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9149 - val_loss: 0.1951 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91111\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9149 - val_loss: 0.1982 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91111\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1877 - accuracy: 0.9143 - val_loss: 0.2048 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91111\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9142 - val_loss: 0.1954 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91111\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9126 - val_loss: 0.1938 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91111\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9143 - val_loss: 0.1959 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91111\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9170 - val_loss: 0.1973 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91111\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9137 - val_loss: 0.1933 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91111\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9145 - val_loss: 0.1945 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91111\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9156 - val_loss: 0.1936 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91111\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1862 - accuracy: 0.9150 - val_loss: 0.1932 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91111\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9139 - val_loss: 0.1929 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91111\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9111\n",
            "\n",
            "3\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4562 - accuracy: 0.8547 - val_loss: 0.2891 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2805 - accuracy: 0.8871 - val_loss: 0.2629 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2515 - accuracy: 0.8896 - val_loss: 0.2365 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88423 to 0.90674, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2345 - accuracy: 0.9058 - val_loss: 0.2200 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90674 to 0.90819, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2162 - accuracy: 0.9100 - val_loss: 0.2133 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90819\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9168 - val_loss: 0.2086 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90819 to 0.90868, saving model to model_r1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2038 - accuracy: 0.9145 - val_loss: 0.2069 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90868 to 0.90892, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2017 - accuracy: 0.9164 - val_loss: 0.2062 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90892\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9157 - val_loss: 0.2071 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.90892 to 0.90916, saving model to model_r1.hdf5\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2014 - accuracy: 0.9134 - val_loss: 0.2047 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90916\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9150 - val_loss: 0.2085 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90916\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1981 - accuracy: 0.9133 - val_loss: 0.2047 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90916\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9175 - val_loss: 0.2088 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90916\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9108 - val_loss: 0.2036 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90916\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1974 - accuracy: 0.9125 - val_loss: 0.2040 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.90916 to 0.90941, saving model to model_r1.hdf5\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9115 - val_loss: 0.2039 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90941\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2031 - accuracy: 0.9113 - val_loss: 0.2026 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.90941 to 0.90965, saving model to model_r1.hdf5\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1958 - accuracy: 0.9122 - val_loss: 0.2027 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90965\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9148 - val_loss: 0.2089 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90965\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1979 - accuracy: 0.9123 - val_loss: 0.2044 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90965\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9125 - val_loss: 0.2032 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90965\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9107 - val_loss: 0.2015 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90965\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9119 - val_loss: 0.2009 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90965\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9142 - val_loss: 0.2026 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90965\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1949 - accuracy: 0.9120 - val_loss: 0.2008 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90965\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9145 - val_loss: 0.1996 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90965\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9178 - val_loss: 0.1993 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90965\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9124 - val_loss: 0.1998 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90965\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9177 - val_loss: 0.1988 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90965\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9152 - val_loss: 0.1991 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90965\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9111 - val_loss: 0.2019 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90965\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9136 - val_loss: 0.1985 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.90965 to 0.90973, saving model to model_r1.hdf5\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9131 - val_loss: 0.1991 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90973\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9126 - val_loss: 0.1975 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90973\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9116 - val_loss: 0.2012 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90973\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9151 - val_loss: 0.1984 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90973\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9147 - val_loss: 0.1982 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.90973 to 0.90997, saving model to model_r1.hdf5\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1902 - accuracy: 0.9150 - val_loss: 0.1968 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90997\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9137 - val_loss: 0.1976 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90997\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9142 - val_loss: 0.1981 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90997\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9148 - val_loss: 0.1988 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90997\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9115 - val_loss: 0.1964 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90997\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1911 - accuracy: 0.9141 - val_loss: 0.1969 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90997\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9148 - val_loss: 0.1981 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90997\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9118 - val_loss: 0.1966 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.90997 to 0.91022, saving model to model_r1.hdf5\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9145 - val_loss: 0.1980 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91022\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9175 - val_loss: 0.1970 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.91022 to 0.91070, saving model to model_r1.hdf5\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9138 - val_loss: 0.1977 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91070\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9147 - val_loss: 0.1960 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91070\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1928 - accuracy: 0.9157 - val_loss: 0.1961 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91070\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9168 - val_loss: 0.1964 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91070\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9162 - val_loss: 0.1978 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91070\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9151 - val_loss: 0.1955 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91070\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9124 - val_loss: 0.1964 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91070\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 1s 2ms/step - loss: 0.1907 - accuracy: 0.9143 - val_loss: 0.1964 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91070\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9140 - val_loss: 0.1951 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91070\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9161 - val_loss: 0.1959 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91070\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9131 - val_loss: 0.1999 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91070\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9145 - val_loss: 0.1946 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91070\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9150 - val_loss: 0.1964 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91070\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1849 - accuracy: 0.9161 - val_loss: 0.1947 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91070\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1844 - accuracy: 0.9182 - val_loss: 0.1967 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91070\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9133 - val_loss: 0.1970 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91070\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9171 - val_loss: 0.1951 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91070\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1970 - accuracy: 0.9107\n",
            "\n",
            "4\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3661 - accuracy: 0.8868 - val_loss: 0.2782 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89573, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2611 - accuracy: 0.9008 - val_loss: 0.2422 - val_accuracy: 0.9036\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89573 to 0.90358, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2302 - accuracy: 0.9080 - val_loss: 0.2231 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90358 to 0.90771, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2139 - accuracy: 0.9112 - val_loss: 0.2176 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90771 to 0.90803, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2133 - accuracy: 0.9081 - val_loss: 0.2136 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90803\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2083 - accuracy: 0.9128 - val_loss: 0.2112 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90803\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2033 - accuracy: 0.9136 - val_loss: 0.2113 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90803 to 0.90925, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9161 - val_loss: 0.2077 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90925\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2009 - accuracy: 0.9152 - val_loss: 0.2072 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90925\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2029 - accuracy: 0.9141 - val_loss: 0.2065 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90925\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1927 - accuracy: 0.9163 - val_loss: 0.2098 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90925\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2011 - accuracy: 0.9127 - val_loss: 0.2046 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90925\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9132 - val_loss: 0.2031 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.90925 to 0.91006, saving model to model_r1.hdf5\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9134 - val_loss: 0.2045 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.91006\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9113 - val_loss: 0.2017 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.91006\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1941 - accuracy: 0.9137 - val_loss: 0.2029 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91006\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9147 - val_loss: 0.2007 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.91006\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9143 - val_loss: 0.2006 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.91006 to 0.91038, saving model to model_r1.hdf5\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9121 - val_loss: 0.1994 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91038\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9178 - val_loss: 0.1985 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91038\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9161 - val_loss: 0.1984 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91038\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9142 - val_loss: 0.1974 - val_accuracy: 0.9112\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.91038 to 0.91119, saving model to model_r1.hdf5\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9148 - val_loss: 0.1970 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91119\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1972 - accuracy: 0.9124 - val_loss: 0.1978 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91119\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9188 - val_loss: 0.1993 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91119\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9156 - val_loss: 0.1952 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91119\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9158 - val_loss: 0.1960 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91119\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1916 - accuracy: 0.9147 - val_loss: 0.1955 - val_accuracy: 0.9119\n",
            "\n",
            "Epoch 00028: val_accuracy improved from 0.91119 to 0.91192, saving model to model_r1.hdf5\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9172 - val_loss: 0.1961 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91192\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1931 - accuracy: 0.9133 - val_loss: 0.1942 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91192\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9153 - val_loss: 0.1938 - val_accuracy: 0.9119\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91192\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9168 - val_loss: 0.1935 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91192\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1889 - accuracy: 0.9157 - val_loss: 0.1946 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91192\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9172 - val_loss: 0.1994 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91192\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9176 - val_loss: 0.1950 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91192\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9140 - val_loss: 0.1939 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91192\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9193 - val_loss: 0.1955 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91192\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9182 - val_loss: 0.1936 - val_accuracy: 0.9122\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.91192 to 0.91224, saving model to model_r1.hdf5\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1872 - accuracy: 0.9155 - val_loss: 0.1937 - val_accuracy: 0.9122\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91224\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9159 - val_loss: 0.1943 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91224\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9173 - val_loss: 0.1927 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91224\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9154 - val_loss: 0.1936 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91224\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9159 - val_loss: 0.1927 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91224\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9132 - val_loss: 0.1924 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91224\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9169 - val_loss: 0.1919 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91224\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9143 - val_loss: 0.1949 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91224\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9112 - val_loss: 0.1916 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91224\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9186 - val_loss: 0.1924 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91224\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9203 - val_loss: 0.1935 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91224\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1819 - accuracy: 0.9184 - val_loss: 0.1927 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91224\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9180 - val_loss: 0.1921 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91224\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9137 - val_loss: 0.1906 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91224\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9135 - val_loss: 0.1915 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91224\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9157 - val_loss: 0.1914 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91224\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9144 - val_loss: 0.1907 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91224\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9150 - val_loss: 0.1913 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91224\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9175 - val_loss: 0.1909 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91224\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1869 - accuracy: 0.9166 - val_loss: 0.1908 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91224\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1836 - accuracy: 0.9177 - val_loss: 0.1910 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91224\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9179 - val_loss: 0.1915 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91224\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9144 - val_loss: 0.1910 - val_accuracy: 0.9118\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91224\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9149 - val_loss: 0.1903 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91224\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9158 - val_loss: 0.1898 - val_accuracy: 0.9115\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91224\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9150 - val_loss: 0.1904 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91224\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1936 - accuracy: 0.9122\n",
            "\n",
            "5\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4232 - accuracy: 0.8192 - val_loss: 0.2786 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89718, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2514 - accuracy: 0.9063 - val_loss: 0.2503 - val_accuracy: 0.9046\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89718 to 0.90463, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2396 - accuracy: 0.9052 - val_loss: 0.2300 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90463 to 0.90617, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2211 - accuracy: 0.9083 - val_loss: 0.2228 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90617 to 0.90771, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2148 - accuracy: 0.9092 - val_loss: 0.2173 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90771\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2140 - accuracy: 0.9100 - val_loss: 0.2144 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90771 to 0.90811, saving model to model_r1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2032 - accuracy: 0.9146 - val_loss: 0.2131 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90811 to 0.90925, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2024 - accuracy: 0.9168 - val_loss: 0.2104 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90925 to 0.90981, saving model to model_r1.hdf5\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9137 - val_loss: 0.2081 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90981\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9152 - val_loss: 0.2071 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90981\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2000 - accuracy: 0.9118 - val_loss: 0.2049 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90981\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9152 - val_loss: 0.2040 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.90981 to 0.91014, saving model to model_r1.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9134 - val_loss: 0.2018 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91014\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2058 - accuracy: 0.9094 - val_loss: 0.2034 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.91014\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1984 - accuracy: 0.9109 - val_loss: 0.1997 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.91014\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9129 - val_loss: 0.1986 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91014\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9146 - val_loss: 0.1996 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.91014\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9138 - val_loss: 0.1952 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91014\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9119 - val_loss: 0.1942 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91014\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9119 - val_loss: 0.1950 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91014\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9122 - val_loss: 0.1950 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91014\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9131 - val_loss: 0.1957 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91014\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9116 - val_loss: 0.1954 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91014\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9159 - val_loss: 0.1936 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91014\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1906 - accuracy: 0.9113 - val_loss: 0.1950 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91014\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9122 - val_loss: 0.1939 - val_accuracy: 0.9056\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91014\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9149 - val_loss: 0.1926 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91014\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9110 - val_loss: 0.1921 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91014\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9109 - val_loss: 0.1910 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91014\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1847 - accuracy: 0.9150 - val_loss: 0.1911 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91014\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9138 - val_loss: 0.1926 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91014\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9153 - val_loss: 0.1930 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91014\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9144 - val_loss: 0.1909 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91014\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1822 - accuracy: 0.9184 - val_loss: 0.1956 - val_accuracy: 0.9043\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91014\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1868 - accuracy: 0.9148 - val_loss: 0.1909 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91014\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9155 - val_loss: 0.1908 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91014\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9168 - val_loss: 0.1919 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.91014 to 0.91038, saving model to model_r1.hdf5\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1830 - accuracy: 0.9161 - val_loss: 0.1903 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91038\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9149 - val_loss: 0.1928 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91038\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9138 - val_loss: 0.1899 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91038\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9129 - val_loss: 0.1904 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91038\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9166 - val_loss: 0.1896 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91038\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9151 - val_loss: 0.1913 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91038\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9136 - val_loss: 0.1897 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91038\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9150 - val_loss: 0.1909 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91038\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9156 - val_loss: 0.1911 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91038\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9140 - val_loss: 0.1915 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91038\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9145 - val_loss: 0.1926 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91038\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9147 - val_loss: 0.1916 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91038\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1790 - accuracy: 0.9212 - val_loss: 0.1903 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91038\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1843 - accuracy: 0.9173 - val_loss: 0.1893 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91038\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9184 - val_loss: 0.1907 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91038\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1818 - accuracy: 0.9192 - val_loss: 0.1902 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91038\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9120 - val_loss: 0.1902 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91038\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9156 - val_loss: 0.1892 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91038\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9160 - val_loss: 0.1891 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91038\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1848 - accuracy: 0.9169 - val_loss: 0.1887 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91038\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9133 - val_loss: 0.1894 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91038\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9176 - val_loss: 0.1914 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91038\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9135 - val_loss: 0.1886 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91038\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9178 - val_loss: 0.1882 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91038\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1854 - accuracy: 0.9165 - val_loss: 0.1898 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91038\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1820 - accuracy: 0.9154 - val_loss: 0.1894 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91038\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9136 - val_loss: 0.1910 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91038\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9104\n",
            "\n",
            "6\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.6158 - accuracy: 0.8932 - val_loss: 0.4485 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89783, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4129 - accuracy: 0.8980 - val_loss: 0.3391 - val_accuracy: 0.9018\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89783 to 0.90180, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3169 - accuracy: 0.9049 - val_loss: 0.2857 - val_accuracy: 0.9040\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90180 to 0.90398, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2728 - accuracy: 0.9059 - val_loss: 0.2563 - val_accuracy: 0.9056\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90398 to 0.90560, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2465 - accuracy: 0.9086 - val_loss: 0.2404 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90560 to 0.90868, saving model to model_r1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2260 - accuracy: 0.9145 - val_loss: 0.2304 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90868\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2228 - accuracy: 0.9106 - val_loss: 0.2245 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90868\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2119 - accuracy: 0.9120 - val_loss: 0.2216 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90868\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2135 - accuracy: 0.9104 - val_loss: 0.2175 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90868\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2150 - accuracy: 0.9112 - val_loss: 0.2155 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90868 to 0.90884, saving model to model_r1.hdf5\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2086 - accuracy: 0.9130 - val_loss: 0.2131 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90884\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2079 - accuracy: 0.9118 - val_loss: 0.2134 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90884\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9144 - val_loss: 0.2103 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90884\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9151 - val_loss: 0.2088 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90884 to 0.90892, saving model to model_r1.hdf5\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2090 - accuracy: 0.9097 - val_loss: 0.2073 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.90892 to 0.90925, saving model to model_r1.hdf5\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9126 - val_loss: 0.2072 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90925\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9168 - val_loss: 0.2048 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.90925 to 0.90941, saving model to model_r1.hdf5\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1954 - accuracy: 0.9153 - val_loss: 0.2047 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90941\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1998 - accuracy: 0.9117 - val_loss: 0.2038 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90941\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9124 - val_loss: 0.2032 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90941\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9121 - val_loss: 0.2053 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.90941 to 0.91014, saving model to model_r1.hdf5\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9145 - val_loss: 0.2014 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.91014 to 0.91070, saving model to model_r1.hdf5\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1980 - accuracy: 0.9127 - val_loss: 0.2010 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91070\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1986 - accuracy: 0.9112 - val_loss: 0.2007 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91070\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1974 - accuracy: 0.9143 - val_loss: 0.1998 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91070\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9120 - val_loss: 0.2015 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91070\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9121 - val_loss: 0.1995 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91070\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9159 - val_loss: 0.2001 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91070\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9161 - val_loss: 0.1986 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91070\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9162 - val_loss: 0.1979 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91070\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9135 - val_loss: 0.1989 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91070\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9131 - val_loss: 0.1989 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91070\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9138 - val_loss: 0.1974 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91070\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9113 - val_loss: 0.1973 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91070\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9124 - val_loss: 0.1972 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91070\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9132 - val_loss: 0.1969 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91070\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9126 - val_loss: 0.2006 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91070\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9156 - val_loss: 0.1960 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91070\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1863 - accuracy: 0.9163 - val_loss: 0.1962 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91070\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1929 - accuracy: 0.9134 - val_loss: 0.1949 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91070\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9156 - val_loss: 0.1958 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91070\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9134 - val_loss: 0.1947 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91070\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9137 - val_loss: 0.1958 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91070\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1919 - accuracy: 0.9138 - val_loss: 0.1972 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91070\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9158 - val_loss: 0.1946 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91070\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1884 - accuracy: 0.9109 - val_loss: 0.1946 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91070\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9143 - val_loss: 0.1946 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91070\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1918 - accuracy: 0.9131 - val_loss: 0.1943 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91070\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9155 - val_loss: 0.1953 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91070\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9132 - val_loss: 0.1934 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91070\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9129 - val_loss: 0.1928 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91070\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9121 - val_loss: 0.1929 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91070\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9103 - val_loss: 0.1944 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91070\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9128 - val_loss: 0.1936 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91070\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1874 - accuracy: 0.9140 - val_loss: 0.1932 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91070\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9147 - val_loss: 0.1937 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91070\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9144 - val_loss: 0.1927 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91070\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9126 - val_loss: 0.1923 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91070\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1871 - accuracy: 0.9137 - val_loss: 0.1928 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91070\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9130 - val_loss: 0.1923 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91070\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1914 - accuracy: 0.9134 - val_loss: 0.1924 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91070\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9146 - val_loss: 0.1917 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91070\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1894 - accuracy: 0.9129 - val_loss: 0.1923 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91070\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1856 - accuracy: 0.9147 - val_loss: 0.1933 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91070\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2014 - accuracy: 0.9107\n",
            "\n",
            "7\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.6189 - accuracy: 0.8309 - val_loss: 0.4522 - val_accuracy: 0.8907\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89071, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4165 - accuracy: 0.8976 - val_loss: 0.3429 - val_accuracy: 0.8992\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89071 to 0.89921, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3226 - accuracy: 0.9004 - val_loss: 0.2877 - val_accuracy: 0.9038\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89921 to 0.90382, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2791 - accuracy: 0.9017 - val_loss: 0.2573 - val_accuracy: 0.9053\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90382 to 0.90528, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2470 - accuracy: 0.9096 - val_loss: 0.2392 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90528 to 0.90665, saving model to model_r1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2349 - accuracy: 0.9087 - val_loss: 0.2296 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90665 to 0.90795, saving model to model_r1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2255 - accuracy: 0.9097 - val_loss: 0.2239 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90795 to 0.90892, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2184 - accuracy: 0.9123 - val_loss: 0.2199 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90892 to 0.90933, saving model to model_r1.hdf5\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2188 - accuracy: 0.9095 - val_loss: 0.2165 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90933\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2139 - accuracy: 0.9105 - val_loss: 0.2146 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90933\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2114 - accuracy: 0.9111 - val_loss: 0.2129 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90933\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2101 - accuracy: 0.9106 - val_loss: 0.2127 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90933\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2097 - accuracy: 0.9099 - val_loss: 0.2109 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90933\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2087 - accuracy: 0.9101 - val_loss: 0.2088 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90933 to 0.90949, saving model to model_r1.hdf5\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2081 - accuracy: 0.9105 - val_loss: 0.2076 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90949\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2025 - accuracy: 0.9127 - val_loss: 0.2072 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.90949 to 0.90957, saving model to model_r1.hdf5\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2010 - accuracy: 0.9137 - val_loss: 0.2058 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.90957 to 0.91022, saving model to model_r1.hdf5\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2087 - accuracy: 0.9089 - val_loss: 0.2051 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.91022 to 0.91046, saving model to model_r1.hdf5\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2019 - accuracy: 0.9111 - val_loss: 0.2046 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91046\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9141 - val_loss: 0.2053 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91046\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1888 - accuracy: 0.9191 - val_loss: 0.2087 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91046\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9146 - val_loss: 0.2023 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91046\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1920 - accuracy: 0.9146 - val_loss: 0.2029 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91046\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1976 - accuracy: 0.9123 - val_loss: 0.2012 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.91046 to 0.91054, saving model to model_r1.hdf5\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9141 - val_loss: 0.2005 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.91054 to 0.91095, saving model to model_r1.hdf5\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9136 - val_loss: 0.2002 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.91095 to 0.91103, saving model to model_r1.hdf5\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9156 - val_loss: 0.2002 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91103\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9132 - val_loss: 0.1999 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91103\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9155 - val_loss: 0.1987 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91103\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1956 - accuracy: 0.9133 - val_loss: 0.1991 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91103\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1928 - accuracy: 0.9126 - val_loss: 0.1980 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91103\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9152 - val_loss: 0.1988 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91103\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1960 - accuracy: 0.9088 - val_loss: 0.1982 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91103\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1937 - accuracy: 0.9137 - val_loss: 0.1980 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91103\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1898 - accuracy: 0.9145 - val_loss: 0.1968 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91103\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9125 - val_loss: 0.1965 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91103\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9149 - val_loss: 0.1957 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91103\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9158 - val_loss: 0.1955 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91103\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9122 - val_loss: 0.1943 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91103\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9149 - val_loss: 0.1956 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91103\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1885 - accuracy: 0.9130 - val_loss: 0.1943 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91103\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1890 - accuracy: 0.9144 - val_loss: 0.1947 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91103\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1897 - accuracy: 0.9155 - val_loss: 0.1933 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91103\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1915 - accuracy: 0.9115 - val_loss: 0.1932 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91103\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9104 - val_loss: 0.1945 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91103\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1879 - accuracy: 0.9127 - val_loss: 0.1927 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91103\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1904 - accuracy: 0.9137 - val_loss: 0.1937 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91103\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9154 - val_loss: 0.1926 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91103\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1860 - accuracy: 0.9135 - val_loss: 0.1921 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91103\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1867 - accuracy: 0.9135 - val_loss: 0.1924 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91103\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9138 - val_loss: 0.1942 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91103\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1864 - accuracy: 0.9144 - val_loss: 0.1935 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91103\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9106 - val_loss: 0.1915 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91103\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1881 - accuracy: 0.9135 - val_loss: 0.1912 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91103\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1837 - accuracy: 0.9151 - val_loss: 0.1932 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91103\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1932 - accuracy: 0.9077 - val_loss: 0.1912 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91103\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9132 - val_loss: 0.1913 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91103\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1910 - accuracy: 0.9110 - val_loss: 0.1905 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91103\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1858 - accuracy: 0.9134 - val_loss: 0.1905 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91103\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1859 - accuracy: 0.9131 - val_loss: 0.1905 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91103\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9111 - val_loss: 0.1902 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91103\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9102 - val_loss: 0.1911 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91103\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1834 - accuracy: 0.9134 - val_loss: 0.1902 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91103\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1825 - accuracy: 0.9138 - val_loss: 0.1918 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91103\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2002 - accuracy: 0.9110\n",
            "\n",
            "8\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3289 - accuracy: 0.8905 - val_loss: 0.2741 - val_accuracy: 0.8981\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89807, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2613 - accuracy: 0.8995 - val_loss: 0.2421 - val_accuracy: 0.9038\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89807 to 0.90382, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2312 - accuracy: 0.9053 - val_loss: 0.2300 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90382 to 0.90625, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2195 - accuracy: 0.9092 - val_loss: 0.2238 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.90625\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2151 - accuracy: 0.9127 - val_loss: 0.2220 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.90625\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2151 - accuracy: 0.9112 - val_loss: 0.2227 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90625\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2113 - accuracy: 0.9116 - val_loss: 0.2198 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90625\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2178 - accuracy: 0.9109 - val_loss: 0.2190 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90625\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2161 - accuracy: 0.9112 - val_loss: 0.2194 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90625\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2104 - accuracy: 0.9147 - val_loss: 0.2171 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90625 to 0.90811, saving model to model_r1.hdf5\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2108 - accuracy: 0.9118 - val_loss: 0.2191 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90811\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2079 - accuracy: 0.9120 - val_loss: 0.2128 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90811\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2100 - accuracy: 0.9113 - val_loss: 0.2125 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90811\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2074 - accuracy: 0.9086 - val_loss: 0.2103 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90811 to 0.90835, saving model to model_r1.hdf5\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2090 - accuracy: 0.9105 - val_loss: 0.2082 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90835\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2058 - accuracy: 0.9096 - val_loss: 0.2070 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90835\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9115 - val_loss: 0.2074 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.90835 to 0.90973, saving model to model_r1.hdf5\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1996 - accuracy: 0.9133 - val_loss: 0.2049 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90973\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9136 - val_loss: 0.2053 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90973\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2049 - accuracy: 0.9102 - val_loss: 0.2054 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90973\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9130 - val_loss: 0.2046 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90973\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9139 - val_loss: 0.2026 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90973\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1990 - accuracy: 0.9113 - val_loss: 0.2023 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.90973 to 0.91054, saving model to model_r1.hdf5\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9107 - val_loss: 0.2021 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91054\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9134 - val_loss: 0.2027 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91054\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1946 - accuracy: 0.9136 - val_loss: 0.2022 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91054\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9124 - val_loss: 0.2000 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91054\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1951 - accuracy: 0.9134 - val_loss: 0.1998 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91054\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9106 - val_loss: 0.1998 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91054\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9134 - val_loss: 0.2010 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91054\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9096 - val_loss: 0.1994 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91054\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1964 - accuracy: 0.9125 - val_loss: 0.2004 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91054\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1934 - accuracy: 0.9154 - val_loss: 0.2010 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91054\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2004 - accuracy: 0.9093 - val_loss: 0.2051 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91054\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1917 - accuracy: 0.9142 - val_loss: 0.1994 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91054\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9125 - val_loss: 0.2024 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91054\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9139 - val_loss: 0.1988 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91054\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1940 - accuracy: 0.9117 - val_loss: 0.1978 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91054\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9144 - val_loss: 0.2005 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91054\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9121 - val_loss: 0.1990 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91054\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9119 - val_loss: 0.1968 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91054\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1963 - accuracy: 0.9115 - val_loss: 0.1956 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91054\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1909 - accuracy: 0.9138 - val_loss: 0.1958 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91054\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9122 - val_loss: 0.1936 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91054\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9134 - val_loss: 0.1933 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91054\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1873 - accuracy: 0.9137 - val_loss: 0.1928 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91054\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9129 - val_loss: 0.1937 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91054\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1829 - accuracy: 0.9175 - val_loss: 0.1941 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91054\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1838 - accuracy: 0.9145 - val_loss: 0.1909 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91054\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9113 - val_loss: 0.1905 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91054\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1840 - accuracy: 0.9129 - val_loss: 0.1902 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91054\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1842 - accuracy: 0.9145 - val_loss: 0.1892 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91054\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1870 - accuracy: 0.9146 - val_loss: 0.1893 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91054\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1846 - accuracy: 0.9152 - val_loss: 0.1890 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91054\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1883 - accuracy: 0.9124 - val_loss: 0.1905 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91054\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1811 - accuracy: 0.9146 - val_loss: 0.1888 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91054\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1805 - accuracy: 0.9169 - val_loss: 0.1893 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00057: val_accuracy improved from 0.91054 to 0.91103, saving model to model_r1.hdf5\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9116 - val_loss: 0.1886 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91103\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1816 - accuracy: 0.9173 - val_loss: 0.1891 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91103\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1815 - accuracy: 0.9158 - val_loss: 0.1893 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91103\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9123 - val_loss: 0.1887 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91103\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1826 - accuracy: 0.9166 - val_loss: 0.1894 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00062: val_accuracy improved from 0.91103 to 0.91135, saving model to model_r1.hdf5\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1855 - accuracy: 0.9118 - val_loss: 0.1893 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91135\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1886 - accuracy: 0.9126 - val_loss: 0.1881 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91135\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1894 - accuracy: 0.9114\n",
            "\n",
            "9\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.5905 - accuracy: 0.8542 - val_loss: 0.2831 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89767, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2749 - accuracy: 0.8979 - val_loss: 0.2545 - val_accuracy: 0.9008\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89767 to 0.90083, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2423 - accuracy: 0.9045 - val_loss: 0.2363 - val_accuracy: 0.9041\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90083 to 0.90406, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2289 - accuracy: 0.9071 - val_loss: 0.2296 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90406 to 0.90609, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2276 - accuracy: 0.9081 - val_loss: 0.2279 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90609 to 0.90617, saving model to model_r1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2162 - accuracy: 0.9099 - val_loss: 0.2275 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90617 to 0.90690, saving model to model_r1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2250 - accuracy: 0.9083 - val_loss: 0.2265 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90690 to 0.90714, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2151 - accuracy: 0.9122 - val_loss: 0.2268 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90714 to 0.90746, saving model to model_r1.hdf5\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2190 - accuracy: 0.9107 - val_loss: 0.2255 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90746\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2147 - accuracy: 0.9128 - val_loss: 0.2269 - val_accuracy: 0.9051\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90746\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2141 - accuracy: 0.9132 - val_loss: 0.2253 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90746\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2134 - accuracy: 0.9130 - val_loss: 0.2248 - val_accuracy: 0.9060\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90746\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2185 - accuracy: 0.9110 - val_loss: 0.2259 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90746\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2197 - accuracy: 0.9113 - val_loss: 0.2250 - val_accuracy: 0.9054\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90746\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2160 - accuracy: 0.9103 - val_loss: 0.2244 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90746\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2171 - accuracy: 0.9118 - val_loss: 0.2249 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90746\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2172 - accuracy: 0.9097 - val_loss: 0.2240 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90746\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2178 - accuracy: 0.9123 - val_loss: 0.2231 - val_accuracy: 0.9049\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90746\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2140 - accuracy: 0.9113 - val_loss: 0.2215 - val_accuracy: 0.9056\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90746\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2107 - accuracy: 0.9129 - val_loss: 0.2198 - val_accuracy: 0.9049\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90746\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2093 - accuracy: 0.9132 - val_loss: 0.2180 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90746\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2113 - accuracy: 0.9096 - val_loss: 0.2172 - val_accuracy: 0.9049\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90746\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2131 - accuracy: 0.9082 - val_loss: 0.2160 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90746\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2079 - accuracy: 0.9124 - val_loss: 0.2137 - val_accuracy: 0.9050\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90746\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2018 - accuracy: 0.9137 - val_loss: 0.2126 - val_accuracy: 0.9052\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90746\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2088 - accuracy: 0.9088 - val_loss: 0.2118 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90746\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2045 - accuracy: 0.9111 - val_loss: 0.2096 - val_accuracy: 0.9045\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90746\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1985 - accuracy: 0.9143 - val_loss: 0.2091 - val_accuracy: 0.9061\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90746\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1948 - accuracy: 0.9149 - val_loss: 0.2081 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90746\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2021 - accuracy: 0.9122 - val_loss: 0.2080 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90746\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9096 - val_loss: 0.2080 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90746\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2023 - accuracy: 0.9109 - val_loss: 0.2070 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.90746\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9121 - val_loss: 0.2075 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90746\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1977 - accuracy: 0.9131 - val_loss: 0.2093 - val_accuracy: 0.9049\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90746\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9127 - val_loss: 0.2061 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90746\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9116 - val_loss: 0.2058 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90746\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9124 - val_loss: 0.2051 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90746\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2031 - accuracy: 0.9108 - val_loss: 0.2064 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.90746 to 0.90795, saving model to model_r1.hdf5\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2028 - accuracy: 0.9067 - val_loss: 0.2085 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90795\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1997 - accuracy: 0.9105 - val_loss: 0.2060 - val_accuracy: 0.9056\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90795\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1992 - accuracy: 0.9125 - val_loss: 0.2080 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90795\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9163 - val_loss: 0.2044 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90795\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2023 - accuracy: 0.9119 - val_loss: 0.2036 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90795\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2040 - accuracy: 0.9090 - val_loss: 0.2206 - val_accuracy: 0.9035\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90795\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2005 - accuracy: 0.9114 - val_loss: 0.2034 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.90795 to 0.90835, saving model to model_r1.hdf5\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2005 - accuracy: 0.9123 - val_loss: 0.2104 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90835\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9121 - val_loss: 0.2149 - val_accuracy: 0.9036\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90835\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9103 - val_loss: 0.2063 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90835\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9116 - val_loss: 0.2055 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90835\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2006 - accuracy: 0.9119 - val_loss: 0.2046 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.90835\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1966 - accuracy: 0.9128 - val_loss: 0.2026 - val_accuracy: 0.9073\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.90835\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1953 - accuracy: 0.9143 - val_loss: 0.2033 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00052: val_accuracy improved from 0.90835 to 0.90868, saving model to model_r1.hdf5\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2034 - accuracy: 0.9085 - val_loss: 0.2018 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.90868 to 0.90949, saving model to model_r1.hdf5\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1969 - accuracy: 0.9127 - val_loss: 0.2035 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90949\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2022 - accuracy: 0.9101 - val_loss: 0.2030 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90949\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2007 - accuracy: 0.9134 - val_loss: 0.2027 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90949\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1994 - accuracy: 0.9118 - val_loss: 0.2028 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90949\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1995 - accuracy: 0.9109 - val_loss: 0.2015 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00058: val_accuracy improved from 0.90949 to 0.91006, saving model to model_r1.hdf5\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9091 - val_loss: 0.2008 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91006\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1999 - accuracy: 0.9129 - val_loss: 0.2027 - val_accuracy: 0.9070\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91006\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1997 - accuracy: 0.9128 - val_loss: 0.1999 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91006\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1961 - accuracy: 0.9135 - val_loss: 0.2038 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91006\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2009 - accuracy: 0.9111 - val_loss: 0.2000 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91006\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2002 - accuracy: 0.9102 - val_loss: 0.1996 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91006\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2015 - accuracy: 0.9101\n",
            "\n",
            "10\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3617 - accuracy: 0.8897 - val_loss: 0.2892 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2799 - accuracy: 0.8880 - val_loss: 0.2690 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2506 - accuracy: 0.8942 - val_loss: 0.2432 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88423 to 0.90479, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2273 - accuracy: 0.9090 - val_loss: 0.2230 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90479 to 0.90746, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2162 - accuracy: 0.9094 - val_loss: 0.2129 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90746 to 0.90779, saving model to model_r1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2095 - accuracy: 0.9119 - val_loss: 0.2094 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.90779 to 0.90787, saving model to model_r1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2048 - accuracy: 0.9115 - val_loss: 0.2067 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90787 to 0.90892, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2001 - accuracy: 0.9144 - val_loss: 0.2056 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90892\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2035 - accuracy: 0.9101 - val_loss: 0.2055 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90892\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2005 - accuracy: 0.9116 - val_loss: 0.2044 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90892\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1991 - accuracy: 0.9115 - val_loss: 0.2042 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90892\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1971 - accuracy: 0.9126 - val_loss: 0.2040 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90892\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2013 - accuracy: 0.9111 - val_loss: 0.2034 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.90892 to 0.90925, saving model to model_r1.hdf5\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9151 - val_loss: 0.2030 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90925\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1975 - accuracy: 0.9134 - val_loss: 0.2027 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90925\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9160 - val_loss: 0.2028 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90925\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1942 - accuracy: 0.9134 - val_loss: 0.2032 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90925\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1957 - accuracy: 0.9132 - val_loss: 0.2016 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90925\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1965 - accuracy: 0.9136 - val_loss: 0.2035 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90925\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1967 - accuracy: 0.9154 - val_loss: 0.2003 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90925\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1950 - accuracy: 0.9134 - val_loss: 0.2006 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90925\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9156 - val_loss: 0.2019 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90925\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9145 - val_loss: 0.1997 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.90925\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9138 - val_loss: 0.1993 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.90925\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1939 - accuracy: 0.9155 - val_loss: 0.1990 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.90925\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1945 - accuracy: 0.9139 - val_loss: 0.1987 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.90925\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1987 - accuracy: 0.9122 - val_loss: 0.1995 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.90925\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1983 - accuracy: 0.9117 - val_loss: 0.1985 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.90925\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1924 - accuracy: 0.9149 - val_loss: 0.1980 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.90925\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1875 - accuracy: 0.9190 - val_loss: 0.2006 - val_accuracy: 0.9077\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.90925\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9148 - val_loss: 0.1980 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.90925\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1878 - accuracy: 0.9177 - val_loss: 0.1971 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.90925 to 0.90957, saving model to model_r1.hdf5\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9122 - val_loss: 0.1977 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.90957\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1933 - accuracy: 0.9140 - val_loss: 0.1966 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.90957\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1887 - accuracy: 0.9168 - val_loss: 0.1987 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.90957\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1944 - accuracy: 0.9132 - val_loss: 0.1964 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.90957\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9155 - val_loss: 0.1962 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.90957\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1923 - accuracy: 0.9153 - val_loss: 0.1961 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.90957\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1946 - accuracy: 0.9133 - val_loss: 0.1957 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.90957\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1943 - accuracy: 0.9138 - val_loss: 0.1973 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.90957\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1925 - accuracy: 0.9142 - val_loss: 0.1959 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.90957\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1876 - accuracy: 0.9184 - val_loss: 0.1959 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.90957\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1974 - accuracy: 0.9130 - val_loss: 0.1955 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.90957\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1891 - accuracy: 0.9142 - val_loss: 0.1955 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.90957\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1913 - accuracy: 0.9155 - val_loss: 0.1960 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.90957\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1865 - accuracy: 0.9152 - val_loss: 0.1949 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.90957\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1922 - accuracy: 0.9145 - val_loss: 0.1948 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.90957\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1952 - accuracy: 0.9138 - val_loss: 0.1953 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.90957\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1908 - accuracy: 0.9158 - val_loss: 0.1947 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.90957\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9167 - val_loss: 0.1942 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.90957 to 0.90973, saving model to model_r1.hdf5\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1899 - accuracy: 0.9156 - val_loss: 0.1946 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00051: val_accuracy improved from 0.90973 to 0.90989, saving model to model_r1.hdf5\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1935 - accuracy: 0.9134 - val_loss: 0.1944 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.90989\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1866 - accuracy: 0.9163 - val_loss: 0.1947 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00053: val_accuracy improved from 0.90989 to 0.90997, saving model to model_r1.hdf5\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9163 - val_loss: 0.1938 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.90997\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1930 - accuracy: 0.9129 - val_loss: 0.1941 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.90997\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9139 - val_loss: 0.1946 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.90997\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1893 - accuracy: 0.9140 - val_loss: 0.1940 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90997\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1880 - accuracy: 0.9132 - val_loss: 0.1944 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90997\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1853 - accuracy: 0.9171 - val_loss: 0.1948 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00059: val_accuracy improved from 0.90997 to 0.91006, saving model to model_r1.hdf5\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1962 - accuracy: 0.9099 - val_loss: 0.1969 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91006\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1905 - accuracy: 0.9146 - val_loss: 0.1939 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91006\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1912 - accuracy: 0.9149 - val_loss: 0.1933 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91006\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1901 - accuracy: 0.9149 - val_loss: 0.1939 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91006\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.1903 - accuracy: 0.9146 - val_loss: 0.1960 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91006\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1948 - accuracy: 0.9101\n",
            "\n",
            "11\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4341 - accuracy: 0.7976 - val_loss: 0.3102 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2978 - accuracy: 0.8910 - val_loss: 0.3017 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2937 - accuracy: 0.8889 - val_loss: 0.2981 - val_accuracy: 0.8883\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88423 to 0.88828, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2939 - accuracy: 0.8928 - val_loss: 0.2960 - val_accuracy: 0.8952\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.88828 to 0.89524, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2858 - accuracy: 0.9002 - val_loss: 0.2948 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.89524 to 0.89605, saving model to model_r1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2895 - accuracy: 0.9000 - val_loss: 0.2944 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.89605 to 0.89702, saving model to model_r1.hdf5\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.9011 - val_loss: 0.2935 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.89702\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2854 - accuracy: 0.9002 - val_loss: 0.2931 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.89702\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2897 - accuracy: 0.8990 - val_loss: 0.2928 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.89702 to 0.89743, saving model to model_r1.hdf5\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2866 - accuracy: 0.8999 - val_loss: 0.2931 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.89743\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2821 - accuracy: 0.9007 - val_loss: 0.2923 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.89743 to 0.89767, saving model to model_r1.hdf5\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2822 - accuracy: 0.9014 - val_loss: 0.2921 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.89767\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2830 - accuracy: 0.9019 - val_loss: 0.2922 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.89767 to 0.89799, saving model to model_r1.hdf5\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8999 - val_loss: 0.2926 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89799\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2800 - accuracy: 0.9002 - val_loss: 0.2925 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.89799\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2808 - accuracy: 0.9014 - val_loss: 0.2933 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.89799\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.9012 - val_loss: 0.2930 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.89799\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2825 - accuracy: 0.9015 - val_loss: 0.2916 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.89799\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2824 - accuracy: 0.8998 - val_loss: 0.2916 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89799\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2820 - accuracy: 0.9019 - val_loss: 0.2916 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89799\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.9008 - val_loss: 0.2915 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89799\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2884 - accuracy: 0.8991 - val_loss: 0.2913 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89799\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2835 - accuracy: 0.9017 - val_loss: 0.2916 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89799\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2765 - accuracy: 0.9036 - val_loss: 0.2916 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89799\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2857 - accuracy: 0.8985 - val_loss: 0.2912 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89799\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2844 - accuracy: 0.9013 - val_loss: 0.2916 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89799\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2909 - accuracy: 0.8984 - val_loss: 0.2912 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.89799\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2910 - accuracy: 0.8971 - val_loss: 0.2926 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.89799\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2814 - accuracy: 0.9025 - val_loss: 0.2917 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89799\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2880 - accuracy: 0.8980 - val_loss: 0.2913 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89799\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.8982 - val_loss: 0.2920 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89799\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2858 - accuracy: 0.9005 - val_loss: 0.2924 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89799\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2870 - accuracy: 0.8999 - val_loss: 0.2912 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89799\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2852 - accuracy: 0.8999 - val_loss: 0.2911 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89799\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2850 - accuracy: 0.8999 - val_loss: 0.2913 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89799\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2846 - accuracy: 0.8993 - val_loss: 0.2911 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89799\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8985 - val_loss: 0.2912 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89799\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2814 - accuracy: 0.9005 - val_loss: 0.2911 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89799\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2876 - accuracy: 0.8971 - val_loss: 0.2918 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89799\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2906 - accuracy: 0.8974 - val_loss: 0.2912 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89799\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2892 - accuracy: 0.8987 - val_loss: 0.2912 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89799\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2802 - accuracy: 0.9031 - val_loss: 0.2910 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89799\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2784 - accuracy: 0.9031 - val_loss: 0.2913 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89799\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8992 - val_loss: 0.2909 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89799\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2726 - accuracy: 0.9048 - val_loss: 0.2912 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89799\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2846 - accuracy: 0.9008 - val_loss: 0.2914 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89799\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2740 - accuracy: 0.9046 - val_loss: 0.2912 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89799\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2909 - accuracy: 0.8999 - val_loss: 0.2909 - val_accuracy: 0.8971\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89799\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.8999 - val_loss: 0.2909 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89799\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2903 - accuracy: 0.8969 - val_loss: 0.2911 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89799\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2797 - accuracy: 0.9021 - val_loss: 0.2909 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89799\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2887 - accuracy: 0.8980 - val_loss: 0.2906 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89799\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2859 - accuracy: 0.9004 - val_loss: 0.2910 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89799\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2851 - accuracy: 0.8997 - val_loss: 0.2911 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89799\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2886 - accuracy: 0.8982 - val_loss: 0.2907 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89799\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2906 - accuracy: 0.8991 - val_loss: 0.2904 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89799\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2863 - accuracy: 0.9002 - val_loss: 0.2905 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89799\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2837 - accuracy: 0.8999 - val_loss: 0.2904 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89799\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2836 - accuracy: 0.8998 - val_loss: 0.2909 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89799\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8999 - val_loss: 0.2900 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89799\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2813 - accuracy: 0.9013 - val_loss: 0.2897 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89799\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.9001 - val_loss: 0.2897 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89799\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2845 - accuracy: 0.9000 - val_loss: 0.2914 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89799\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2794 - accuracy: 0.9025 - val_loss: 0.2899 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89799\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2922 - accuracy: 0.8980\n",
            "\n",
            "12\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.6653 - accuracy: 0.6070 - val_loss: 0.3048 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88407, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2966 - accuracy: 0.8919 - val_loss: 0.2958 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88407 to 0.89726, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2889 - accuracy: 0.8988 - val_loss: 0.2937 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.89726\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8982 - val_loss: 0.2929 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.89726 to 0.89775, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2892 - accuracy: 0.8972 - val_loss: 0.2922 - val_accuracy: 0.8979\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.89775 to 0.89791, saving model to model_r1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2940 - accuracy: 0.8960 - val_loss: 0.2916 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.89791\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2829 - accuracy: 0.9001 - val_loss: 0.2912 - val_accuracy: 0.8959\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.89791\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2845 - accuracy: 0.8980 - val_loss: 0.2906 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.89791\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.8988 - val_loss: 0.2903 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.89791\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2812 - accuracy: 0.8992 - val_loss: 0.2900 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.89791\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2762 - accuracy: 0.9028 - val_loss: 0.2898 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.89791\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2824 - accuracy: 0.8986 - val_loss: 0.2900 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.89791 to 0.89799, saving model to model_r1.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2760 - accuracy: 0.9005 - val_loss: 0.2893 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.89799\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2829 - accuracy: 0.8980 - val_loss: 0.2896 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89799\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2867 - accuracy: 0.8962 - val_loss: 0.2891 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.89799\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2810 - accuracy: 0.8999 - val_loss: 0.2889 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.89799\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2800 - accuracy: 0.8989 - val_loss: 0.2890 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.89799\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2766 - accuracy: 0.9003 - val_loss: 0.2893 - val_accuracy: 0.8931\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.89799\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2813 - accuracy: 0.8988 - val_loss: 0.2885 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89799\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2789 - accuracy: 0.9006 - val_loss: 0.2887 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89799\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2837 - accuracy: 0.8991 - val_loss: 0.2892 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89799\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8973 - val_loss: 0.2884 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89799\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2736 - accuracy: 0.9025 - val_loss: 0.2893 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89799\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2810 - accuracy: 0.9002 - val_loss: 0.2888 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89799\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2776 - accuracy: 0.9006 - val_loss: 0.2882 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89799\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2784 - accuracy: 0.9016 - val_loss: 0.2879 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89799\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2825 - accuracy: 0.8995 - val_loss: 0.2880 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.89799\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2809 - accuracy: 0.8986 - val_loss: 0.2880 - val_accuracy: 0.8958\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.89799\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2847 - accuracy: 0.8973 - val_loss: 0.2880 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89799\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2870 - accuracy: 0.8976 - val_loss: 0.2878 - val_accuracy: 0.8951\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89799\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2790 - accuracy: 0.8999 - val_loss: 0.2877 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89799\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2878 - accuracy: 0.8989 - val_loss: 0.2877 - val_accuracy: 0.8949\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89799\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2769 - accuracy: 0.8999 - val_loss: 0.2896 - val_accuracy: 0.8931\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89799\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2839 - accuracy: 0.8971 - val_loss: 0.2880 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89799\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2752 - accuracy: 0.9016 - val_loss: 0.2875 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89799\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2769 - accuracy: 0.9003 - val_loss: 0.2880 - val_accuracy: 0.8947\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89799\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2772 - accuracy: 0.9019 - val_loss: 0.2880 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89799\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2812 - accuracy: 0.8979 - val_loss: 0.2871 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89799\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2798 - accuracy: 0.9007 - val_loss: 0.2877 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89799\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2801 - accuracy: 0.9002 - val_loss: 0.2890 - val_accuracy: 0.8953\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89799\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8981 - val_loss: 0.2874 - val_accuracy: 0.8960\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89799\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.8978 - val_loss: 0.2879 - val_accuracy: 0.8944\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89799\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2807 - accuracy: 0.8983 - val_loss: 0.2873 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89799\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2806 - accuracy: 0.8994 - val_loss: 0.2871 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89799\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2793 - accuracy: 0.9023 - val_loss: 0.2868 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89799\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2771 - accuracy: 0.9022 - val_loss: 0.2868 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89799\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2828 - accuracy: 0.8997 - val_loss: 0.2870 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89799\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2895 - accuracy: 0.8966 - val_loss: 0.2888 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89799\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2785 - accuracy: 0.9003 - val_loss: 0.2873 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89799\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2780 - accuracy: 0.9024 - val_loss: 0.2868 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89799\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2761 - accuracy: 0.9016 - val_loss: 0.2867 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89799\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2825 - accuracy: 0.8976 - val_loss: 0.2869 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89799\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2847 - accuracy: 0.8972 - val_loss: 0.2878 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89799\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2826 - accuracy: 0.8989 - val_loss: 0.2868 - val_accuracy: 0.8951\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89799\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2820 - accuracy: 0.8996 - val_loss: 0.2876 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89799\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2827 - accuracy: 0.8985 - val_loss: 0.2867 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89799\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2806 - accuracy: 0.9000 - val_loss: 0.2867 - val_accuracy: 0.8978\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89799\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2831 - accuracy: 0.8995 - val_loss: 0.2866 - val_accuracy: 0.8980\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89799\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2818 - accuracy: 0.8997 - val_loss: 0.2865 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89799\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2826 - accuracy: 0.8987 - val_loss: 0.2865 - val_accuracy: 0.8957\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89799\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8971 - val_loss: 0.2865 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89799\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2820 - accuracy: 0.9004 - val_loss: 0.2865 - val_accuracy: 0.8977\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89799\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2791 - accuracy: 0.9012 - val_loss: 0.2868 - val_accuracy: 0.8948\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89799\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2806 - accuracy: 0.8995 - val_loss: 0.2873 - val_accuracy: 0.8949\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89799\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2900 - accuracy: 0.8980\n",
            "\n",
            "13\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3236 - accuracy: 0.8859 - val_loss: 0.3001 - val_accuracy: 0.8844\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88439, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2935 - accuracy: 0.8908 - val_loss: 0.2962 - val_accuracy: 0.8923\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88439 to 0.89233, saving model to model_r1.hdf5\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2907 - accuracy: 0.8919 - val_loss: 0.2949 - val_accuracy: 0.8931\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.89233 to 0.89313, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2875 - accuracy: 0.8959 - val_loss: 0.2932 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.89313 to 0.89386, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2867 - accuracy: 0.8931 - val_loss: 0.2933 - val_accuracy: 0.8952\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.89386 to 0.89516, saving model to model_r1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2834 - accuracy: 0.8970 - val_loss: 0.2912 - val_accuracy: 0.8951\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.89516\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2924 - accuracy: 0.8947 - val_loss: 0.2907 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.89516 to 0.89629, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2836 - accuracy: 0.8980 - val_loss: 0.2901 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.89629 to 0.89653, saving model to model_r1.hdf5\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2866 - accuracy: 0.8963 - val_loss: 0.2895 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.89653\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2793 - accuracy: 0.8992 - val_loss: 0.2912 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.89653\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2858 - accuracy: 0.8960 - val_loss: 0.2895 - val_accuracy: 0.8952\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.89653\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.8949 - val_loss: 0.2881 - val_accuracy: 0.8971\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.89653 to 0.89710, saving model to model_r1.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2824 - accuracy: 0.8990 - val_loss: 0.2886 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.89710\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2766 - accuracy: 0.9024 - val_loss: 0.2912 - val_accuracy: 0.8948\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89710\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2791 - accuracy: 0.9011 - val_loss: 0.2875 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.89710\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2809 - accuracy: 0.8990 - val_loss: 0.2874 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.89710 to 0.89751, saving model to model_r1.hdf5\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2807 - accuracy: 0.9007 - val_loss: 0.2878 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.89751\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2840 - accuracy: 0.8978 - val_loss: 0.2874 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.89751 to 0.89759, saving model to model_r1.hdf5\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2773 - accuracy: 0.9020 - val_loss: 0.2882 - val_accuracy: 0.8952\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89759\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2757 - accuracy: 0.9019 - val_loss: 0.2883 - val_accuracy: 0.8974\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89759\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2802 - accuracy: 0.9005 - val_loss: 0.2875 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89759\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8969 - val_loss: 0.2878 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89759\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2803 - accuracy: 0.9005 - val_loss: 0.2872 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89759\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2821 - accuracy: 0.8965 - val_loss: 0.2878 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89759\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2813 - accuracy: 0.9008 - val_loss: 0.2877 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89759\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2776 - accuracy: 0.9015 - val_loss: 0.2871 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89759\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2740 - accuracy: 0.9017 - val_loss: 0.2879 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.89759\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8986 - val_loss: 0.2880 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.89759\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.8957 - val_loss: 0.2879 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89759\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8995 - val_loss: 0.2874 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89759\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2783 - accuracy: 0.9016 - val_loss: 0.2874 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89759\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2772 - accuracy: 0.9017 - val_loss: 0.2875 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89759\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2784 - accuracy: 0.9007 - val_loss: 0.2872 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89759\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2752 - accuracy: 0.9030 - val_loss: 0.2880 - val_accuracy: 0.8951\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89759\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2795 - accuracy: 0.9014 - val_loss: 0.2875 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89759\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2794 - accuracy: 0.9004 - val_loss: 0.2876 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89759\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2845 - accuracy: 0.8986 - val_loss: 0.2870 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89759\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.8979 - val_loss: 0.2870 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89759\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2786 - accuracy: 0.9004 - val_loss: 0.2883 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89759\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2750 - accuracy: 0.9023 - val_loss: 0.2872 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89759\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2816 - accuracy: 0.9012 - val_loss: 0.2872 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89759\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2790 - accuracy: 0.9019 - val_loss: 0.2877 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89759\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8989 - val_loss: 0.2876 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89759\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2736 - accuracy: 0.9018 - val_loss: 0.2870 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89759\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2802 - accuracy: 0.8989 - val_loss: 0.2872 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89759\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2816 - accuracy: 0.9007 - val_loss: 0.2881 - val_accuracy: 0.8951\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89759\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2750 - accuracy: 0.9013 - val_loss: 0.2876 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89759\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2775 - accuracy: 0.9012 - val_loss: 0.2872 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89759\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8994 - val_loss: 0.2880 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89759\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2855 - accuracy: 0.8977 - val_loss: 0.2873 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89759\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2772 - accuracy: 0.9035 - val_loss: 0.2872 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89759\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2864 - accuracy: 0.8947 - val_loss: 0.2870 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89759\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2796 - accuracy: 0.8993 - val_loss: 0.2880 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89759\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2849 - accuracy: 0.8987 - val_loss: 0.2879 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89759\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2812 - accuracy: 0.9003 - val_loss: 0.2877 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89759\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2760 - accuracy: 0.9015 - val_loss: 0.2870 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89759\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2826 - accuracy: 0.9004 - val_loss: 0.2869 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89759\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2742 - accuracy: 0.9021 - val_loss: 0.2872 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89759\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2740 - accuracy: 0.9015 - val_loss: 0.2871 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89759\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2805 - accuracy: 0.8987 - val_loss: 0.2874 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89759\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2833 - accuracy: 0.8986 - val_loss: 0.2882 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89759\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2829 - accuracy: 0.8978 - val_loss: 0.2883 - val_accuracy: 0.8950\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.89759\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2838 - accuracy: 0.8970 - val_loss: 0.2871 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89759\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2803 - accuracy: 0.8995 - val_loss: 0.2872 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89759\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2874 - accuracy: 0.8976\n",
            "\n",
            "14\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.3562 - accuracy: 0.8852 - val_loss: 0.3055 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88293, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2940 - accuracy: 0.8915 - val_loss: 0.2976 - val_accuracy: 0.8814\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88293\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2913 - accuracy: 0.8887 - val_loss: 0.2965 - val_accuracy: 0.8896\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88293 to 0.88957, saving model to model_r1.hdf5\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2887 - accuracy: 0.8976 - val_loss: 0.2943 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.88957 to 0.89435, saving model to model_r1.hdf5\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8945 - val_loss: 0.2932 - val_accuracy: 0.8914\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.89435\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2867 - accuracy: 0.8972 - val_loss: 0.2925 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.89435\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2811 - accuracy: 0.8998 - val_loss: 0.2918 - val_accuracy: 0.8948\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.89435 to 0.89483, saving model to model_r1.hdf5\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2830 - accuracy: 0.8983 - val_loss: 0.2912 - val_accuracy: 0.8939\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.89483\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2830 - accuracy: 0.8987 - val_loss: 0.2924 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.89483 to 0.89629, saving model to model_r1.hdf5\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2762 - accuracy: 0.9025 - val_loss: 0.2900 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.89629\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.8990 - val_loss: 0.2899 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.89629\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8956 - val_loss: 0.2897 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.89629 to 0.89694, saving model to model_r1.hdf5\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.8980 - val_loss: 0.2894 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.89694 to 0.89702, saving model to model_r1.hdf5\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2833 - accuracy: 0.8993 - val_loss: 0.2890 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.89702\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2859 - accuracy: 0.8981 - val_loss: 0.2891 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.89702\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2791 - accuracy: 0.9011 - val_loss: 0.2891 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.89702\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.8982 - val_loss: 0.2887 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.89702 to 0.89726, saving model to model_r1.hdf5\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8993 - val_loss: 0.2890 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.89726 to 0.89751, saving model to model_r1.hdf5\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2842 - accuracy: 0.8994 - val_loss: 0.2884 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.89751\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8958 - val_loss: 0.2885 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.89751\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2827 - accuracy: 0.8993 - val_loss: 0.2886 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.89751\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2805 - accuracy: 0.9012 - val_loss: 0.2891 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.89751\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.8993 - val_loss: 0.2884 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.89751\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2773 - accuracy: 0.9025 - val_loss: 0.2883 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.89751\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2801 - accuracy: 0.8997 - val_loss: 0.2884 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.89751\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2773 - accuracy: 0.9011 - val_loss: 0.2883 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.89751\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2789 - accuracy: 0.8996 - val_loss: 0.2883 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.89751\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2777 - accuracy: 0.9004 - val_loss: 0.2882 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.89751\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2811 - accuracy: 0.8982 - val_loss: 0.2880 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.89751\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2837 - accuracy: 0.8986 - val_loss: 0.2876 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.89751\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2804 - accuracy: 0.8989 - val_loss: 0.2895 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.89751\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2732 - accuracy: 0.9034 - val_loss: 0.2884 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.89751\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2837 - accuracy: 0.8988 - val_loss: 0.2875 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.89751\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.8968 - val_loss: 0.2877 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.89751\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2763 - accuracy: 0.9019 - val_loss: 0.2879 - val_accuracy: 0.8961\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.89751\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2820 - accuracy: 0.8988 - val_loss: 0.2873 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.89751\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.8983 - val_loss: 0.2873 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.89751\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2792 - accuracy: 0.9002 - val_loss: 0.2875 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.89751\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2807 - accuracy: 0.9009 - val_loss: 0.2878 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.89751\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2797 - accuracy: 0.9008 - val_loss: 0.2873 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89751\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.8953 - val_loss: 0.2872 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89751\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2787 - accuracy: 0.9004 - val_loss: 0.2881 - val_accuracy: 0.8952\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89751\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2810 - accuracy: 0.8992 - val_loss: 0.2873 - val_accuracy: 0.8952\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89751\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2784 - accuracy: 0.9000 - val_loss: 0.2871 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89751\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2849 - accuracy: 0.8988 - val_loss: 0.2871 - val_accuracy: 0.8965\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89751\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.8980 - val_loss: 0.2879 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.89751\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2794 - accuracy: 0.9003 - val_loss: 0.2882 - val_accuracy: 0.8972\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89751\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2777 - accuracy: 0.9014 - val_loss: 0.2869 - val_accuracy: 0.8964\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89751\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2772 - accuracy: 0.9007 - val_loss: 0.2871 - val_accuracy: 0.8968\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89751\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2798 - accuracy: 0.9006 - val_loss: 0.2875 - val_accuracy: 0.8956\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89751\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2831 - accuracy: 0.8977 - val_loss: 0.2873 - val_accuracy: 0.8969\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89751\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2784 - accuracy: 0.8999 - val_loss: 0.2872 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89751\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2758 - accuracy: 0.9020 - val_loss: 0.2870 - val_accuracy: 0.8962\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89751\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2746 - accuracy: 0.9014 - val_loss: 0.2871 - val_accuracy: 0.8954\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89751\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2788 - accuracy: 0.8995 - val_loss: 0.2867 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89751\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2766 - accuracy: 0.9021 - val_loss: 0.2872 - val_accuracy: 0.8929\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.89751\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2851 - accuracy: 0.8970 - val_loss: 0.2867 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.89751\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2758 - accuracy: 0.9016 - val_loss: 0.2867 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.89751\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8977 - val_loss: 0.2866 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.89751\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2763 - accuracy: 0.9008 - val_loss: 0.2867 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.89751\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2731 - accuracy: 0.9016 - val_loss: 0.2861 - val_accuracy: 0.8970\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.89751\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2799 - accuracy: 0.9005 - val_loss: 0.2863 - val_accuracy: 0.8976\n",
            "\n",
            "Epoch 00062: val_accuracy improved from 0.89751 to 0.89759, saving model to model_r1.hdf5\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2857 - accuracy: 0.8972 - val_loss: 0.2860 - val_accuracy: 0.8967\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.89759\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2743 - accuracy: 0.9015 - val_loss: 0.2865 - val_accuracy: 0.8963\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.89759\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2863 - accuracy: 0.8976\n",
            "\n",
            "15\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.6179 - accuracy: 0.8476 - val_loss: 0.4604 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4294 - accuracy: 0.8902 - val_loss: 0.3844 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3724 - accuracy: 0.8873 - val_loss: 0.3630 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3513 - accuracy: 0.8898 - val_loss: 0.3588 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3505 - accuracy: 0.8883 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3497 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3491 - accuracy: 0.8888 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3444 - accuracy: 0.8910 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3553 - accuracy: 0.8858 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3512 - accuracy: 0.8878 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3505 - accuracy: 0.8881 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3475 - accuracy: 0.8895 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3427 - accuracy: 0.8918 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3514 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3483 - accuracy: 0.8892 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3446 - accuracy: 0.8909 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3479 - accuracy: 0.8894 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3494 - accuracy: 0.8886 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3462 - accuracy: 0.8902 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3495 - accuracy: 0.8886 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3539 - accuracy: 0.8865 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3472 - accuracy: 0.8897 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3490 - accuracy: 0.8888 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3503 - accuracy: 0.8882 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3516 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3567 - accuracy: 0.8851 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3591 - accuracy: 0.8840 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3467 - accuracy: 0.8899 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3495 - accuracy: 0.8886 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3531 - accuracy: 0.8868 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3457 - accuracy: 0.8904 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3518 - accuracy: 0.8875 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3474 - accuracy: 0.8896 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3518 - accuracy: 0.8875 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3519 - accuracy: 0.8874 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3424 - accuracy: 0.8920 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3528 - accuracy: 0.8870 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3505 - accuracy: 0.8881 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3472 - accuracy: 0.8897 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3491 - accuracy: 0.8888 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88423\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3484 - accuracy: 0.8891 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88423\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3433 - accuracy: 0.8916 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88423\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3541 - accuracy: 0.8864 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88423\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3459 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88423\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3488 - accuracy: 0.8889 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88423\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3437 - accuracy: 0.8914 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88423\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3549 - accuracy: 0.8859 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88423\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3427 - accuracy: 0.8919 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.88423\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3537 - accuracy: 0.8866 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88423\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3513 - accuracy: 0.8877 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88423\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3453 - accuracy: 0.8906 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88423\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88423\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3455 - accuracy: 0.8905 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88423\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3471 - accuracy: 0.8897 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.88423\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3450 - accuracy: 0.8907 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.88423\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.88423\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3463 - accuracy: 0.8901 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.88423\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3485 - accuracy: 0.8890 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.88423\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3508 - accuracy: 0.8879 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.88423\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3597 - accuracy: 0.8837 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.88423\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3431 - accuracy: 0.8916 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.88423\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3535 - accuracy: 0.8866 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.88423\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.4604 - accuracy: 0.8842\n",
            "\n",
            "16\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4240 - accuracy: 0.8746 - val_loss: 0.3111 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2997 - accuracy: 0.8900 - val_loss: 0.3017 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2988 - accuracy: 0.8857 - val_loss: 0.3002 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2950 - accuracy: 0.8870 - val_loss: 0.2998 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2918 - accuracy: 0.8873 - val_loss: 0.2994 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.88423 to 0.88585, saving model to model_r1.hdf5\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2920 - accuracy: 0.8894 - val_loss: 0.2992 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88585\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2930 - accuracy: 0.8911 - val_loss: 0.2989 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88585\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2905 - accuracy: 0.8903 - val_loss: 0.2990 - val_accuracy: 0.8863\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.88585 to 0.88633, saving model to model_r1.hdf5\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2882 - accuracy: 0.8896 - val_loss: 0.2997 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88633\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2922 - accuracy: 0.8896 - val_loss: 0.2986 - val_accuracy: 0.8861\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88633\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2888 - accuracy: 0.8917 - val_loss: 0.2979 - val_accuracy: 0.8856\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88633\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2836 - accuracy: 0.8932 - val_loss: 0.2976 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88633\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2904 - accuracy: 0.8903 - val_loss: 0.2976 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.88633 to 0.88666, saving model to model_r1.hdf5\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2833 - accuracy: 0.8921 - val_loss: 0.2974 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88666\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2855 - accuracy: 0.8937 - val_loss: 0.2974 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88666\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2887 - accuracy: 0.8929 - val_loss: 0.2972 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88666\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2917 - accuracy: 0.8894 - val_loss: 0.2975 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88666\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2866 - accuracy: 0.8936 - val_loss: 0.2970 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88666\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2837 - accuracy: 0.8939 - val_loss: 0.2973 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88666\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2896 - accuracy: 0.8907 - val_loss: 0.2974 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88666\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2812 - accuracy: 0.8946 - val_loss: 0.2969 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88666\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2907 - accuracy: 0.8897 - val_loss: 0.2967 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88666\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2847 - accuracy: 0.8921 - val_loss: 0.2966 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88666\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2919 - accuracy: 0.8900 - val_loss: 0.2968 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88666\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.8936 - val_loss: 0.2966 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88666\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2907 - accuracy: 0.8916 - val_loss: 0.2966 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88666\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2920 - accuracy: 0.8925 - val_loss: 0.2969 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88666\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2888 - accuracy: 0.8901 - val_loss: 0.2965 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88666\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8919 - val_loss: 0.2967 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88666\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2863 - accuracy: 0.8923 - val_loss: 0.2965 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88666\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2968 - accuracy: 0.8891 - val_loss: 0.2971 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88666\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2890 - accuracy: 0.8909 - val_loss: 0.2964 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88666\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2886 - accuracy: 0.8908 - val_loss: 0.2962 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88666\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2881 - accuracy: 0.8899 - val_loss: 0.2964 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88666\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2905 - accuracy: 0.8928 - val_loss: 0.2963 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88666\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2868 - accuracy: 0.8915 - val_loss: 0.2961 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88666\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2836 - accuracy: 0.8946 - val_loss: 0.2966 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88666\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2889 - accuracy: 0.8912 - val_loss: 0.2966 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88666\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8925 - val_loss: 0.2962 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88666\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2919 - accuracy: 0.8910 - val_loss: 0.2960 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88666\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2947 - accuracy: 0.8900 - val_loss: 0.2960 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88666\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2850 - accuracy: 0.8946 - val_loss: 0.2966 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88666\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2820 - accuracy: 0.8937 - val_loss: 0.2965 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88666\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2871 - accuracy: 0.8913 - val_loss: 0.2968 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88666\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2912 - accuracy: 0.8920 - val_loss: 0.2960 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88666\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2888 - accuracy: 0.8910 - val_loss: 0.2961 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88666\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2985 - accuracy: 0.8880 - val_loss: 0.2962 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88666\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2941 - accuracy: 0.8878 - val_loss: 0.2958 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88666\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2903 - accuracy: 0.8902 - val_loss: 0.2957 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88666\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2889 - accuracy: 0.8934 - val_loss: 0.2967 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.88666\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2894 - accuracy: 0.8907 - val_loss: 0.2959 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88666\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2876 - accuracy: 0.8913 - val_loss: 0.2958 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88666\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2978 - accuracy: 0.8857 - val_loss: 0.2969 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88666\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2867 - accuracy: 0.8923 - val_loss: 0.2956 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88666\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2800 - accuracy: 0.8957 - val_loss: 0.2958 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88666\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2932 - accuracy: 0.8890 - val_loss: 0.2958 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.88666\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2842 - accuracy: 0.8933 - val_loss: 0.2956 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.88666\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2909 - accuracy: 0.8895 - val_loss: 0.2959 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.88666\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2907 - accuracy: 0.8888 - val_loss: 0.2957 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.88666\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2798 - accuracy: 0.8959 - val_loss: 0.2962 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.88666\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2834 - accuracy: 0.8943 - val_loss: 0.2959 - val_accuracy: 0.8863\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.88666\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8940 - val_loss: 0.2959 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.88666\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2881 - accuracy: 0.8940 - val_loss: 0.2954 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.88666\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2879 - accuracy: 0.8907 - val_loss: 0.2957 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.88666\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2976 - accuracy: 0.8867\n",
            "\n",
            "17\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.3400 - accuracy: 0.8883 - val_loss: 0.3073 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2964 - accuracy: 0.8898 - val_loss: 0.3023 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2943 - accuracy: 0.8905 - val_loss: 0.3005 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2973 - accuracy: 0.8868 - val_loss: 0.2985 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2884 - accuracy: 0.8903 - val_loss: 0.2979 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2910 - accuracy: 0.8908 - val_loss: 0.2958 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.8885 - val_loss: 0.2960 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2874 - accuracy: 0.8895 - val_loss: 0.2958 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2888 - accuracy: 0.8878 - val_loss: 0.2937 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2929 - accuracy: 0.8879 - val_loss: 0.2941 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2932 - accuracy: 0.8863 - val_loss: 0.2939 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.8868 - val_loss: 0.2950 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2910 - accuracy: 0.8876 - val_loss: 0.2934 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2843 - accuracy: 0.8917 - val_loss: 0.2933 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.8889 - val_loss: 0.2937 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2892 - accuracy: 0.8893 - val_loss: 0.2930 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2850 - accuracy: 0.8883 - val_loss: 0.2940 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2924 - accuracy: 0.8872 - val_loss: 0.2934 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8892 - val_loss: 0.2937 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2870 - accuracy: 0.8882 - val_loss: 0.2928 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2898 - accuracy: 0.8853 - val_loss: 0.2928 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2880 - accuracy: 0.8896 - val_loss: 0.2934 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2888 - accuracy: 0.8859 - val_loss: 0.2926 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2835 - accuracy: 0.8878 - val_loss: 0.2926 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2796 - accuracy: 0.8920 - val_loss: 0.2930 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2819 - accuracy: 0.8886 - val_loss: 0.2927 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2875 - accuracy: 0.8896 - val_loss: 0.2926 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2824 - accuracy: 0.8904 - val_loss: 0.2928 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2881 - accuracy: 0.8882 - val_loss: 0.2924 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2922 - accuracy: 0.8855 - val_loss: 0.2922 - val_accuracy: 0.8840\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2836 - accuracy: 0.8907 - val_loss: 0.2921 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2896 - accuracy: 0.8872 - val_loss: 0.2932 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2839 - accuracy: 0.8892 - val_loss: 0.2923 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2819 - accuracy: 0.8898 - val_loss: 0.2925 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.8859 - val_loss: 0.2921 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2926 - accuracy: 0.8858 - val_loss: 0.2925 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2882 - accuracy: 0.8878 - val_loss: 0.2924 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2835 - accuracy: 0.8894 - val_loss: 0.2925 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2916 - accuracy: 0.8870 - val_loss: 0.2920 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2862 - accuracy: 0.8882 - val_loss: 0.2931 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2917 - accuracy: 0.8860 - val_loss: 0.2921 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2867 - accuracy: 0.8901 - val_loss: 0.2920 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88423\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2882 - accuracy: 0.8877 - val_loss: 0.2921 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88423\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2865 - accuracy: 0.8892 - val_loss: 0.2935 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88423\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2767 - accuracy: 0.8923 - val_loss: 0.2922 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88423\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2877 - accuracy: 0.8864 - val_loss: 0.2927 - val_accuracy: 0.8841\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88423\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2845 - accuracy: 0.8909 - val_loss: 0.2922 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00047: val_accuracy improved from 0.88423 to 0.88544, saving model to model_r1.hdf5\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2779 - accuracy: 0.8940 - val_loss: 0.2918 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88544\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2805 - accuracy: 0.8929 - val_loss: 0.2918 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88544\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8889 - val_loss: 0.2918 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00050: val_accuracy improved from 0.88544 to 0.88601, saving model to model_r1.hdf5\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2828 - accuracy: 0.8908 - val_loss: 0.2918 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88601\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2843 - accuracy: 0.8911 - val_loss: 0.2917 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88601\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8883 - val_loss: 0.2918 - val_accuracy: 0.8857\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88601\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2843 - accuracy: 0.8906 - val_loss: 0.2934 - val_accuracy: 0.8857\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88601\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2912 - accuracy: 0.8855 - val_loss: 0.2919 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88601\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2921 - accuracy: 0.8869 - val_loss: 0.2922 - val_accuracy: 0.8855\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.88601\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2910 - accuracy: 0.8879 - val_loss: 0.2921 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.88601\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2884 - accuracy: 0.8904 - val_loss: 0.2915 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.88601\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2909 - accuracy: 0.8884 - val_loss: 0.2914 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.88601\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2832 - accuracy: 0.8901 - val_loss: 0.2914 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.88601\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2847 - accuracy: 0.8909 - val_loss: 0.2926 - val_accuracy: 0.8854\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.88601\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2807 - accuracy: 0.8918 - val_loss: 0.2916 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.88601\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2891 - accuracy: 0.8898 - val_loss: 0.2918 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.88601\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2912 - accuracy: 0.8883 - val_loss: 0.2921 - val_accuracy: 0.8860\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.88601\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2918 - accuracy: 0.8860\n",
            "\n",
            "18\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.4544 - accuracy: 0.8835 - val_loss: 0.3047 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2898 - accuracy: 0.8913 - val_loss: 0.3011 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2997 - accuracy: 0.8855 - val_loss: 0.3007 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2931 - accuracy: 0.8887 - val_loss: 0.3004 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2918 - accuracy: 0.8895 - val_loss: 0.3001 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2861 - accuracy: 0.8917 - val_loss: 0.3002 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2931 - accuracy: 0.8888 - val_loss: 0.3000 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2953 - accuracy: 0.8870 - val_loss: 0.3002 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2930 - accuracy: 0.8899 - val_loss: 0.2997 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2991 - accuracy: 0.8847 - val_loss: 0.2998 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.88423 to 0.88504, saving model to model_r1.hdf5\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2902 - accuracy: 0.8919 - val_loss: 0.2995 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88504\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2931 - accuracy: 0.8888 - val_loss: 0.2997 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88504\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2958 - accuracy: 0.8889 - val_loss: 0.2996 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88504\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2910 - accuracy: 0.8891 - val_loss: 0.2994 - val_accuracy: 0.8834\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88504\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2907 - accuracy: 0.8893 - val_loss: 0.2992 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88504\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2966 - accuracy: 0.8873 - val_loss: 0.2992 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88504\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2950 - accuracy: 0.8872 - val_loss: 0.2989 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88504\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2936 - accuracy: 0.8903 - val_loss: 0.2986 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88504\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2950 - accuracy: 0.8874 - val_loss: 0.2991 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88504\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2914 - accuracy: 0.8905 - val_loss: 0.2990 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88504\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2932 - accuracy: 0.8910 - val_loss: 0.2984 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88504\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2931 - accuracy: 0.8902 - val_loss: 0.2986 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88504\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2914 - accuracy: 0.8893 - val_loss: 0.2985 - val_accuracy: 0.8845\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88504\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2902 - accuracy: 0.8883 - val_loss: 0.2985 - val_accuracy: 0.8826\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88504\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2932 - accuracy: 0.8900 - val_loss: 0.2979 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88504\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2911 - accuracy: 0.8876 - val_loss: 0.2982 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88504\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2912 - accuracy: 0.8882 - val_loss: 0.2978 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88504\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3017 - accuracy: 0.8840 - val_loss: 0.2983 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88504\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2917 - accuracy: 0.8901 - val_loss: 0.2977 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88504\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2928 - accuracy: 0.8872 - val_loss: 0.2985 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88504\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2901 - accuracy: 0.8926 - val_loss: 0.2975 - val_accuracy: 0.8846\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88504\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8893 - val_loss: 0.2977 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88504\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2885 - accuracy: 0.8888 - val_loss: 0.2975 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88504\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2909 - accuracy: 0.8882 - val_loss: 0.2976 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88504\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2881 - accuracy: 0.8918 - val_loss: 0.2977 - val_accuracy: 0.8836\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88504\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2859 - accuracy: 0.8904 - val_loss: 0.2976 - val_accuracy: 0.8824\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88504\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8885 - val_loss: 0.2973 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88504\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2878 - accuracy: 0.8908 - val_loss: 0.2971 - val_accuracy: 0.8831\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88504\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2851 - accuracy: 0.8908 - val_loss: 0.2972 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88504\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2955 - accuracy: 0.8890 - val_loss: 0.2972 - val_accuracy: 0.8824\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88504\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2867 - accuracy: 0.8902 - val_loss: 0.2971 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88504\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2885 - accuracy: 0.8895 - val_loss: 0.2971 - val_accuracy: 0.8830\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88504\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2882 - accuracy: 0.8882 - val_loss: 0.2970 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88504\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2859 - accuracy: 0.8905 - val_loss: 0.2970 - val_accuracy: 0.8833\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88504\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2913 - accuracy: 0.8856 - val_loss: 0.2970 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88504\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2884 - accuracy: 0.8881 - val_loss: 0.2975 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88504\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2897 - accuracy: 0.8879 - val_loss: 0.2969 - val_accuracy: 0.8829\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88504\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2866 - accuracy: 0.8875 - val_loss: 0.2969 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88504\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2889 - accuracy: 0.8889 - val_loss: 0.2968 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88504\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2892 - accuracy: 0.8877 - val_loss: 0.2968 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.88504\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8893 - val_loss: 0.2969 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88504\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2885 - accuracy: 0.8892 - val_loss: 0.2972 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88504\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2846 - accuracy: 0.8909 - val_loss: 0.2974 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88504\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2920 - accuracy: 0.8883 - val_loss: 0.2971 - val_accuracy: 0.8826\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88504\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8922 - val_loss: 0.2967 - val_accuracy: 0.8826\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88504\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2906 - accuracy: 0.8876 - val_loss: 0.2968 - val_accuracy: 0.8837\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.88504\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2917 - accuracy: 0.8864 - val_loss: 0.2978 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.88504\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2893 - accuracy: 0.8879 - val_loss: 0.2969 - val_accuracy: 0.8827\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.88504\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2860 - accuracy: 0.8896 - val_loss: 0.2967 - val_accuracy: 0.8827\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.88504\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2928 - accuracy: 0.8868 - val_loss: 0.2976 - val_accuracy: 0.8839\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.88504\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2848 - accuracy: 0.8891 - val_loss: 0.2968 - val_accuracy: 0.8826\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.88504\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2869 - accuracy: 0.8892 - val_loss: 0.2968 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.88504\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2894 - accuracy: 0.8893 - val_loss: 0.2967 - val_accuracy: 0.8824\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.88504\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2906 - accuracy: 0.8874 - val_loss: 0.2968 - val_accuracy: 0.8824\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.88504\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.2998 - accuracy: 0.8850\n",
            "\n",
            "19\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.3623 - accuracy: 0.8877 - val_loss: 0.3078 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2968 - accuracy: 0.8889 - val_loss: 0.3033 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2923 - accuracy: 0.8902 - val_loss: 0.3037 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2938 - accuracy: 0.8892 - val_loss: 0.3026 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2934 - accuracy: 0.8887 - val_loss: 0.3027 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2954 - accuracy: 0.8895 - val_loss: 0.3028 - val_accuracy: 0.8838\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2904 - accuracy: 0.8904 - val_loss: 0.3026 - val_accuracy: 0.8825\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2910 - accuracy: 0.8904 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3014 - accuracy: 0.8863 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2929 - accuracy: 0.8888 - val_loss: 0.3031 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2982 - accuracy: 0.8877 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2952 - accuracy: 0.8906 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2939 - accuracy: 0.8899 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2935 - accuracy: 0.8896 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2956 - accuracy: 0.8869 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2971 - accuracy: 0.8885 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2968 - accuracy: 0.8867 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2983 - accuracy: 0.8863 - val_loss: 0.3030 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2925 - accuracy: 0.8907 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2888 - accuracy: 0.8907 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2932 - accuracy: 0.8910 - val_loss: 0.3029 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2939 - accuracy: 0.8880 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2937 - accuracy: 0.8896 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2994 - accuracy: 0.8868 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3010 - accuracy: 0.8872 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2993 - accuracy: 0.8870 - val_loss: 0.3029 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2940 - accuracy: 0.8897 - val_loss: 0.3029 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2958 - accuracy: 0.8885 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2991 - accuracy: 0.8874 - val_loss: 0.3031 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2963 - accuracy: 0.8885 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3012 - accuracy: 0.8843 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2913 - accuracy: 0.8909 - val_loss: 0.3030 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2928 - accuracy: 0.8897 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2941 - accuracy: 0.8903 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3030 - accuracy: 0.8843 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3016 - accuracy: 0.8861 - val_loss: 0.3037 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2980 - accuracy: 0.8882 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2906 - accuracy: 0.8928 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2950 - accuracy: 0.8897 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2986 - accuracy: 0.8881 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2886 - accuracy: 0.8919 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2977 - accuracy: 0.8888 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88423\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2981 - accuracy: 0.8880 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88423\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2968 - accuracy: 0.8880 - val_loss: 0.3031 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88423\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2913 - accuracy: 0.8909 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88423\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2919 - accuracy: 0.8903 - val_loss: 0.3028 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88423\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2993 - accuracy: 0.8864 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88423\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2990 - accuracy: 0.8881 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88423\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2951 - accuracy: 0.8882 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88423\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2940 - accuracy: 0.8914 - val_loss: 0.3029 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.88423\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2937 - accuracy: 0.8917 - val_loss: 0.3029 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88423\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2981 - accuracy: 0.8887 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88423\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3013 - accuracy: 0.8861 - val_loss: 0.3030 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88423\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2975 - accuracy: 0.8874 - val_loss: 0.3031 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88423\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2954 - accuracy: 0.8905 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88423\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2943 - accuracy: 0.8885 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.88423\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2898 - accuracy: 0.8917 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.88423\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2998 - accuracy: 0.8852 - val_loss: 0.3029 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.88423\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2923 - accuracy: 0.8899 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.88423\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2972 - accuracy: 0.8887 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.88423\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2973 - accuracy: 0.8880 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.88423\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2988 - accuracy: 0.8867 - val_loss: 0.3026 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.88423\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.2955 - accuracy: 0.8873 - val_loss: 0.3031 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.88423\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3026 - accuracy: 0.8860 - val_loss: 0.3027 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.88423\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.3078 - accuracy: 0.8842\n",
            "\n",
            "20\n",
            "\n",
            "Epoch 1/64\n",
            "961/961 [==============================] - 3s 2ms/step - loss: 0.6177 - accuracy: 0.8878 - val_loss: 0.4612 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r1.hdf5\n",
            "Epoch 2/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.4308 - accuracy: 0.8893 - val_loss: 0.3847 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3700 - accuracy: 0.8890 - val_loss: 0.3632 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.88423\n",
            "Epoch 4/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3554 - accuracy: 0.8877 - val_loss: 0.3588 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.88423\n",
            "Epoch 5/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3577 - accuracy: 0.8847 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.88423\n",
            "Epoch 6/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3534 - accuracy: 0.8867 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.88423\n",
            "Epoch 7/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3460 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.88423\n",
            "Epoch 8/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3488 - accuracy: 0.8889 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.88423\n",
            "Epoch 9/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3452 - accuracy: 0.8907 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.88423\n",
            "Epoch 10/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3435 - accuracy: 0.8915 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.88423\n",
            "Epoch 11/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8883 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.88423\n",
            "Epoch 12/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3531 - accuracy: 0.8868 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.88423\n",
            "Epoch 13/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3466 - accuracy: 0.8900 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.88423\n",
            "Epoch 14/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3521 - accuracy: 0.8873 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.88423\n",
            "Epoch 15/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3452 - accuracy: 0.8907 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.88423\n",
            "Epoch 16/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3506 - accuracy: 0.8880 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.88423\n",
            "Epoch 17/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3429 - accuracy: 0.8917 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.88423\n",
            "Epoch 18/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3502 - accuracy: 0.8882 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.88423\n",
            "Epoch 19/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3456 - accuracy: 0.8904 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.88423\n",
            "Epoch 20/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3450 - accuracy: 0.8908 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.88423\n",
            "Epoch 21/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3531 - accuracy: 0.8869 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88423\n",
            "Epoch 22/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3462 - accuracy: 0.8902 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.88423\n",
            "Epoch 23/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3572 - accuracy: 0.8849 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.88423\n",
            "Epoch 24/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3498 - accuracy: 0.8884 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.88423\n",
            "Epoch 25/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3416 - accuracy: 0.8924 - val_loss: 0.3584 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.88423\n",
            "Epoch 26/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3491 - accuracy: 0.8888 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.88423\n",
            "Epoch 27/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3482 - accuracy: 0.8892 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.88423\n",
            "Epoch 28/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3543 - accuracy: 0.8863 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.88423\n",
            "Epoch 29/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3517 - accuracy: 0.8875 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.88423\n",
            "Epoch 30/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3533 - accuracy: 0.8867 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.88423\n",
            "Epoch 31/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3507 - accuracy: 0.8880 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.88423\n",
            "Epoch 32/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3450 - accuracy: 0.8907 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.88423\n",
            "Epoch 33/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3516 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88423\n",
            "Epoch 34/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3463 - accuracy: 0.8901 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88423\n",
            "Epoch 35/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3470 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88423\n",
            "Epoch 36/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3499 - accuracy: 0.8884 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88423\n",
            "Epoch 37/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3460 - accuracy: 0.8902 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88423\n",
            "Epoch 38/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3463 - accuracy: 0.8901 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88423\n",
            "Epoch 39/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3475 - accuracy: 0.8895 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.88423\n",
            "Epoch 40/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3469 - accuracy: 0.8898 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.88423\n",
            "Epoch 41/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3476 - accuracy: 0.8895 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.88423\n",
            "Epoch 42/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3456 - accuracy: 0.8905 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.88423\n",
            "Epoch 43/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3573 - accuracy: 0.8848 - val_loss: 0.3586 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.88423\n",
            "Epoch 44/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3492 - accuracy: 0.8887 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.88423\n",
            "Epoch 45/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3513 - accuracy: 0.8877 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.88423\n",
            "Epoch 46/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3456 - accuracy: 0.8905 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.88423\n",
            "Epoch 47/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3520 - accuracy: 0.8874 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.88423\n",
            "Epoch 48/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3433 - accuracy: 0.8915 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.88423\n",
            "Epoch 49/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3459 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.88423\n",
            "Epoch 50/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3476 - accuracy: 0.8895 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.88423\n",
            "Epoch 51/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3541 - accuracy: 0.8864 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.88423\n",
            "Epoch 52/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3516 - accuracy: 0.8876 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.88423\n",
            "Epoch 53/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3437 - accuracy: 0.8914 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.88423\n",
            "Epoch 54/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3496 - accuracy: 0.8885 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.88423\n",
            "Epoch 55/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3445 - accuracy: 0.8910 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.88423\n",
            "Epoch 56/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3468 - accuracy: 0.8899 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.88423\n",
            "Epoch 57/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3434 - accuracy: 0.8915 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.88423\n",
            "Epoch 58/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3467 - accuracy: 0.8899 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.88423\n",
            "Epoch 59/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3486 - accuracy: 0.8890 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.88423\n",
            "Epoch 60/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3532 - accuracy: 0.8868 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.88423\n",
            "Epoch 61/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3572 - accuracy: 0.8849 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.88423\n",
            "Epoch 62/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3458 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.88423\n",
            "Epoch 63/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3464 - accuracy: 0.8901 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.88423\n",
            "Epoch 64/64\n",
            "961/961 [==============================] - 2s 2ms/step - loss: 0.3459 - accuracy: 0.8903 - val_loss: 0.3585 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.88423\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.4612 - accuracy: 0.8842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "ROx5gv6h1YpX",
        "outputId": "3ddfbcdf-1d52-4cda-a9a0-0ff1aefaac1e"
      },
      "source": [
        "column_names = ['ALL-features','Removing 0', 'Removing 1', 'Removing 2', 'Removing 3', 'Removing 4', 'Removing 5', 'Removing 6','Removing 7', 'Removing 8', 'Removing 9', 'Removing 10', 'Removing 11', 'Removing 12', 'Removing 13', 'Removing 14', 'Removing 15', 'Removing 16','Removing 17', 'Removing 18', 'Removing 19']\n",
        "plt.bar(column_names[:21], accuracy_2)\n",
        "plt.ylim(0.85,0.92)\n",
        "plt.xlabel('Features', fontsize=10)\n",
        "plt.ylabel('Accuracy', fontsize=10)\n",
        "plt.xticks(column_names[:21], fontsize=5, rotation=90)\n",
        "plt.title('Validation accuracy as features are removed')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqtnhd9WuGbV"
      },
      "source": [
        "**Removing Least-2 significant Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP0_7OudbSpA",
        "outputId": "b419b97e-96a1-437d-8912-5788c0f57807"
      },
      "source": [
        "t = np.delete(dataset,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "X  = t[:,:-1]\n",
        "Y = dataset[:,-1]\n",
        "XVALID = X[:index]\n",
        "YVALID = Y[:index]\n",
        "XTRAIN = X[index:]\n",
        "YTRAIN = Y[index:]\n",
        "model_r2 = Sequential()\n",
        "model_r2.add(Dense(8, input_dim = 18, activation='relu'))\n",
        "model_r2.add(Dense(4,activation='relu'))\n",
        "model_r2.add(Dense(1,activation='sigmoid'))\n",
        "model_r2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "callback_a = ModelCheckpoint(filepath = 'model_r2.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "  # The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=64, verbose=1)\n",
        "history_r2 = model_r2.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64, batch_size = 40, callbacks = [callback_a,callback_b])\n",
        "model_r2.load_weights('model_r2.hdf5')\n",
        "p_r2 = model_r2.predict(XVALID)\n",
        "acc_r2 = model_r2.evaluate(XVALID, YVALID)\n",
        "accuracy_3.append(acc_r2[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "721/721 [==============================] - 2s 2ms/step - loss: 0.4116 - accuracy: 0.8594 - val_loss: 0.2835 - val_accuracy: 0.8973\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.89726, saving model to model_r2.hdf5\n",
            "Epoch 2/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2712 - accuracy: 0.9007 - val_loss: 0.2590 - val_accuracy: 0.9014\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.89726 to 0.90139, saving model to model_r2.hdf5\n",
            "Epoch 3/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2442 - accuracy: 0.9052 - val_loss: 0.2344 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90139 to 0.90585, saving model to model_r2.hdf5\n",
            "Epoch 4/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2242 - accuracy: 0.9089 - val_loss: 0.2217 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90585 to 0.90722, saving model to model_r2.hdf5\n",
            "Epoch 5/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2141 - accuracy: 0.9127 - val_loss: 0.2166 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90722 to 0.90844, saving model to model_r2.hdf5\n",
            "Epoch 6/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2116 - accuracy: 0.9128 - val_loss: 0.2168 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90844\n",
            "Epoch 7/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2134 - accuracy: 0.9107 - val_loss: 0.2131 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90844\n",
            "Epoch 8/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2071 - accuracy: 0.9125 - val_loss: 0.2116 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90844\n",
            "Epoch 9/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2065 - accuracy: 0.9124 - val_loss: 0.2102 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90844\n",
            "Epoch 10/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1997 - accuracy: 0.9140 - val_loss: 0.2106 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90844 to 0.90941, saving model to model_r2.hdf5\n",
            "Epoch 11/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2029 - accuracy: 0.9145 - val_loss: 0.2085 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.90941\n",
            "Epoch 12/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9131 - val_loss: 0.2071 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90941\n",
            "Epoch 13/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1964 - accuracy: 0.9153 - val_loss: 0.2050 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90941\n",
            "Epoch 14/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1999 - accuracy: 0.9159 - val_loss: 0.2079 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90941\n",
            "Epoch 15/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1989 - accuracy: 0.9128 - val_loss: 0.2021 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90941\n",
            "Epoch 16/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1973 - accuracy: 0.9127 - val_loss: 0.2033 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.90941\n",
            "Epoch 17/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1996 - accuracy: 0.9104 - val_loss: 0.2007 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90941\n",
            "Epoch 18/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1961 - accuracy: 0.9150 - val_loss: 0.2019 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90941\n",
            "Epoch 19/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2004 - accuracy: 0.9089 - val_loss: 0.1993 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90941\n",
            "Epoch 20/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1967 - accuracy: 0.9119 - val_loss: 0.1985 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.90941 to 0.90973, saving model to model_r2.hdf5\n",
            "Epoch 21/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1913 - accuracy: 0.9155 - val_loss: 0.1980 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.90973 to 0.91014, saving model to model_r2.hdf5\n",
            "Epoch 22/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9181 - val_loss: 0.2014 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91014\n",
            "Epoch 23/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1941 - accuracy: 0.9147 - val_loss: 0.1972 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91014\n",
            "Epoch 24/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1973 - accuracy: 0.9113 - val_loss: 0.1974 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91014\n",
            "Epoch 25/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1898 - accuracy: 0.9159 - val_loss: 0.1974 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91014\n",
            "Epoch 26/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1976 - accuracy: 0.9134 - val_loss: 0.1964 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91014\n",
            "Epoch 27/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1935 - accuracy: 0.9149 - val_loss: 0.1962 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91014\n",
            "Epoch 28/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1943 - accuracy: 0.9120 - val_loss: 0.1961 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91014\n",
            "Epoch 29/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1903 - accuracy: 0.9149 - val_loss: 0.1958 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91014\n",
            "Epoch 30/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1934 - accuracy: 0.9143 - val_loss: 0.1984 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91014\n",
            "Epoch 31/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9163 - val_loss: 0.1963 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.91014 to 0.91022, saving model to model_r2.hdf5\n",
            "Epoch 32/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1910 - accuracy: 0.9149 - val_loss: 0.1948 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91022\n",
            "Epoch 33/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9143 - val_loss: 0.1954 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91022\n",
            "Epoch 34/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9155 - val_loss: 0.1950 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.91022 to 0.91095, saving model to model_r2.hdf5\n",
            "Epoch 35/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1907 - accuracy: 0.9127 - val_loss: 0.1938 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91095\n",
            "Epoch 36/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1894 - accuracy: 0.9139 - val_loss: 0.1937 - val_accuracy: 0.9116\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.91095 to 0.91159, saving model to model_r2.hdf5\n",
            "Epoch 37/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1899 - accuracy: 0.9144 - val_loss: 0.1966 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91159\n",
            "Epoch 38/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1866 - accuracy: 0.9160 - val_loss: 0.1989 - val_accuracy: 0.9051\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91159\n",
            "Epoch 39/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1900 - accuracy: 0.9131 - val_loss: 0.1949 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91159\n",
            "Epoch 40/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9149 - val_loss: 0.1925 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91159\n",
            "Epoch 41/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9149 - val_loss: 0.1928 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91159\n",
            "Epoch 42/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1904 - accuracy: 0.9124 - val_loss: 0.1962 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91159\n",
            "Epoch 43/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9145 - val_loss: 0.1937 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91159\n",
            "Epoch 44/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9131 - val_loss: 0.1923 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91159\n",
            "Epoch 45/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.9147 - val_loss: 0.1924 - val_accuracy: 0.9118\n",
            "\n",
            "Epoch 00045: val_accuracy improved from 0.91159 to 0.91176, saving model to model_r2.hdf5\n",
            "Epoch 46/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1948 - accuracy: 0.9106 - val_loss: 0.1933 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91176\n",
            "Epoch 47/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1921 - accuracy: 0.9140 - val_loss: 0.1918 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91176\n",
            "Epoch 48/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9125 - val_loss: 0.1936 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91176\n",
            "Epoch 49/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1940 - accuracy: 0.9114 - val_loss: 0.1912 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91176\n",
            "Epoch 50/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9139 - val_loss: 0.1921 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91176\n",
            "Epoch 51/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9131 - val_loss: 0.1909 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91176\n",
            "Epoch 52/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9111 - val_loss: 0.1908 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91176\n",
            "Epoch 53/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9143 - val_loss: 0.1914 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91176\n",
            "Epoch 54/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1932 - accuracy: 0.9124 - val_loss: 0.1942 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91176\n",
            "Epoch 55/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1882 - accuracy: 0.9120 - val_loss: 0.1921 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91176\n",
            "Epoch 56/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9126 - val_loss: 0.1928 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91176\n",
            "Epoch 57/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1865 - accuracy: 0.9147 - val_loss: 0.1909 - val_accuracy: 0.9115\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91176\n",
            "Epoch 58/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9166 - val_loss: 0.1910 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91176\n",
            "Epoch 59/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1876 - accuracy: 0.9114 - val_loss: 0.1907 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91176\n",
            "Epoch 60/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1913 - accuracy: 0.9103 - val_loss: 0.1901 - val_accuracy: 0.9112\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91176\n",
            "Epoch 61/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9142 - val_loss: 0.1904 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91176\n",
            "Epoch 62/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9151 - val_loss: 0.1900 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91176\n",
            "Epoch 63/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1823 - accuracy: 0.9173 - val_loss: 0.1902 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91176\n",
            "Epoch 64/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1843 - accuracy: 0.9129 - val_loss: 0.1906 - val_accuracy: 0.9116\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91176\n",
            "386/386 [==============================] - 1s 3ms/step - loss: 0.1924 - accuracy: 0.9118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8lm60kwuA7T"
      },
      "source": [
        "**Removing Least-3 significant Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6JZdAnkzCUA",
        "outputId": "1a9a0fab-3c44-4652-b593-5916d144c2cb"
      },
      "source": [
        "t = np.delete(dataset,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "X  = t[:,:-1]\n",
        "Y = dataset[:,-1]\n",
        "XVALID = X[:index]\n",
        "YVALID = Y[:index]\n",
        "XTRAIN = X[index:]\n",
        "YTRAIN = Y[index:]\n",
        "model_r3 = Sequential()\n",
        "model_r3.add(Dense(8, input_dim = 17, activation='relu'))\n",
        "model_r3.add(Dense(4,activation='relu'))\n",
        "model_r3.add(Dense(1,activation='sigmoid'))\n",
        "model_r3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "callback_a = ModelCheckpoint(filepath = 'model_r3.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "  # The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=64, verbose=1)\n",
        "history_r3 = model_r3.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64, batch_size = 40, callbacks = [callback_a,callback_b])\n",
        "model_r3.load_weights('model_r3.hdf5')\n",
        "p_r3 = model_r3.predict(XVALID)\n",
        "acc_r3 = model_r3.evaluate(XVALID, YVALID)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "721/721 [==============================] - 2s 2ms/step - loss: 0.3816 - accuracy: 0.8893 - val_loss: 0.2892 - val_accuracy: 0.8850\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88496, saving model to model_r3.hdf5\n",
            "Epoch 2/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2822 - accuracy: 0.8909 - val_loss: 0.2611 - val_accuracy: 0.9005\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88496 to 0.90050, saving model to model_r3.hdf5\n",
            "Epoch 3/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2473 - accuracy: 0.9021 - val_loss: 0.2321 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90050 to 0.90568, saving model to model_r3.hdf5\n",
            "Epoch 4/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2238 - accuracy: 0.9079 - val_loss: 0.2219 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90568 to 0.90690, saving model to model_r3.hdf5\n",
            "Epoch 5/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2078 - accuracy: 0.9146 - val_loss: 0.2116 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90690 to 0.90941, saving model to model_r3.hdf5\n",
            "Epoch 6/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2098 - accuracy: 0.9118 - val_loss: 0.2071 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90941\n",
            "Epoch 7/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2005 - accuracy: 0.9141 - val_loss: 0.2054 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90941 to 0.90997, saving model to model_r3.hdf5\n",
            "Epoch 8/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9138 - val_loss: 0.2035 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.90997\n",
            "Epoch 9/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9145 - val_loss: 0.2106 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90997\n",
            "Epoch 10/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1960 - accuracy: 0.9134 - val_loss: 0.2017 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90997 to 0.91006, saving model to model_r3.hdf5\n",
            "Epoch 11/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1957 - accuracy: 0.9140 - val_loss: 0.2014 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.91006 to 0.91070, saving model to model_r3.hdf5\n",
            "Epoch 12/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9147 - val_loss: 0.2013 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.91070\n",
            "Epoch 13/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1973 - accuracy: 0.9126 - val_loss: 0.2023 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91070\n",
            "Epoch 14/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1997 - accuracy: 0.9128 - val_loss: 0.2024 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.91070\n",
            "Epoch 15/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1929 - accuracy: 0.9151 - val_loss: 0.1995 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.91070\n",
            "Epoch 16/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9165 - val_loss: 0.1989 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.91070 to 0.91095, saving model to model_r3.hdf5\n",
            "Epoch 17/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1942 - accuracy: 0.9129 - val_loss: 0.1986 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.91095\n",
            "Epoch 18/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1961 - accuracy: 0.9127 - val_loss: 0.1979 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.91095\n",
            "Epoch 19/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1937 - accuracy: 0.9126 - val_loss: 0.1970 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.91095\n",
            "Epoch 20/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1922 - accuracy: 0.9136 - val_loss: 0.1968 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91095\n",
            "Epoch 21/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9117 - val_loss: 0.1962 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91095\n",
            "Epoch 22/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9146 - val_loss: 0.1954 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91095\n",
            "Epoch 23/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9152 - val_loss: 0.1948 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91095\n",
            "Epoch 24/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1905 - accuracy: 0.9135 - val_loss: 0.1944 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.91095 to 0.91111, saving model to model_r3.hdf5\n",
            "Epoch 25/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1905 - accuracy: 0.9120 - val_loss: 0.1934 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91111\n",
            "Epoch 26/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1874 - accuracy: 0.9139 - val_loss: 0.2046 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91111\n",
            "Epoch 27/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1929 - accuracy: 0.9128 - val_loss: 0.1947 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91111\n",
            "Epoch 28/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1922 - accuracy: 0.9125 - val_loss: 0.1934 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91111\n",
            "Epoch 29/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9140 - val_loss: 0.1920 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91111\n",
            "Epoch 30/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1852 - accuracy: 0.9151 - val_loss: 0.1917 - val_accuracy: 0.9110\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91111\n",
            "Epoch 31/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1936 - accuracy: 0.9140 - val_loss: 0.1927 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91111\n",
            "Epoch 32/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9153 - val_loss: 0.1923 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91111\n",
            "Epoch 33/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1845 - accuracy: 0.9146 - val_loss: 0.1939 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91111\n",
            "Epoch 34/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9144 - val_loss: 0.1915 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91111\n",
            "Epoch 35/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9160 - val_loss: 0.1903 - val_accuracy: 0.9118\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.91111 to 0.91176, saving model to model_r3.hdf5\n",
            "Epoch 36/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9117 - val_loss: 0.1900 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91176\n",
            "Epoch 37/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.9135 - val_loss: 0.1910 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91176\n",
            "Epoch 38/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1857 - accuracy: 0.9128 - val_loss: 0.1919 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91176\n",
            "Epoch 39/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1908 - accuracy: 0.9136 - val_loss: 0.1900 - val_accuracy: 0.9115\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91176\n",
            "Epoch 40/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9116 - val_loss: 0.1898 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91176\n",
            "Epoch 41/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.9172 - val_loss: 0.1899 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91176\n",
            "Epoch 42/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1876 - accuracy: 0.9132 - val_loss: 0.1890 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91176\n",
            "Epoch 43/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9156 - val_loss: 0.1896 - val_accuracy: 0.9090\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91176\n",
            "Epoch 44/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9152 - val_loss: 0.1903 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91176\n",
            "Epoch 45/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1876 - accuracy: 0.9133 - val_loss: 0.1892 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91176\n",
            "Epoch 46/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1896 - accuracy: 0.9146 - val_loss: 0.1893 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91176\n",
            "Epoch 47/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1925 - accuracy: 0.9121 - val_loss: 0.1903 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91176\n",
            "Epoch 48/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9160 - val_loss: 0.1889 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91176\n",
            "Epoch 49/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9122 - val_loss: 0.1886 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91176\n",
            "Epoch 50/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1853 - accuracy: 0.9151 - val_loss: 0.1891 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91176\n",
            "Epoch 51/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9121 - val_loss: 0.1894 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91176\n",
            "Epoch 52/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1839 - accuracy: 0.9168 - val_loss: 0.1882 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91176\n",
            "Epoch 53/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9125 - val_loss: 0.1880 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91176\n",
            "Epoch 54/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9131 - val_loss: 0.1889 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91176\n",
            "Epoch 55/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1860 - accuracy: 0.9154 - val_loss: 0.1881 - val_accuracy: 0.9108\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91176\n",
            "Epoch 56/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1835 - accuracy: 0.9156 - val_loss: 0.1902 - val_accuracy: 0.9081\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91176\n",
            "Epoch 57/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1875 - accuracy: 0.9115 - val_loss: 0.1879 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91176\n",
            "Epoch 58/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9151 - val_loss: 0.1908 - val_accuracy: 0.9087\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91176\n",
            "Epoch 59/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1832 - accuracy: 0.9160 - val_loss: 0.1887 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91176\n",
            "Epoch 60/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9167 - val_loss: 0.1882 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91176\n",
            "Epoch 61/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1886 - accuracy: 0.9128 - val_loss: 0.1883 - val_accuracy: 0.9112\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91176\n",
            "Epoch 62/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1813 - accuracy: 0.9168 - val_loss: 0.1886 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91176\n",
            "Epoch 63/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9128 - val_loss: 0.1878 - val_accuracy: 0.9114\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91176\n",
            "Epoch 64/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1840 - accuracy: 0.9141 - val_loss: 0.1891 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91176\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1903 - accuracy: 0.9118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0uhUw3at5OG"
      },
      "source": [
        "\n",
        "\n",
        "**Removing Least-4 significant Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JUQW7qI03-E",
        "outputId": "bf64e845-c149-474f-c208-c67f049f19f4"
      },
      "source": [
        "t = np.delete(dataset,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "X  = t[:,:-1]\n",
        "Y = dataset[:,-1]\n",
        "XVALID = X[:index]\n",
        "YVALID = Y[:index]\n",
        "XTRAIN = X[index:]\n",
        "YTRAIN = Y[index:]\n",
        "model_r4 = Sequential()\n",
        "model_r4.add(Dense(8, input_dim = 16, activation='relu'))\n",
        "model_r4.add(Dense(4,activation='relu'))\n",
        "model_r4.add(Dense(1,activation='sigmoid'))\n",
        "model_r4.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "callback_a = ModelCheckpoint(filepath = 'model_r4.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "  # The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=64, verbose=1)\n",
        "history_r4 = model_r4.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64, batch_size = 40, callbacks = [callback_a,callback_b])\n",
        "model_r4.load_weights('model_r4.hdf5')\n",
        "p_r4 = model_r4.predict(XVALID)\n",
        "acc_r4 = model_r4.evaluate(XVALID, YVALID)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "721/721 [==============================] - 2s 2ms/step - loss: 0.4426 - accuracy: 0.8474 - val_loss: 0.2893 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88423, saving model to model_r4.hdf5\n",
            "Epoch 2/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2787 - accuracy: 0.8877 - val_loss: 0.2603 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.88423\n",
            "Epoch 3/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2466 - accuracy: 0.8906 - val_loss: 0.2358 - val_accuracy: 0.9051\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.88423 to 0.90512, saving model to model_r4.hdf5\n",
            "Epoch 4/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2260 - accuracy: 0.9071 - val_loss: 0.2212 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90512 to 0.90795, saving model to model_r4.hdf5\n",
            "Epoch 5/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2117 - accuracy: 0.9130 - val_loss: 0.2121 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90795 to 0.90916, saving model to model_r4.hdf5\n",
            "Epoch 6/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2067 - accuracy: 0.9132 - val_loss: 0.2107 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90916\n",
            "Epoch 7/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2032 - accuracy: 0.9123 - val_loss: 0.2067 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.90916\n",
            "Epoch 8/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2006 - accuracy: 0.9124 - val_loss: 0.2051 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90916 to 0.90949, saving model to model_r4.hdf5\n",
            "Epoch 9/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2028 - accuracy: 0.9097 - val_loss: 0.2037 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90949\n",
            "Epoch 10/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1985 - accuracy: 0.9122 - val_loss: 0.2027 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90949 to 0.91006, saving model to model_r4.hdf5\n",
            "Epoch 11/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2016 - accuracy: 0.9122 - val_loss: 0.2009 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.91006\n",
            "Epoch 12/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1975 - accuracy: 0.9132 - val_loss: 0.1988 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.91006\n",
            "Epoch 13/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1939 - accuracy: 0.9136 - val_loss: 0.1978 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.91006\n",
            "Epoch 14/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1945 - accuracy: 0.9136 - val_loss: 0.1968 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.91006 to 0.91014, saving model to model_r4.hdf5\n",
            "Epoch 15/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1998 - accuracy: 0.9094 - val_loss: 0.1973 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.91014\n",
            "Epoch 16/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1932 - accuracy: 0.9120 - val_loss: 0.1963 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.91014\n",
            "Epoch 17/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1879 - accuracy: 0.9169 - val_loss: 0.1997 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.91014\n",
            "Epoch 18/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1923 - accuracy: 0.9122 - val_loss: 0.1951 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.91014 to 0.91030, saving model to model_r4.hdf5\n",
            "Epoch 19/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9129 - val_loss: 0.1949 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.91030 to 0.91070, saving model to model_r4.hdf5\n",
            "Epoch 20/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9133 - val_loss: 0.1943 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.91070\n",
            "Epoch 21/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1892 - accuracy: 0.9162 - val_loss: 0.1946 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.91070\n",
            "Epoch 22/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1915 - accuracy: 0.9136 - val_loss: 0.1941 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.91070\n",
            "Epoch 23/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1878 - accuracy: 0.9162 - val_loss: 0.1937 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.91070\n",
            "Epoch 24/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9140 - val_loss: 0.1955 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.91070 to 0.91111, saving model to model_r4.hdf5\n",
            "Epoch 25/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1827 - accuracy: 0.9180 - val_loss: 0.1934 - val_accuracy: 0.9100\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91111\n",
            "Epoch 26/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9133 - val_loss: 0.1973 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.91111\n",
            "Epoch 27/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1909 - accuracy: 0.9153 - val_loss: 0.1938 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91111\n",
            "Epoch 28/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9144 - val_loss: 0.1931 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91111\n",
            "Epoch 29/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1858 - accuracy: 0.9159 - val_loss: 0.1948 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91111\n",
            "Epoch 30/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1863 - accuracy: 0.9158 - val_loss: 0.1941 - val_accuracy: 0.9103\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91111\n",
            "Epoch 31/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9172 - val_loss: 0.1955 - val_accuracy: 0.9075\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91111\n",
            "Epoch 32/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1871 - accuracy: 0.9172 - val_loss: 0.1937 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91111\n",
            "Epoch 33/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1847 - accuracy: 0.9180 - val_loss: 0.1939 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91111\n",
            "Epoch 34/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1881 - accuracy: 0.9145 - val_loss: 0.1924 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91111\n",
            "Epoch 35/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1928 - accuracy: 0.9137 - val_loss: 0.1926 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91111\n",
            "Epoch 36/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1870 - accuracy: 0.9144 - val_loss: 0.1939 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91111\n",
            "Epoch 37/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1864 - accuracy: 0.9143 - val_loss: 0.1991 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91111\n",
            "Epoch 38/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1855 - accuracy: 0.9140 - val_loss: 0.1972 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.91111\n",
            "Epoch 39/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1854 - accuracy: 0.9161 - val_loss: 0.1935 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91111\n",
            "Epoch 40/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9168 - val_loss: 0.1932 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91111\n",
            "Epoch 41/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1885 - accuracy: 0.9155 - val_loss: 0.1949 - val_accuracy: 0.9066\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91111\n",
            "Epoch 42/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9156 - val_loss: 0.1929 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91111\n",
            "Epoch 43/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1897 - accuracy: 0.9105 - val_loss: 0.1919 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91111\n",
            "Epoch 44/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1906 - accuracy: 0.9103 - val_loss: 0.1914 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91111\n",
            "Epoch 45/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1873 - accuracy: 0.9140 - val_loss: 0.1933 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91111\n",
            "Epoch 46/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1864 - accuracy: 0.9149 - val_loss: 0.1923 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91111\n",
            "Epoch 47/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1827 - accuracy: 0.9141 - val_loss: 0.1931 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91111\n",
            "Epoch 48/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1862 - accuracy: 0.9146 - val_loss: 0.1918 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91111\n",
            "Epoch 49/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1841 - accuracy: 0.9137 - val_loss: 0.1917 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91111\n",
            "Epoch 50/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1819 - accuracy: 0.9153 - val_loss: 0.1914 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91111\n",
            "Epoch 51/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1818 - accuracy: 0.9186 - val_loss: 0.1917 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91111\n",
            "Epoch 52/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1831 - accuracy: 0.9165 - val_loss: 0.1910 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91111\n",
            "Epoch 53/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9101 - val_loss: 0.1917 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91111\n",
            "Epoch 54/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1861 - accuracy: 0.9150 - val_loss: 0.1937 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91111\n",
            "Epoch 55/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1856 - accuracy: 0.9154 - val_loss: 0.1937 - val_accuracy: 0.9063\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91111\n",
            "Epoch 56/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1810 - accuracy: 0.9170 - val_loss: 0.1916 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91111\n",
            "Epoch 57/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1851 - accuracy: 0.9136 - val_loss: 0.1913 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91111\n",
            "Epoch 58/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1834 - accuracy: 0.9157 - val_loss: 0.1914 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91111\n",
            "Epoch 59/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1825 - accuracy: 0.9173 - val_loss: 0.1905 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91111\n",
            "Epoch 60/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1844 - accuracy: 0.9135 - val_loss: 0.1905 - val_accuracy: 0.9111\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91111\n",
            "Epoch 61/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1849 - accuracy: 0.9115 - val_loss: 0.1917 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91111\n",
            "Epoch 62/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1872 - accuracy: 0.9140 - val_loss: 0.1904 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91111\n",
            "Epoch 63/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1837 - accuracy: 0.9157 - val_loss: 0.1903 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91111\n",
            "Epoch 64/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1848 - accuracy: 0.9157 - val_loss: 0.1912 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91111\n",
            "386/386 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFeHKtUI3dkT"
      },
      "source": [
        "**Removing Least-5 significant Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi8IDUok1hcX",
        "outputId": "b7eaa87c-bd15-4bd1-b6b4-8d7e56090631"
      },
      "source": [
        "t = np.delete(dataset,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "t = np.delete(t,1,1)\n",
        "index = int(0.3*len(dataset[:,0]))\n",
        "X  = t[:,:-1]\n",
        "Y = dataset[:,-1]\n",
        "XVALID = X[:index]\n",
        "YVALID = Y[:index]\n",
        "XTRAIN = X[index:]\n",
        "YTRAIN = Y[index:]\n",
        "model_r5 = Sequential()\n",
        "model_r5.add(Dense(8, input_dim = 15, activation='relu'))\n",
        "model_r5.add(Dense(4,activation='relu'))\n",
        "model_r5.add(Dense(1,activation='sigmoid'))\n",
        "model_r5.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "callback_a = ModelCheckpoint(filepath = 'model_r5.hdf5', monitor='val_accuracy', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "  # The patience value can be 10, 20, 100, etc. depending on when your model starts to overfit\n",
        "callback_b = EarlyStopping(monitor='val_accuracy', mode='min', patience=64, verbose=1)\n",
        "history_r5 = model_r5.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=64, batch_size = 40, callbacks = [callback_a,callback_b])\n",
        "model_r5.load_weights('model_r5.hdf5')\n",
        "p_r5 = model_r5.predict(XVALID)\n",
        "acc_r5 = model_r5.evaluate(XVALID, YVALID)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "721/721 [==============================] - 2s 2ms/step - loss: 0.5280 - accuracy: 0.7619 - val_loss: 0.2885 - val_accuracy: 0.8862\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.88617, saving model to model_r5.hdf5\n",
            "Epoch 2/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2727 - accuracy: 0.8979 - val_loss: 0.2575 - val_accuracy: 0.9007\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.88617 to 0.90074, saving model to model_r5.hdf5\n",
            "Epoch 3/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2458 - accuracy: 0.9047 - val_loss: 0.2360 - val_accuracy: 0.9048\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.90074 to 0.90479, saving model to model_r5.hdf5\n",
            "Epoch 4/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2298 - accuracy: 0.9083 - val_loss: 0.2258 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.90479 to 0.90665, saving model to model_r5.hdf5\n",
            "Epoch 5/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2183 - accuracy: 0.9109 - val_loss: 0.2216 - val_accuracy: 0.9067\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.90665 to 0.90674, saving model to model_r5.hdf5\n",
            "Epoch 6/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2184 - accuracy: 0.9093 - val_loss: 0.2208 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.90674\n",
            "Epoch 7/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2074 - accuracy: 0.9148 - val_loss: 0.2198 - val_accuracy: 0.9069\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.90674 to 0.90690, saving model to model_r5.hdf5\n",
            "Epoch 8/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2218 - accuracy: 0.9082 - val_loss: 0.2201 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.90690 to 0.90763, saving model to model_r5.hdf5\n",
            "Epoch 9/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2132 - accuracy: 0.9113 - val_loss: 0.2184 - val_accuracy: 0.9072\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.90763\n",
            "Epoch 10/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2079 - accuracy: 0.9130 - val_loss: 0.2193 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.90763\n",
            "Epoch 11/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2056 - accuracy: 0.9137 - val_loss: 0.2170 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.90763 to 0.90860, saving model to model_r5.hdf5\n",
            "Epoch 12/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2084 - accuracy: 0.9118 - val_loss: 0.2157 - val_accuracy: 0.9074\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.90860\n",
            "Epoch 13/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2017 - accuracy: 0.9156 - val_loss: 0.2149 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.90860\n",
            "Epoch 14/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2023 - accuracy: 0.9136 - val_loss: 0.2133 - val_accuracy: 0.9062\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.90860\n",
            "Epoch 15/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2072 - accuracy: 0.9113 - val_loss: 0.2116 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.90860\n",
            "Epoch 16/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2076 - accuracy: 0.9102 - val_loss: 0.2126 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.90860 to 0.90989, saving model to model_r5.hdf5\n",
            "Epoch 17/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2044 - accuracy: 0.9097 - val_loss: 0.2094 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.90989\n",
            "Epoch 18/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2055 - accuracy: 0.9090 - val_loss: 0.2076 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.90989\n",
            "Epoch 19/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2022 - accuracy: 0.9131 - val_loss: 0.2060 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.90989\n",
            "Epoch 20/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1981 - accuracy: 0.9139 - val_loss: 0.2062 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.90989\n",
            "Epoch 21/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2030 - accuracy: 0.9120 - val_loss: 0.2037 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.90989\n",
            "Epoch 22/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1998 - accuracy: 0.9130 - val_loss: 0.2040 - val_accuracy: 0.9079\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.90989\n",
            "Epoch 23/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1992 - accuracy: 0.9133 - val_loss: 0.2024 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.90989 to 0.91054, saving model to model_r5.hdf5\n",
            "Epoch 24/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1971 - accuracy: 0.9136 - val_loss: 0.2011 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.91054\n",
            "Epoch 25/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1959 - accuracy: 0.9141 - val_loss: 0.2032 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.91054\n",
            "Epoch 26/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2006 - accuracy: 0.9102 - val_loss: 0.2001 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.91054 to 0.91127, saving model to model_r5.hdf5\n",
            "Epoch 27/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1934 - accuracy: 0.9142 - val_loss: 0.1997 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.91127\n",
            "Epoch 28/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1982 - accuracy: 0.9114 - val_loss: 0.2013 - val_accuracy: 0.9080\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.91127\n",
            "Epoch 29/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1928 - accuracy: 0.9141 - val_loss: 0.1990 - val_accuracy: 0.9112\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.91127\n",
            "Epoch 30/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.2013 - accuracy: 0.9096 - val_loss: 0.1982 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.91127\n",
            "Epoch 31/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9128 - val_loss: 0.1984 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.91127\n",
            "Epoch 32/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1917 - accuracy: 0.9142 - val_loss: 0.1977 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.91127\n",
            "Epoch 33/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1963 - accuracy: 0.9102 - val_loss: 0.1987 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.91127\n",
            "Epoch 34/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1935 - accuracy: 0.9121 - val_loss: 0.1983 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.91127\n",
            "Epoch 35/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1956 - accuracy: 0.9128 - val_loss: 0.1981 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.91127\n",
            "Epoch 36/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9117 - val_loss: 0.1970 - val_accuracy: 0.9096\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.91127\n",
            "Epoch 37/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1947 - accuracy: 0.9121 - val_loss: 0.1976 - val_accuracy: 0.9101\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.91127\n",
            "Epoch 38/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9102 - val_loss: 0.1962 - val_accuracy: 0.9116\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.91127 to 0.91159, saving model to model_r5.hdf5\n",
            "Epoch 39/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1951 - accuracy: 0.9122 - val_loss: 0.1961 - val_accuracy: 0.9105\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.91159\n",
            "Epoch 40/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1973 - accuracy: 0.9120 - val_loss: 0.1975 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.91159\n",
            "Epoch 41/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1940 - accuracy: 0.9119 - val_loss: 0.1958 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.91159\n",
            "Epoch 42/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1880 - accuracy: 0.9152 - val_loss: 0.1958 - val_accuracy: 0.9095\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.91159\n",
            "Epoch 43/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1916 - accuracy: 0.9137 - val_loss: 0.1956 - val_accuracy: 0.9104\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.91159\n",
            "Epoch 44/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1969 - accuracy: 0.9105 - val_loss: 0.1956 - val_accuracy: 0.9093\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.91159\n",
            "Epoch 45/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1969 - accuracy: 0.9106 - val_loss: 0.1957 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.91159\n",
            "Epoch 46/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1889 - accuracy: 0.9142 - val_loss: 0.1952 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.91159\n",
            "Epoch 47/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1911 - accuracy: 0.9135 - val_loss: 0.1951 - val_accuracy: 0.9099\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.91159\n",
            "Epoch 48/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1924 - accuracy: 0.9117 - val_loss: 0.1983 - val_accuracy: 0.9071\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.91159\n",
            "Epoch 49/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9112 - val_loss: 0.1959 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.91159\n",
            "Epoch 50/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1918 - accuracy: 0.9138 - val_loss: 0.1950 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.91159\n",
            "Epoch 51/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1919 - accuracy: 0.9128 - val_loss: 0.1955 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.91159\n",
            "Epoch 52/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1966 - accuracy: 0.9113 - val_loss: 0.1960 - val_accuracy: 0.9086\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.91159\n",
            "Epoch 53/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1937 - accuracy: 0.9124 - val_loss: 0.1950 - val_accuracy: 0.9082\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.91159\n",
            "Epoch 54/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1883 - accuracy: 0.9151 - val_loss: 0.1955 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.91159\n",
            "Epoch 55/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9113 - val_loss: 0.1956 - val_accuracy: 0.9085\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.91159\n",
            "Epoch 56/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1887 - accuracy: 0.9140 - val_loss: 0.1949 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.91159\n",
            "Epoch 57/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1842 - accuracy: 0.9146 - val_loss: 0.1953 - val_accuracy: 0.9107\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.91159\n",
            "Epoch 58/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1933 - accuracy: 0.9115 - val_loss: 0.1948 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.91159\n",
            "Epoch 59/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1912 - accuracy: 0.9120 - val_loss: 0.1948 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.91159\n",
            "Epoch 60/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1955 - accuracy: 0.9110 - val_loss: 0.1945 - val_accuracy: 0.9102\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.91159\n",
            "Epoch 61/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1924 - accuracy: 0.9119 - val_loss: 0.1951 - val_accuracy: 0.9097\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.91159\n",
            "Epoch 62/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1868 - accuracy: 0.9135 - val_loss: 0.1946 - val_accuracy: 0.9092\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.91159\n",
            "Epoch 63/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1902 - accuracy: 0.9129 - val_loss: 0.1969 - val_accuracy: 0.9091\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.91159\n",
            "Epoch 64/64\n",
            "721/721 [==============================] - 1s 2ms/step - loss: 0.1914 - accuracy: 0.9147 - val_loss: 0.1968 - val_accuracy: 0.9084\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.91159\n",
            "386/386 [==============================] - 1s 1ms/step - loss: 0.1962 - accuracy: 0.9116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVJnGy92tjuN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "1PpInxw82E3H",
        "outputId": "0d7c0336-34fa-4d9d-d3fe-18b5b5f706b7"
      },
      "source": [
        "column_names = ['ALL-features','Removing-least2','Removing-least3','Removing-least4','Removing-least5']\n",
        "accuracies = []\n",
        "accuracies.append(accuracy_2[1])\n",
        "accuracies.append(acc_r2[1])\n",
        "accuracies.append(acc_r3[1])\n",
        "accuracies.append(acc_r4[1])\n",
        "accuracies.append(acc_r5[1])\n",
        "plt.bar(column_names, accuracies)\n",
        "plt.ylim(0.89,0.92)\n",
        "plt.xlabel('Features', fontsize=10)\n",
        "plt.ylabel('Accuracy', fontsize=10)\n",
        "plt.xticks(column_names, fontsize=5, rotation=90)\n",
        "plt.title('Comparision graph after removing least significant features')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}